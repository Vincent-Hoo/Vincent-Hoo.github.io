<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>BERT的儿子们简介</title>
    <url>/2021/12/07/BERT%E7%9A%84%E5%84%BF%E5%AD%90%E4%BB%AC%E7%AE%80%E4%BB%8B/</url>
    <content><![CDATA[<p>Bert 的面世带动了 NLP 预训练模型的蓬勃发展，同时也衍生了很多的 bert 的变种，这里我们介绍一下 bert 的这些变种模型，如 ALBERT，fastbert，tinybert等，看下他们都是做了哪些优化的。</p>
<span id="more"></span>
<p><br></p>
<h3 id="RoBERTa"><a href="#RoBERTa" class="headerlink" title="RoBERTa"></a>RoBERTa</h3><blockquote>
<p>RoBERTa: A Robustly Optimized BERT Pretraining Approach</p>
</blockquote>
<p>roberta 在结构上和 bert 完全一样，只是通过一些其他的方式证明 bert 还没完全训练完，性能还能提升，与 bert 比较，roberta 在预训练的时候做了以下改进</p>
<p>1：动态 mask：对比 bert 的 mask，稍微好一点</p>
<ul>
<li>bert 的训练数据在训练前就全部构造好了，因此对于一个 sample，它的 mask 的位置是固定的，每个 epoch 都是一样</li>
<li>而 roberta 构造训练数据是在数据输入模型前，这样能保证每个 epoch 的 sample，mask 的位置不同<img src="/2021/12/07/BERT%E7%9A%84%E5%84%BF%E5%AD%90%E4%BB%AC%E7%AE%80%E4%BB%8B/roberta-1.png" class="" width="500" height="500">
</li>
</ul>
<p>2： 去掉 NSP 任务，并且更改数据输入格式为全部填充可以跨越多个文档</p>
<ul>
<li>结论就是 NSP 任务没什么用，ALBERT 也验证了并换成难度更大的 SOP 任务</li>
<li>roberta 试验了四种输入格式<ul>
<li>segment-pair + NSP：就是 bert 的输入格式</li>
<li>sentence-pair + NSP：输入的是一对句子，前后都是单个句子；对比 segment-pair，这个效果更差，因为两句句子太短了，对比一大段的两段文字，前后关系不好判断。</li>
<li>full-sentence：如果输入的最大长度为 512，那么就是尽量选择 512 长度的连续句子。如果跨 document 了，就在中间加上一个特殊分隔符。无 NSP。实验使用了这个，因为能够固定 batch size 的大小。</li>
<li>doc-sentence：输入和 full-sentence 类似，但不能跨两个 document</li>
</ul>
</li>
</ul>
<img src="/2021/12/07/BERT%E7%9A%84%E5%84%BF%E5%AD%90%E4%BB%AC%E7%AE%80%E4%BB%8B/roberta-2.png" class="" width="500" height="500">
<p>3：更多的数据，更大的 batch size，更多的步数，更长的训练时间</p>
<ul>
<li>数据：bert：16G，roberta：160G</li>
<li>batchsize：bert：256，roberta：8k</li>
</ul>
<p><br></p>
<h3 id="ALBERT"><a href="#ALBERT" class="headerlink" title="ALBERT"></a>ALBERT</h3><blockquote>
<p> ALBERT: A Lite BERT For Self-Supervised Learning Of Language Representations</p>
</blockquote>
<p>从论文题目看，albert 是主打 lightweight 的，对比 bert 主要有下面几点改进</p>
<ol>
<li>Factorized embedding parameterization：bert 的 embedding 层，每个词向量的大小 E 都和 transformer 隐藏层 H 一样，$E = H = 768$，因此词表占了很大的一个参数量，因此 albert 将 E 和 H 分开，令 $E &lt; H$，通过一个矩阵进行映射即可</li>
<li>Cross-layer parameter sharing：为了减少网络的参数量，albert 对不同层的参数进行共享，参数共享有三种方式：只共享 feed-forward network 的参数、只共享 attention 的参数、共享全部参数。albert 默认是共享全部参数的</li>
<li>Inter-sentence coherence loss：去掉 NSP 任务，改做 sentence-order-prediction（SOP）任务，NSP 任务比较简单，模型更多的会通过两个句子的 topic 是否一致来判断，而不是通过句子间的关系来判断，而 SOP 任务直接判断两个句子是正的还是反的，并且两个句子来自同一个文档，topic 保持一致。</li>
</ol>
<p>作者实验了以下几种网络参数，通过共享所有层的参数，参数量大幅度下降</p>
<img src="/2021/12/07/BERT%E7%9A%84%E5%84%BF%E5%AD%90%E4%BB%AC%E7%AE%80%E4%BB%8B/albert-1.png" class="">
<p>其实 Albert 只能做到减少参数量，并不能很好的加快推理速度，因为网络还是那么深，对比 bert-large 和 albert-large，他们唯一不同就在于 embedding 层变小了，但就这一点不同，就有 1.7 倍的加速了；另外我们可以看出共享参数是会掉精度的，这也很好理解</p>
<img src="/2021/12/07/BERT%E7%9A%84%E5%84%BF%E5%AD%90%E4%BB%AC%E7%AE%80%E4%BB%8B/albert-2.png" class="">
<p>改变 embedding size，对于 bert 和 albert 都有一定影响，128 效果最好</p>
<img src="/2021/12/07/BERT%E7%9A%84%E5%84%BF%E5%AD%90%E4%BB%AC%E7%AE%80%E4%BB%8B/albert-3.png" class="">
<p>共享不同参数，可以看出 FFN 比较重要，attention 没那么重要，但考虑到压缩量，作者还是选择全部共享</p>
<img src="/2021/12/07/BERT%E7%9A%84%E5%84%BF%E5%AD%90%E4%BB%AC%E7%AE%80%E4%BB%8B/albert-4.png" class="">
<p>SOP 任务训练可以解决 NSP 任务，但 NSP 任务训练不可以解决 SOP 任务</p>
<img src="/2021/12/07/BERT%E7%9A%84%E5%84%BF%E5%AD%90%E4%BB%AC%E7%AE%80%E4%BB%8B/albert-5.png" class="">
<p>最后，作者尝试了用更多的数据（roberta的训练数据集），训练更长时间，都能得到提升，并且作者去掉 dropout 也能提升，因为模型还没到过拟合的程度，因此不需要 dropout。</p>
<p>albert 能做到好的结果，得益于 更大的 H，更多的数据，SOP 任务，dropout 的去除，将这些 trick 应用到 bert 上，bert 也能得到提升。</p>
<p><br></p>
<h3 id="BERT-Distillation"><a href="#BERT-Distillation" class="headerlink" title="BERT Distillation"></a>BERT Distillation</h3><blockquote>
<p>Distilling Task-Specific Knowledge from BERT into Simple Neural Networks.<br>Patient Knowledge Distillation for BERT Model Compression.<br>DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter.<br>TinyBERT: Distilling BERT for Natural Language Understanding.<br>MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices.<br>MINILM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers.<br>FastBERT: a Self-distilling BERT with Adaptive Inference Time.</p>
</blockquote>
<p>上面蒸馏方法的对比如下表，可以看出主要的差别在于</p>
<ol>
<li>KD 的阶段是在预训练阶段还是微调阶段</li>
<li>KD 的位置，从最后的输出 logit 到中间特征层，和 CV 的 KD 一样</li>
<li>是否用教师网络来初始化学生网络</li>
</ol>
<img src="/2021/12/07/BERT%E7%9A%84%E5%84%BF%E5%AD%90%E4%BB%AC%E7%AE%80%E4%BB%8B/distill-table.png" class="">
<h4 id="Distilled-BiLSTM"><a href="#Distilled-BiLSTM" class="headerlink" title="Distilled BiLSTM"></a>Distilled BiLSTM</h4><p>教师网络是先将 BERT-large 微调到具体任务，学生网络是 BiLSTM，将教师网络的 logit 输出蒸馏到学生网络的 logit，这里应该是用的教师网络的 [CLS] 的那个logit 输出，距离函数直接用的 MSE，针对不同输入的任务，都是将 lstm 两边的输出拼接然后接 softmax 得到任务的 logit。最终的损失函数为 ground-true label 的 CE loss + 教师网络和学生网络 logit 之间的 MSE（作者发现 MSE 比 CE 好）。</p>
<p>另外因为 KD 是在 fine-tune 阶段，所以做了 data augmentation。</p>
<img src="/2021/12/07/BERT%E7%9A%84%E5%84%BF%E5%AD%90%E4%BB%AC%E7%AE%80%E4%BB%8B/distill-bilstm.png" class="" width="500" height="500">
<h4 id="BERT-PKD"><a href="#BERT-PKD" class="headerlink" title="BERT-PKD"></a>BERT-PKD</h4><p>同样是 finetune 阶段的 KD，将 BERT-BASE 在下游任务上进行 finetune 得到教师网络，学生网络同样是 BERT，但是深度比教师网络浅，比如 3 或6 层，因为只是深度减少了，所以可以用教师网络来初始化学生网络，损失函数为下游任务的 ground-true CE loss + 教师网络和学生网络 logit CE loss + 中间层的蒸馏，中间层蒸馏也不是每个 token 都比较，只蒸馏 [CLS]，用的是 MSE loss，中间层的选择有两种，分别是 PKD-last 和 PKD-skip，skip 的结果好一点</p>
<img src="/2021/12/07/BERT%E7%9A%84%E5%84%BF%E5%AD%90%E4%BB%AC%E7%AE%80%E4%BB%8B/bert-pkd.png" class="" width="500" height="500">
<h4 id="DistilledBert"><a href="#DistilledBert" class="headerlink" title="DistilledBert"></a>DistilledBert</h4><p>前面两个 KD 的工作都是在 finetune 阶段，学生网络从教师网络中学习到的都是 task-specific 的知识，DistilledBert 选择预训练阶段进行蒸馏，也叫做通用蒸馏（general distillation）。</p>
<ul>
<li>教师网络是 BERT-base，学生网络为 6 层 bert，采取 PKD-skip 的方式初始化</li>
<li>训练方式采取 roberta 的方式，去掉 NSP 任务，采取动态 mask，但训练数据和 bert 保持一致，16G</li>
<li>由于只有 MLM 任务，因此损失函数有 MLM loss，教师网络-学生网络最后一层的交叉熵，除此之外，作者加了一个 cosine embedding loss，比较最后一层 hidden（softmax 层之前）</li>
</ul>
<h4 id="Tinybert"><a href="#Tinybert" class="headerlink" title="Tinybert"></a>Tinybert</h4><p>将前面所有工作混在一起，既进行预训练阶段的蒸馏，也进行微调阶段的蒸馏，蒸馏的位置包括输出 logit、中间层特征（attention map，hidden state）、word embedding。</p>
<p>整体的流程如下，首先有一个教师网络 bert-base，和一个没训练的学生网络，这里不拿教师网络去初始化，因为学生网络的隐藏层和教师网络可能不一样，初始化不了，然后用教师网络进行 general distillation 得到 general tinybert，然后将教师网络在下游任务中进行 finetune，再拿这个 finetune 的教师网络去进行 task-specific 的蒸馏得到 finetune tinybert（有数据增广）。</p>
<img src="/2021/12/07/BERT%E7%9A%84%E5%84%BF%E5%AD%90%E4%BB%AC%E7%AE%80%E4%BB%8B/tinybert-1.png" class="" width="500" height="500">
<p>损失函数如下，注意 general distillation 没有 logit 的 loss，中间层的蒸馏采用 pkd-skip 的方式对应。</p>
<ol>
<li>输出层 logit 采取 CE loss</li>
<li>中间层 attention map 蒸馏（MSE），用的是未 softmax 之前的 map</li>
<li>中间层 hidden state 蒸馏（MSE），由于维度不同，会加一个映射矩阵</li>
<li>embedding 蒸馏（MSE），同样维度不同，也会加一个映射矩阵</li>
</ol>
<img src="/2021/12/07/BERT%E7%9A%84%E5%84%BF%E5%AD%90%E4%BB%AC%E7%AE%80%E4%BB%8B/tinybert-2.png" class="" width="700" height="700">
<h4 id="miniLM"><a href="#miniLM" class="headerlink" title="miniLM"></a>miniLM</h4><p>这篇论文的方法是最简洁的，也提出了一种新的知识——value-value 矩阵，attention map 是 K 和 V 点乘得到的，而 value-value 矩阵是将 V 矩阵和 V 矩阵的转置进行点乘，然后进行蒸馏；考虑到教师网络和学生网络层数、维度等都不同，所以只蒸馏最后一层，距离度量采取的是 KL 散度；最后引入助教制度，先蒸馏到助教，助教再蒸馏到学生。</p>
<p>性能的话，还要比 tinybert 和 distilledbert 要好不少，miniLM 只进行 general distillation，简洁效果又好，比 tinybert 蒸馏一大堆性价比高太多了</p>
<img src="/2021/12/07/BERT%E7%9A%84%E5%84%BF%E5%AD%90%E4%BB%AC%E7%AE%80%E4%BB%8B/miniLM.png" class="" width="700" height="700">
<h4 id="mobileBert"><a href="#mobileBert" class="headerlink" title="mobileBert"></a>mobileBert</h4><p>个人最不喜欢的一篇文章，结构复杂，虽说像 mobilenet 那样引入 bottleneck 的结构，但整体的流程太复杂，不推荐。</p>
<h4 id="FastBert"><a href="#FastBert" class="headerlink" title="FastBert"></a>FastBert</h4><p>fastbert 参考了 CV 领域里面的 self-distillation 和 adaptive inference 的策略，由于是自蒸馏，因此教师网络就是学生网络，具体流程是：先拿一个预训练好的 bert 模型，在下游任务中进行 finetune 好，然后在每一层 transformer 后加入一个 classifier，这些 classifier 叫做 student classifier，最后一层的 classifier（finetune 好的）叫做 teacher classifier，然后就是自蒸馏的部分，teacher classifier 的输出分别蒸馏给每一个 student classifier，用的是 KL 散度。</p>
<p>adaptive inference 部分就是对于一个样本，每经过一层 transformer，就经过一个 student classifier，然后输出的 logit，计算不确定性，不确定性用 logit 的 entropy 来表示，如果不确定性低于阈值，就停止往下过网络，反之继续过网络。</p>
<img src="/2021/12/07/BERT%E7%9A%84%E5%84%BF%E5%AD%90%E4%BB%AC%E7%AE%80%E4%BB%8B/fastbert.png" class="" width="700" height="700">
]]></content>
  </entry>
  <entry>
    <title>3D Human Pose and Shape Reconstruction Review</title>
    <url>/2020/09/09/3D-Human-Pose-and-Shape-Reconstruction-Review/</url>
    <content><![CDATA[<p>该文章总结了一些三维人体姿态估计和重构的文章，分别为：</p>
<ul>
<li>HMR：End-to-end Recovery of Human Shape and Pose</li>
<li>SPIN：Learning to Reconstruct 3D Human Pose and Shape via Model-fitting in the Loop</li>
<li>VIBE：Video Inference for Human Body Pose and Shape Estimation</li>
</ul>
<span id="more"></span>
<h2 id="End-to-end-Recovery-of-Human-Shape-and-Pose"><a href="#End-to-end-Recovery-of-Human-Shape-and-Pose" class="headerlink" title="End-to-end Recovery of Human Shape and Pose"></a>End-to-end Recovery of Human Shape and Pose</h2><blockquote>
<p>Authors: Angjoo Kanazawa<br>Conference: CVPR 2018</p>
</blockquote>
<p>该模型的输入为一张图片，或者说是一个 bounding box，bbox 中心为人，输出是重构的三维人体。图片首先经过一个 encoder 提取特征，encoder 用的是在分类任务上预训练好的 ResNet50，特征输入到 regressor 里面经过几轮的迭代最后预测一系列参数，包括 SMPL 模型的参数 $\theta$ 、$\beta$ 以及 weak-perspective camera model 的参数 scale、rotation、translation。</p>
<p>输入的图片均有二维关节点的 annotation，从 regressor 得到的参数中生成三维人体 mesh（$M(\theta,\beta)\in R^{3\times N}$，N=6890），然后从 mesh 中通过线性回归得到三维关节点 $X(\theta, \beta)\in R^{3 \times P}$，P 为关节点个数，相机参数 $s\in R, t\in R^2$，从 3 维关节点经过投影生成 2 维的关节点，R 为 global orient，$\Pi$ 为 orthographic projection。</p>
<script type="math/tex; mode=display">
\hat{x}=s \Pi(RX(\theta, \beta))+t</script><p>由于每张图片都有 2 维关节点，通过这种方式，约束生成的 3 维关节点经过投影后与 2 维关节点相似，从而训练网络。由于 SMPL 的关节点和 2 维不同数据集的关节点数有一些不同，这里作者做的是将不同数据集融合，保证 SMPL 的点在 2 维关节点的 annotation 中有。</p>
<script type="math/tex; mode=display">
L_{reproj}=\sum_i ||v_i(x_i-\hat{x}_i)||_1</script><p>除了每张图片都有 2D supervision 外，有一些图片会有 3D 的 annotation，一般 3D 的 annotation 会以 3D keypoint 的形式给出，用 MoSh 工具可以从 3D keypoint 中得到 SMPL 的参数 $\theta$ 和 $\beta$，3D supervision 的 loss 如下，joint loss 加上参数 loss</p>
<script type="math/tex; mode=display">
L_{3D} = ||X_i-\hat{X}_i||_2^2 + ||[\beta_i, \theta_i] - [\hat{\beta}_i, \hat{\theta}_i]||_2^2</script><p>除了一般的 reprojection loss 外，以前的文章都会加入一些人为 的先验，来约束 SMPL 的 $\theta$ 参数来避免一些不自然的动作或者关节点扭动，这里作者采取一种 data-driven 的先验方式，通过训练一个判别器，让网络自己去判断回归出来的 SMPL 参数是否像正常人，首先需要一个 3D 人体的数据库（每个人体的 SMPL 参数都有），然后判别器的网络结构方面，采取的是公用前面的 feature extraction，后面每个关节点逐一判别。</p>
<p>训练网络总的 loss 如下</p>
<script type="math/tex; mode=display">
L=\lambda(L_{reproj}+1L_{3D})+L_{adv}</script><img src="/2020/09/09/3D-Human-Pose-and-Shape-Reconstruction-Review/1.png" class="">
<p>训练用到的 2D 数据集有：LSP、LSP-extended、MPII、MS COCO，3D 数据集有：Human3.6M、MPI-INF-3DHP。由于 SMPL 的 23 个关节点和用到的数据集不是很 match，因此用一个 regressor 从生成的 mesh 中回归出 Human3.6M 的14 个点，同样的方法加上 MS COCO 脸上的 5 个点，共 19 个点用于 reprojection loss。</p>
<p><br></p>
<h2 id="Learning-to-Reconstruct-3D-Human-Pose-and-Shape-via-Model-fitting-in-the-Loop"><a href="#Learning-to-Reconstruct-3D-Human-Pose-and-Shape-via-Model-fitting-in-the-Loop" class="headerlink" title="Learning to Reconstruct 3D Human Pose and Shape via Model-fitting in the Loop"></a>Learning to Reconstruct 3D Human Pose and Shape via Model-fitting in the Loop</h2><blockquote>
<p>Authors: Nikos Kolotouros<br>Conference: ICCV 2019</p>
</blockquote>
<p>作者结合了 regression-based 和 optimization-based 两种方法，整体的思路就是先用 HMR 的网络回归出来一个初步的人体模型（下图右），然后再用这个人体去初始化 smplify 模块对人体模型进行优化，并用优化后的人体模型作为 GT 去监督回归网络的训练。</p>
<img src="/2020/09/09/3D-Human-Pose-and-Shape-Reconstruction-Review/2.png" class="">
<p>回归网络和 HMR 几乎一样，唯一不同是 HMR 用欧拉角来表征每个关节点的旋转，而 SPIN 用一个 6 维的向量来表征三维旋转。得到的初步三维人体模型再用 smplify 进一步优化。</p>
<blockquote>
<p>SMPLify tries to fit the SMPL model to a set of 2D keypoints using an optimization-based approach.</p>
</blockquote>
<p>优化的目标函数包含 reprojection loss 还有一些 pose 和 shape 的 prior，比如惩罚肘部和膝盖不自然的旋转，实现上 smplify 分为两步，第一步是固定 initial pose 和 shape，优化 camera translation 和 body orientation，简单说就是调整人体的整体转向；第二步是优化 pose 和 shape。通过 iterative fitting，得到优化后的人体模型 </p>
<p>SPIN 和上一篇 HMR 不同的点在于，它没有直接用 reprojection loss 来训练回归网络，而是用优化后的人体模型参数来作为回归网络输出的 GT，smplify 后的 SMPL 参数为 $\Theta<em>{opt}={\theta</em>{opt}, \beta<em>{opt} }$，回归网络输出的 SMPL 参数为 $\Theta</em>{reg}={\theta<em>{reg}, \beta</em>{reg} }$；同理 smplify 后的 mesh 为 $M_{opt}$，损失函数包括</p>
<script type="math/tex; mode=display">
L_{3D}=||\Theta_{reg}-\Theta_{opt}||</script><script type="math/tex; mode=display">
L_M=||M_{reg} - M_{opt}||</script><p>SPIN 的一大特点在于 self-improving，A good initial network estimate $\Theta<em>{reg}$ will lead the optimization to a better fit $\Theta</em>{opt}$, while a good fit from the iterative routine will provide even better supervision to the network. 对比上一篇的 HMR，SPIN 完全不需要用到 3D 的数据进行训练，并且对比 HMR 训练判别器告诉回归网络生成的 pose 是否正常，SPIN explicitly 用 smplify 提供更加有效的 pose 信息作为 supervision。</p>
<p>训练集有用到 Human3.6M、MPI-INF-3DHP、LSP 并且混合了其他 2D 数据集如 MPII 和 COCO，2D annotation 方面混合所有数据集的关节点，总数为 24 个点，对于某一张图片上没有标注的点，标记 visibility 为 0，表示不可见。</p>
<p><br></p>
<h2 id="VIBE-Video-Inference-for-Human-Body-Pose-and-Shape-Estimation"><a href="#VIBE-Video-Inference-for-Human-Body-Pose-and-Shape-Estimation" class="headerlink" title="VIBE: Video Inference for Human Body Pose and Shape Estimation"></a>VIBE: Video Inference for Human Body Pose and Shape Estimation</h2><blockquote>
<p>Authors: Muhammed Kocabas<br>Conference: CVPR 2020</p>
</blockquote>
<p>整体流程如下，给定一个 T 帧的视频 $V={I<em>t }</em>{t=1}^T$，temporal generator 对输入的每一帧图片进行 CNN 提取信息，GRU 处理时序信息，最后 regressor 回归出 SMPL 的参数，对于 $\beta$ 参数，取 T 帧的平均值作为整体的 shape，generator 最后的输出 $\hat\Theta = [(\hat\theta<em>1,…,\hat\theta_T), \hat\beta]$，与 AMASS（一个 3D 人体模型的库）的真实人体 $\Theta</em>{real}$ 输出到判别器进行动作的判定。</p>
<img src="/2020/09/09/3D-Human-Pose-and-Shape-Reconstruction-Review/3.png" class="">
<p><strong>Temporal Generator</strong></p>
<p>整体的网络结构和 SPIN 基本一样，只是在 CNN 和 Regressor 之间插入了一个 GRU 的模块，SPIN 是单帧处理的，这里加入了 temporal 的信息，因为前面帧的信息对后面帧的处理有帮助，同样用的 6D 旋转表示，生成器的损失项如下，看不同的数据有哪些 annotation 来决定有哪些损失项，2D 损失项是必定有的，reprojected 2D keypoint 和之前的方法一样</p>
<script type="math/tex; mode=display">
L_G=L_{3D} + L_{2D} + L_{SMPL} + L_{adv}</script><script type="math/tex; mode=display">
L_{3D} = \sum_{i=1}^T||X_t - \hat{X}_t||_2</script><script type="math/tex; mode=display">
L_{2D} = \sum_{i=1}^T||x_t - \hat{x}_t||_2</script><script type="math/tex; mode=display">
L_{SMPL} = \sum_{i=1}^T||\theta_t - \hat{\theta}_t||_2 + ||\beta-\hat\beta||_2</script><p><strong>Motion Discriminator</strong></p>
<p>这里的判别器和 HMR 不同，同样加入了时序的信息，而不是一个单帧动作的判别器，而是一个序列动作的判别器，如下图所示，generator 生成的序列的 SMPL 参数会经过 GRU，然后再经过一个 self-attention 模块最后输入到一个线性层判定动作是否 valid</p>
<img src="/2020/09/09/3D-Human-Pose-and-Shape-Reconstruction-Review/4.png" class="">
<p>训练数据和前两个论文的不同，由于 VIBE 不是单帧处理，而是处理视频数据，因此 2D 数据上用到的是 PoseTrack 和 PennAction，对于 3D keypoint 数据，用到的还是 MPI-INF-3DHP 和 Human3.6M，而 3DPW 和 Human3.6M 提供的 SMPL annotation 也同样用来训练，AMASS 用来训练判别器。</p>
<p><em>In Conclusion</em></p>
<p>三篇文章都是用回归的方式去学习 SMPL 的参数，也有加入对抗学习来作为先验，VIBE 考虑到了 temporal 的信息，但依旧有很多值得改进的地方。</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Kanazawa_End-to-End_Recovery_of_CVPR_2018_paper.pdf">HMR paper</a></li>
<li><a href="https://arxiv.org/pdf/1909.12828.pdf">SPIN paper</a></li>
<li><a href="https://github.com/nkolot/SPIN">SPIN Github</a></li>
<li><a href="https://arxiv.org/abs/1912.05656">VIBE paper</a></li>
<li><a href="https://github.com/mkocabas/VIBE">VIBE Github</a></li>
</ul>
<p><br></p>
]]></content>
  </entry>
  <entry>
    <title>Coursera深度学习课程第五课第三周————序列模型及注意力机制</title>
    <url>/2018/11/05/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%AC%E4%BA%94%E8%AF%BE%E7%AC%AC%E4%B8%89%E5%91%A8%E2%80%94%E2%80%94%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</url>
    <content><![CDATA[<img src="/2018/11/05/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%AC%E4%BA%94%E8%AF%BE%E7%AC%AC%E4%B8%89%E5%91%A8%E2%80%94%E2%80%94%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/certificate.png" class="" width="400" height="10">
<p>该课程讲述序列模型，如机器翻译，语音识别，以及注意力机制将如何提升序列模型的性能。</p>
<span id="more"></span>
<p><br></p>
<h2 id="Seq2Seq模型"><a href="#Seq2Seq模型" class="headerlink" title="Seq2Seq模型"></a>Seq2Seq模型</h2><p>seq2seq模型常用于机器翻译，语音识别，输入和输出都是序列</p>
<h3 id="encoder-decoder-model"><a href="#encoder-decoder-model" class="headerlink" title="encoder-decoder model"></a>encoder-decoder model</h3><p>以机器翻译作为例子，encoder是一个RNN模型，输入句子，然后encoder将这个句子编码，输出一个向量，叫做context vector或者encoding representation，然后这个context vector作为decoder的隐藏层输入，<SOS>作为decoder的第一个输入，得到第一个输出词，再将这个输出词作为输入，得到第二个输出词，以此类推。</p>
<img src="/2018/11/05/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%AC%E4%BA%94%E8%AF%BE%E7%AC%AC%E4%B8%89%E5%91%A8%E2%80%94%E2%80%94%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/encoder_decoder_model.png" class="" width="500" height="250">
<p>image captioning同样也是生成序列的任务，给定一张图片，需要生成一段描述图片的文字，只是这里encoding representation用CNN来实现。</p>
<img src="/2018/11/05/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%AC%E4%BA%94%E8%AF%BE%E7%AC%AC%E4%B8%89%E5%91%A8%E2%80%94%E2%80%94%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image_captioning.png" class="" width="500" height="250">
<p><br></p>
<h3 id="Machine-Translation-vs-Language-Model"><a href="#Machine-Translation-vs-Language-Model" class="headerlink" title="Machine Translation vs Language Model"></a>Machine Translation vs Language Model</h3><p>Language Model 是用来评判一个句子的合理性的，它需要最大化概率 $P(y^{<1>}, y^{<2>}, …, y^{T_y})$</p>
<img src="/2018/11/05/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%AC%E4%BA%94%E8%AF%BE%E7%AC%AC%E4%B8%89%E5%91%A8%E2%80%94%E2%80%94%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/language_model.png" class="" width="500" height="200">
<p> 而Machine Translation采用encoder-decoder的模型，decoder的部分和language model是一模一样的，差别在于language model的隐藏层输入是0向量，而machine translation的隐藏层输入是encoder的输出，因此machine translation又可以作为 conditional language model，条件语言模型，它也是最大化句子的概率，但是这个概率是在输入句子是x的前提下，$P(y^{<1>}, y^{<2>}, …, y^{T_y} |  x^{<1>}, x^{<2>}, …, x^{T_x})$   </p>
<img src="/2018/11/05/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%AC%E4%BA%94%E8%AF%BE%E7%AC%AC%E4%B8%89%E5%91%A8%E2%80%94%E2%80%94%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/machine_translation_model.png" class="" width="500" height="200">
<p>当我们训练完一个机器翻译模型之后，输入一个句子，它的输出并不一定是固定的，输出的结果会在一定范围内变化，但是差别不大，且一般翻译都是正确的，那么我们怎样才能找到最好的翻译呢？有人提出用贪心去搜索，先选择概率最大的第一个词，再基于第一个词，选择概率最大的第二个词，然而我们整体的目标是要最大化整句话的概率，而非每一次一个词的概率，所以贪心搜索就会导致以下情况会选择第二个句子，因为前三个词 $P(Jane/is/visiting) &lt; P(Jane/is/going)$ </p>
<blockquote>
<p>Jane is visiting Africa in September<br>Jane is going to be visiting Africa in September</p>
</blockquote>
<p><br></p>
<h3 id="Beam-Search"><a href="#Beam-Search" class="headerlink" title="Beam Search"></a>Beam Search</h3><p>greedy search每一次只选择一个candidate作为下一次搜索的前提，Beam Search不同之处在于有一个Beam Width的参数，每一次会选择Beam Width个candidate，假如Beam Width等于3，那么decoder的第一步会选择3个first word candidate，比如是in，jane，September，然后将decoder复制3份，分别以encoding vector和这3个first word candidate作为前提，继续搜索3个second word candidate，然后发现 in september，jane is，jane visits是最有可能的3个candidate，这样就把September开头的可能翻译全部去掉，然后以这3个作为输入，继续搜索</p>
<img src="/2018/11/05/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%AC%E4%BA%94%E8%AF%BE%E7%AC%AC%E4%B8%89%E5%91%A8%E2%80%94%E2%80%94%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/beam_search1.png" class="" width="600" height="300">
<img src="/2018/11/05/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%AC%E4%BA%94%E8%AF%BE%E7%AC%AC%E4%B8%89%E5%91%A8%E2%80%94%E2%80%94%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/beam_search2.png" class="" width="600" height="300">
<p><br></p>
<h2 id="Attention-Mechanism"><a href="#Attention-Mechanism" class="headerlink" title="Attention Mechanism"></a>Attention Mechanism</h2><p>attention机制最早提出是在机器翻译领域，传统的encoder-decoder模型在生成每一个输出词的时候，认为每一个输入词都是同等重要的，但是事实上，我们在翻译的时候，我们只会关注整个句子的某一部分去翻译出某一个词，而不是整个句子，因此对于输出词，每一个输入词的重要性是不一样的，attention机制就是计算输出词与每一个输入词的相关性，相关性可以理解为一个权重，$\alpha^{<2,1>}$ 表示第二个输出词与第一个输入词的关系，通过权重可以计算出一个context vector，作为decoder每一轮的输入</p>
<img src="/2018/11/05/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%AC%E4%BA%94%E8%AF%BE%E7%AC%AC%E4%B8%89%E5%91%A8%E2%80%94%E2%80%94%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/attention1.png" class="" width="600" height="300">
<p>权重的计算一般采用一个简单神经网络去实现，输入是decoder上一个状态的隐藏层和encoder的某一个输入词的隐藏层，然后神经网络输出一个权重，再把权重进行softmax的归一化</p>
<img src="/2018/11/05/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%AC%E4%BA%94%E8%AF%BE%E7%AC%AC%E4%B8%89%E5%91%A8%E2%80%94%E2%80%94%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/attention2.png" class="" width="600" height="300">
<p>下面左图是加入注意力机制的网络图，右图是计算注意力机制的图。<br><img src="/2018/11/05/Coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%AC%E4%BA%94%E8%AF%BE%E7%AC%AC%E4%B8%89%E5%91%A8%E2%80%94%E2%80%94%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/attention3.png" class=""></p>
<p><br></p>
]]></content>
  </entry>
  <entry>
    <title>Discussion on the efficacy of teacher networks in knowledge distillation</title>
    <url>/2019/11/04/Discussion-on-the-efficacy-of-teacher-networks-in-knowledge-distillation/</url>
    <content><![CDATA[<p>本文将介绍几篇最近看到的论文，这些论文都共同讨论了教师网络的有效性，当我们在进行知识蒸馏的时候，都会想到的一个问题是：应该选择什么样的教师网络去进行知识迁移？是否越深准确率越高的大网络就一定有利于知识蒸馏呢？答案不是肯定的。这几篇文章从实验的角度给我们展示了在不同教师网络设定下的知识蒸馏的结果，并且提出改进。</p>
<p>本文讨论的论文如下：</p>
<ul>
<li>Improved Knowledge Distillation via Teacher Assistant: Bridging the Gap Between Student and Teacher</li>
<li>On the Efﬁcacy of Knowledge Distillation</li>
<li>Distilling Knowledge From a Deep Pose Regressor Network</li>
</ul>
<span id="more"></span>
<p><br></p>
<h2 id="Improved-Knowledge-Distillation-via-Teacher-Assistant-Bridging-the-Gap-Between-Student-and-Teacher"><a href="#Improved-Knowledge-Distillation-via-Teacher-Assistant-Bridging-the-Gap-Between-Student-and-Teacher" class="headerlink" title="Improved Knowledge Distillation via Teacher Assistant: Bridging the Gap Between Student and Teacher"></a>Improved Knowledge Distillation via Teacher Assistant: Bridging the Gap Between Student and Teacher</h2><p>这是 DeepMind 今年放在 arxiv 上的一篇文章，作者通过一些小实验发现，在进行知识蒸馏的时候，增加教师网络的模型大小反而会使得知识蒸馏的效果变差，结果如下，学生网络是一个 2 层的 CNN，而教师网络分别是 4-10 层的 CNN，从结果可以看出，继续增加教师网络深度，并不一定能提高知识蒸馏的性能。</p>
<img src="/2019/11/04/Discussion-on-the-efficacy-of-teacher-networks-in-knowledge-distillation/1.png" class="" width="500" height="500">
<p>从这个 toy experiment 可以看出，教师网络和学生网络之间的 Gap 确实是影响知识蒸馏的一个重要因素，当 Gap 变大的时候，从图中可以得出以下几个结论</p>
<ul>
<li>教师网络的性能不断提高，证明不是过拟合导致的知识蒸馏性能下降。</li>
<li>教师网络的 capacity 不断增大，而学生网络的 capacity 不变，当 gap 增大到一定程度，学生网络的 capacity 不足以支撑其模仿教师网络。</li>
<li>教师网络的性能不断提高，对于训练样本置信度增高，使得教师网络提供的软标签变“硬”，这也使得知识蒸馏性能下降。</li>
</ul>
<p>为了进一步证明 gap 确实影响知识蒸馏，作者做了一个小对比实验，固定教师网络为 10 层 CNN，改变学生网络的层数，结果如下，确实当 gap 大于某个阈值或者小于某个阈值的时候，知识蒸馏的效果都不是最优。</p>
<img src="/2019/11/04/Discussion-on-the-efficacy-of-teacher-networks-in-knowledge-distillation/2.png" class="" width="500" height="500">
<p>基于这个观察，作者很自然地提出在大网络和小网络之间增加一个中等大小的网络（teacher assistant，TA），先用教师网络蒸馏 TA，再用 TA 蒸馏学生网络。</p>
<img src="/2019/11/04/Discussion-on-the-efficacy-of-teacher-networks-in-knowledge-distillation/3.png" class="" width="500" height="500">
<p>当加入 TA 后，学生网络的蒸馏效果明显比直接蒸馏教师网络要好，NOKD = 没有 KD，BLKD = baseline KD，TAKD 是作者提出的方法。</p>
<img src="/2019/11/04/Discussion-on-the-efficacy-of-teacher-networks-in-knowledge-distillation/4.png" class="" width="500" height="500">
<p>一个很自然的问题是，为什么要用蒸馏的 TA，而不直接训练一个 TA 呢？结果显示蒸馏的 TA 效果更好，从右图可以看出，蒸馏的 TA (KD-TA) 比直接训练的 TA (FS-TA) 要好。</p>
<img src="/2019/11/04/Discussion-on-the-efficacy-of-teacher-networks-in-knowledge-distillation/5.png" class="" width="500" height="500">
<p>作者同时尝试了多阶段的 TA，即蒸馏一个 TA，再用该 TA 蒸馏下一个 TA，以此类推，结果如下，越多个阶段效果越好，但是越费时间和空间，这里的结果和上面 Table 1 的结果有一点矛盾，可以看出 10 -&gt; 2 实际就是 BLKD，但是这里显示的结果是 42.56，Table 1 显示的结果是 44.57，而 44.57 是 10 -&gt; 6 -&gt; 2 的结果，不知道是笔误还是什么。</p>
<img src="/2019/11/04/Discussion-on-the-efficacy-of-teacher-networks-in-knowledge-distillation/6.png" class="" width="700" height="700">
<p>作者同时也对比了其他蒸馏方法，BSS 为基于对抗样本的蒸馏，MUTUAL 为 deep mutual learning，虽然结果显示作者的方法结果最好，但是这些方法差异较大，没有太多比较的必要，FITNET, AT, FSP 都是特征蒸馏方法，而 TAKD 为软标签蒸馏，但是可以看出作者提出的 TAKD 还是普遍好于其他的方法。同样的，这里也有一个和 Table 1 矛盾的地方，ResNet8 的 NOKD 在 Table 1 为 88.52，这里就变成了 86.02。</p>
<img src="/2019/11/04/Discussion-on-the-efficacy-of-teacher-networks-in-knowledge-distillation/7.png" class="">
<p>这篇文章提出了一个影响知识蒸馏的因素：gap，学生网络的表达能力或者 capacity 有限，而如果教师网络太过强，就会提供一些超过学生网络能够学习范围的知识，就会导致反作用，因此如何找到恰当大小的教师网络是一个比较难的问题，作者采用的 TA 的方式，层层蒸馏，下一层吸收上一层的知识，再传递给下一层，确实是一个很好的办法，这解决了我们不知道选什么大小的教师网络的问题。</p>
<p><br></p>
<h2 id="On-the-Efﬁcacy-of-Knowledge-Distillation"><a href="#On-the-Efﬁcacy-of-Knowledge-Distillation" class="headerlink" title="On the Efﬁcacy of Knowledge Distillation"></a>On the Efﬁcacy of Knowledge Distillation</h2><blockquote>
<p>Authors: Jang Hyun Cho,  Bharath Hariharan<br>Conference: ICCV 2019</p>
</blockquote>
<p>跟上面的文章一样，作者也认为 bigger models are not better teachers，但是他并不是做的 toy experiment，而是用 wide-resnet 在 cifar 和 ImageNet 上做了实验。教师网络无论是 wide-resnet 还是 DenseNet，无论是增加深度还是宽度，都是会有 degradation。</p>
<img src="/2019/11/04/Discussion-on-the-efficacy-of-teacher-networks-in-knowledge-distillation/8.png" class="">
<img src="/2019/11/04/Discussion-on-the-efficacy-of-teacher-networks-in-knowledge-distillation/9.png" class="">
<p>而在 ImageNet 上的结果，有点出人意料，可能是 wide-resnet 的缘故，也可能是 ImageNet 的缘故，增加深度，性能不断下降，且 KD 完全无效。学生网络为 ResNet18，教师网络无论选什么，都比学生网络没有蒸馏的差。</p>
<img src="/2019/11/04/Discussion-on-the-efficacy-of-teacher-networks-in-knowledge-distillation/10.png" class="" width="500" height="500">
<p>作者将 degradation 的原因归于教师网络和学生网络 capacity 的 mismatch，教师网络太过强大，因此作者提出两种小策略，来选出 capacity 相对较小的教师网络。</p>
<h3 id="Early-stopping-KD-ESKD"><a href="#Early-stopping-KD-ESKD" class="headerlink" title="Early stopping KD(ESKD)"></a>Early stopping KD(ESKD)</h3><p>作者比较 ResNet18 在蒸馏下和无蒸馏下的 ImageNet 训练曲线发现，KD 一开始是有用的，但到后来就比 scratch 的要差，无论教师网络选择 ResNet34 还是 ResNet50。这两张图直接否定了 KD 的有效性，因此 ImageNet 是否是一个比较难进行知识蒸馏的数据集呢？因为很多 KD 的文章都没有在 ImageNet 上进行验证。</p>
<img src="/2019/11/04/Discussion-on-the-efficacy-of-teacher-networks-in-knowledge-distillation/11.png" class="">
<p>基于上面的观察，作者决定提前结束 KD 过程，即 ESKD，结果如下，这时候 KD 确实有效果了，但是网络深度增加，degradation 现象依旧存在。</p>
<img src="/2019/11/04/Discussion-on-the-efficacy-of-teacher-networks-in-knowledge-distillation/12.png" class="" width="500" height="500">
<p>ESKD 的训练曲线</p>
<img src="/2019/11/04/Discussion-on-the-efficacy-of-teacher-networks-in-knowledge-distillation/13.png" class="">
<h3 id="Early-stopped-teacher"><a href="#Early-stopped-teacher" class="headerlink" title="Early-stopped teacher"></a>Early-stopped teacher</h3><p>解决 degradation 问题，就要把 teacher 的 capacity 降到 student 能够触碰的范围内，一个简单的方法就是早停策略，即不完全训练的 teacher。不完全训练的 teacher 表现上就和小网络一样。完全训练的 teacher 的实验设定为 200 epoch，60 epoch 将学习率（60/200），不完全训练的 teacher 遵循 (15/50，10/35) 等策略。</p>
<p>下图为训练不完全的 teacher 用来蒸馏的效果，可以看出都比训练完全的 teacher 要好。</p>
<img src="/2019/11/04/Discussion-on-the-efficacy-of-teacher-networks-in-knowledge-distillation/14.png" class="">
<p>用不同的学生和教师网络同样验证 ES teacher 的有效性</p>
<img src="/2019/11/04/Discussion-on-the-efficacy-of-teacher-networks-in-knowledge-distillation/15.png" class="">
<p>这篇文章主要是实验型文章（实验结果很多），研究教师网络对于 KD 有效性的影响，并且提出两种策略来使得教师网络的 capacity 和学生网络相匹配，第一种方法为提前结束 KD 过程，至于为什么 KD 到了后面会不如 scratch，原因或许出于软标签上；而第二种方法为教师网络主动降低 capacity 来 match 学生网络。</p>
<p>目前为止的这两篇文章都是围绕如何找到 match 学生网络 capacity 的教师网络。这里改进的点有很多。</p>
<p><br></p>
<h2 id="Distilling-Knowledge-From-a-Deep-Pose-Regressor-Network"><a href="#Distilling-Knowledge-From-a-Deep-Pose-Regressor-Network" class="headerlink" title="Distilling Knowledge From a Deep Pose Regressor Network"></a>Distilling Knowledge From a Deep Pose Regressor Network</h2><p>这篇文章在前面有专门的博客介绍，其关注的是回归问题，而不是分类问题，作者认为回归问题的解空间过大，因此对于学生网络不容易学习，并且回归问题中，网络输出是一张图，而不是一个概率分布，一张图是没有任何限制的，因此教师网络很有可能会给出非常错误的知识，因为回归网络的输出并没有任何约束。所以作者提出要对教师网络的输出进行约束，约束为：若教师网络生成的结果很差，就不用 KD，在 KD Loss 前面为每个样本添加一个系数，来衡量该样本是否值得知识迁移。</p>
<p><br></p>
]]></content>
  </entry>
  <entry>
    <title>GAN theory</title>
    <url>/2018/11/21/GAN-theory/</url>
    <content><![CDATA[<blockquote>
<p>课程来源：李宏毅2018生成对抗网络课程<br>课程主页：<a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses_MLDS18.html">http://speech.ee.ntu.edu.tw/~tlkagk/courses_MLDS18.html</a><br>论文：<a href="/2018/11/21/GAN-theory/Generative%20Adversarial%20Nets.pdf" title="Generative Adversarial Nets.pdf">Generative Adversarial Nets.pdf</a><br>文章梗概：该文章讲述 Ian Goodfellow 在 2014 年提出 GAN 时候的理论基础。</p>
</blockquote>
<span id="more"></span>
<p><br></p>
<p><em>preface</em></p>
<p>GAN所做的事情就是生成，学习真实数据的分布，所谓数据的分布，可以看下面这张图，假如数据是二次元图像，图像的维度是 $10 \times 10$，因此整个Image Space就是 $10 \times 10$ 的空间，$x$ 就是这个空间中的一个样本，即一张图片，显然这个空间囊括了所有 $10 \times 10$ 的图片，而假如真正的二次元图像在这个空间里面服从某一个分布 $P<em>{data}(x)$，我们可以暂且把Image Space看成是离散的点所构成，因此 $P</em>{data}(x)$ 就是空间中某个点 $x$ 属于二次元图像的概率，下图中区域内的点属于二次元图像的概率较高，而区域外的点概率较低。</p>
<img src="/2018/11/21/GAN-theory/1.png" class="" width="500" height="500">
<p><br></p>
<h2 id="maximum-likelihood-estimation"><a href="#maximum-likelihood-estimation" class="headerlink" title="maximum likelihood estimation"></a>maximum likelihood estimation</h2><p>在GAN之前，是通过最大似然的方法是估计真实数据的分布，从而去做一个generator的，假设我们用一个类型已知的分布 $P<em>G(x; \theta)$ 去逼近真实的分布 $P</em>{data}(x)$，这个已知的分布可能是高斯分布，则 $\theta$ 就代表均值和方差，根据极大似然估计的方法，流程如下</p>
<ol>
<li>从真实的数据分布 $P_{data}(x)$ 中，抽样 $m$ 个样本 {$x^1, x^2, …, x^m$} </li>
<li>计算每个样本点在 $P_G(x;\theta)$ 分布下的概率 $P_G(x^i; \theta)$</li>
<li>似然函数 $L = \prod_{i=1}^m P_G(x^i; \theta)$，代表的就是 $P_G(x;\theta)$ 生成该 $m$ 个样本点的概率</li>
<li>找出 $\theta^*$ 使得该概率最大化</li>
</ol>
<p>两个概率分布的 KL-divergence 的公式如下，当两个概率分布越接近，KL-divergence 越小，当两个分布完全相同时，divergence 为 0.</p>
<script type="math/tex; mode=display">D_{KL}(P || Q) = \int p(x)log(\frac{p(x)}{q(x)})</script><p>而实际上最大化概率可以转换为最小化两个概率分布的 KL-divergence，推导如下</p>
<script type="math/tex; mode=display">\theta^* = {arg\max} \prod_{i=1}^m P_G(x^i;\theta) = arg\max \sum logP_G(x^i;\theta)</script><p>上式近似等于</p>
<script type="math/tex; mode=display">\approx arg \max E_{x \sim P_{data}} [logP_G(x; \theta)] = arg\max \int_x P_{data}(x)logP_G(x;\theta)dx</script><p>由于待求的参数是 $\theta$，加上一项与 $\theta$ 无关的项，对结果没有影响</p>
<script type="math/tex; mode=display">= arg\max (\int_x P_{data}(x)logP_G(x;\theta)dx - \int_x P_{data}(x)logP_{data}(x)dx)</script><script type="math/tex; mode=display">=argmax(\int_xP_{data(x)}log(\frac{P_G(x;\theta)}{P_{data}(x)})) = arg \min D_{KL}(P_{data}||P_G)</script><p>因此，最大似然的方法去求解生成数据的概率分布，实际上就是在最小化两个分布的 KL-divergence。</p>
<p><br></p>
<h2 id="generator-objective-function"><a href="#generator-objective-function" class="headerlink" title="generator objective function"></a>generator objective function</h2><p>上面的推导是基于 $P_G(x;\theta)$ 分布类型已知的情况，如高斯分布，用高斯分布去拟合高维的数据，有时候是很难实现的，无论怎样调节均值和方差。因此我们需要一个更加普通的概率分布，但是这样我们就无法计算概率值，因此上面的方法就变得不适用了。</p>
<p>GAN就是用一个generator就实现一个普遍的概率分布 $P<em>G(x)$，generator的作用就是将一个普通的概率分布，如正态分布，转变成一个高维空间的概率分布。而generator的目标是使得生成图片的概率分布 $P_G(x)$ 和 $P</em>{data}(x)$ 尽可能地接近，即两个概率分布的divergence尽可能小。</p>
<img src="/2018/11/21/GAN-theory/2.JPG" class="" width="500" height="500">
<p>我们不知道 $P<em>G(x)$ 和 $P</em>{data}(x)$ 的概率分布，因而也无法计算它们之间的divergence，无法求divergence的最小值。虽然概率分布和分布类型是未知的，但是我们可以进行抽样，样本的概率分布与总体的概率分布是一致的，因此我们需要从两个概率分布的样本值去计算这两个概率分布的divergence。<strong>GAN神奇的地方在于可以透过discriminator计算两个概率分布的divergence</strong>，</p>
<p><br></p>
<h2 id="discriminator-与-divergence的联系"><a href="#discriminator-与-divergence的联系" class="headerlink" title="discriminator 与 divergence的联系"></a>discriminator 与 divergence的联系</h2><p>discriminator的objective function如下，G是固定的，discriminator的目标是最大化objective function，即求解 $D^* = arg \ max \ V(G,D)$ </p>
<script type="math/tex; mode=display">V(G,D) = E_{x\sim P_{data}}[logD(x)] + E_{x \sim P_G}[log(1-D(x))]</script><p>discriminator实际上就是一个二分类器，最大化上面的objective function实际上就是最小化cross entropy，而当我们把discriminator训练好的时候，<strong>目标函数 $V(G,D)$ 的最大值就是我们想要的两个概率分布的divergence</strong>，一个直观的感受如下，当两个概率分布的divergence较小的时候，discriminator比较难区分它们，因此objective function value也比较难拉高；而当两个概率分布差别很大时，objective function value就会容易取到比较大的值。即：$max(V(D,G))$ 越小，divergence越小。</p>
<img src="/2018/11/21/GAN-theory/3.png" class="" width="500" height="500">
<p><br></p>
<p><strong>数学证明 $max(V(G,D))$ 和 divergence的关系如下</strong></p>
<p>$V(G,D) = E<em>{x\sim P</em>{data}}[logD(x)] + E_{x \sim P_G}[log(1-D(x))]$</p>
<p>​                 <script type="math/tex">= \int_x P_{data}(x)logD(x)dx + \int_x P_G(x)log(1-D(x))dx</script></p>
<p>​                 <script type="math/tex">=\int_x [P_{data}(x)logD(x) + P_G(x)log(1-D(x))]dx</script> </p>
<p>这里我们假设 $D(x)$ 可以是任意的函数，即我们假设discriminator的网络可以有无穷多的神经元，给定任意的x，可以输出任意的值。如果这样的话，我们可以把积分看成是求和，然后把被积部分的每个x单独拿出来，求出取得最大值时候的 $D(x)$ 的值，所以 $D(x)$ 就是我们构造出来的一个函数，如下</p>
<script type="math/tex; mode=display">D^*(x) = arg\ max\ P_{data}(x)logD(x) + P_G(x)log(1-D(x))</script><p>接下来就是求出这个 $D^*(x)$</p>
<img src="/2018/11/21/GAN-theory/4.png" class="" width="500" height="500">
<p>求到 $D^*(x)$ 之后，我们可以算出 $max(V(G,D))$ </p>
<p>$max(V(G,D)) = V(G, D^*) = E<em>{x\sim P</em>{data}}[log\ \frac{P<em>{data}(x)}{P</em>{data}(x) + P<em>G(x)}] + E</em>{x\sim P<em>G}[log\ \frac{P</em>{G}(x)}{P_{data}(x) + P_G(x)}]$</p>
<p>$=\int<em>x P</em>{data}(x)log[ \frac{P<em>{data}(x)}{P</em>{data}(x) + P<em>G(x)}]dx + \int_x P_G(x) log[\frac{P</em>{G}(x)}{P_{data}(x) + P_G(x)}]dx$   </p>
<p>进行一些简单的变换，分子分母同除2</p>
<p>$=\int<em>x P</em>{data}(x)log[ \frac{0.5P<em>{data}(x)}{0.5（P</em>{data}(x) + P<em>G(x)）}]dx + \int_x P_G(x) log[\frac{0.5P</em>{G}(x)}{0.5(P_{data}(x) + P_G(x))}]dx$   </p>
<p>$=\int<em>xP</em>{data}(x)log(0.5)dx + \int<em>xP_G(x)log(0.5)dx + \int_x P</em>{data}(x)log[ \frac{P<em>{data}(x)}{0.5（P</em>{data}(x) + P<em>G(x)）}]dx + \int_x P_G(x) log[\frac{P</em>{G}(x)}{0.5(P_{data}(x) + P_G(x))}]dx$</p>
<p>$=2log(0.5) + D<em>{KL}(P</em>{data} || \frac{P<em>{data} + P_G}{2}) + D</em>{KL}(P<em>{G} || \frac{P</em>{data} + P_G}{2})$</p>
<p>$=2log(0.5) + 2D<em>{JSD}(P</em>{data} || P_G)$ </p>
<p>可以看出，$max(V(G,D))$ 实际上就是两个概率分布的JS-divergence和一个常数的和，两个分布divergence越小，$max(V(G,D))$ 越小，因为两种数据分布接近，难以区分，所以 $V(G,D)$ 能取到的最大值也越小。</p>
<p><br></p>
<h2 id="回到最原始的objective-function"><a href="#回到最原始的objective-function" class="headerlink" title="回到最原始的objective function"></a>回到最原始的objective function</h2><p>最原始的objective function是想找到一个generator，使得generator生成图片的概率分布 $P<em>G$ 和 真实数据 $P</em>{data}$ 的divergence最小，即 $G^* = arg \ min \ Div(P<em>G, P</em>{data})$ </p>
<p>然后我们通过上面的证明可以知道，$max(V(G,D))$ 就是两个概率分布的divergence，所以原始的目标函数可以变为 $G^* = arg \ min_G \ max_D V(G,D)$ </p>
<p>下面这张图描述了如何解决 $minmax$ 这种优化问题，我们假设就只有三个 $G$，固定一个 $G$，可以得到选择不同 $D$ 下的 $V(G,D)$，然后比较不同 $G$ 下的 $max \  V(G,D)$，选择最小的，即 $G<em>3$。可以看出 $V(G,D)$ 的最大值就是 $P_G$ 和 $P</em>{data}$ 之间的divergence</p>
<img src="/2018/11/21/GAN-theory/5.png" class="" width="500" height="500">
<p><br></p>
<h2 id="Algorithm-for-minmax-problem"><a href="#Algorithm-for-minmax-problem" class="headerlink" title="Algorithm for minmax problem"></a>Algorithm for minmax problem</h2><p>objective function：$G^* = arg \ min_G \ max_D V(G,D)$</p>
<p>令 $L(G) = max_D \ V(G,D)$ </p>
<p>因此问题变成找 $L(G)$ 的最小值，自然可以用梯度下降去解决 $\theta_G  = \theta_G - \alpha \frac{\partial L(G)}{\partial \theta_G}$ </p>
<p>$L(G) = max_D V(G,D) = max(L_1(G), L_2(G), L_3(G), …)$ </p>
<p>$L(G)$ 函数的意思是，给定一个 $G$，我就搜索一个 $D$ 使得 $V(G,D)$ 最大，我们可以将它看成是无数个函数 $L_i(G)$ 求最大值，一个 $L_i(G)$ 对应一个 $D$，好比下面的图中，有三条曲线，相当于只有 3 个 $D$，而实际上应该是有无穷多个 $D$，当给定一个 $G_0$ 的时候，就会从众多条曲线 $L_i(G)$ 中，选择一条使得 $L(G_0)$ 取最大值的曲线，然后把这条曲线记作是 $L^*(G)$，就可以求梯度了 $\frac{\partial L^{\ast}(G)}{\partial \theta_G}$</p>
<img src="/2018/11/21/GAN-theory/6.png" class="" width="500" height="500">
<p>上面提到从众多条曲线 $L_i(G)$ 中，选择一条使得 $L(G_0)$ 取最大值的曲线 $L^*(G)$，实际上 $L^{\ast}(G) = V(G, D^{\ast})$，$D^{\ast} = arg \ max \ V(G_0, D)$，所以解决这个minmax problem的算法如下</p>
<ol>
<li>给定一个初始的 $G_0$</li>
<li>找到 $D^*$ 使得 $V(G_0, D)$ 取最大值，即找出那条曲线 $L^{\ast}(G)$</li>
<li>得到 $G_1 = G_0 - \frac{\partial V(G, D^{\ast})}{\partial G}$</li>
<li>重复2、3步</li>
</ol>
<p>下面是实际上的algorithm，我们用均值来代替期望，一点需要注意的是discriminator需要重复多次，尽可能找到更好的D，能够使得 $V(G,D)$ 较大，实际上我们只能找到 $V(G,D)$ 的下界，因为理论上我们假设的D可以是任意的函数，但是实际上D的神经元并不可能无穷多。所以我们只能尽可能地找到一个使得 $V(G,D)$ 大一点的D，来衡量 $P_{data}$ 和 $P_G$ 的divergence。</p>
<p>固定D，我们需要求解 $V(G,D)$ 的最小值，来降低divergence，由于 $V(G,D)$ 的第一项和G没有关系，所以可以去掉。注意的是G一次只需要更新一次。</p>
<img src="/2018/11/21/GAN-theory/7.png" class="" width="500" height="500">
<p>这里解释一下为什么只能更新一次G，我们在更新G的参数的时候，是固定D的，这个D是我们找到的 $D^{\ast}$，它能够使得 $V(G<em>0, D^{\ast})$ 取到最大值，也意味着这个 $D^{\ast}$ 是用来衡量 $P</em>{G<em>0}$ 和 $P</em>{data}$ 之间的divergence，然后我们用梯度下降，想减小这个divergence，更新 $G<em>0$ 成 $G_1$，这时候我们的D还是固定不变的，那么 $V(G_1, D^{\ast})$ 是否同样取到最大值，是否能够表示 $P</em>{G<em>1}$ 和 $P</em>{data}$ 的divergence呢？答案是不肯定的，假如下面的情况出现，更新后的 $G_1$ 所对应的 $V(G_1,D)$ 曲线发生了较大的变化，导致取到最大值的点并不是原来的 $D_0^{\ast}$，而是 $D_1^{\ast}$，这时候如果再用原来的 $D_0^{\ast}$ 去计算 $V(G,D)$ 就不能表达两个数据之间的divergence了。所以G不能更新太多，或者一次不能更新太多，使得下面的情况出现，我们只能更新一点点，并且假设 $D_1^{\ast} \approx D_0^{\ast}$，两条曲线没有太大变化，这样才能保证算法的正确性。</p>
<img src="/2018/11/21/GAN-theory/9.png" class="" width="500" height="500">
<p><br></p>
<h2 id="minmax-GAN-vs-non-saturating-GAN"><a href="#minmax-GAN-vs-non-saturating-GAN" class="headerlink" title="minmax GAN vs non-saturating GAN"></a>minmax GAN vs non-saturating GAN</h2><p>在更新generator的时候，minmax GAN是更新一个和discriminator相同的objective function，这会导致gradient vanishing的问题，Ian Goodfellow在2016年的GAN tutorial里面提到</p>
<blockquote>
<p>In the minimax game, the discriminator minimizes a cross-entropy, but the generator maximizes the same cross-entropy. This is unfortunate for the generator, because when the discriminator successfully rejects generator samples with high confidence, the generator’s gradient vanishes. </p>
</blockquote>
<p>而non-saturating GAN则是把generator的objective function改为</p>
<script type="math/tex; mode=display">J^G = -\frac{1}{2}E[logD(G(z))]</script><p>non-saturating GAN另外一个好处是，在训练的一开始，$D(G(z))$ 一般都很小，minmax GAN的话，梯度很小，更新很慢；而non-saturating GAN一开始的梯度都比较大，因此收敛快一点。</p>
<img src="/2018/11/21/GAN-theory/10.png" class="" width="200" height="500">
<p><br></p>
<p><em>In Conclusion</em></p>
<p>这是Ian Goodfellow在2014年提出GAN的时候的理论，但是理论总是在发展，discriminator是否真的在描述两个概率分布的divergence也不好说，最后discriminator是否真的会坏掉，还得继续考察。</p>
<p><br></p>
]]></content>
  </entry>
  <entry>
    <title>Getting Started with Git</title>
    <url>/2018/08/15/Getting-Started-with-Git/</url>
    <content><![CDATA[<img src="/2018/08/15/Getting-Started-with-Git/header.jpg" class="" width="500" height="250">
<blockquote>
<p>Git是一个分布式的版本控制软件，不需要服务器端软件，就可以运作版本控制，使得代码的发布和交流机器方便。Git的速度很快，这对于Linux内核的项目来说很重要，且Git拥有出色的合并追踪(merge tracking)能力，能够记录修改的代码。</p>
</blockquote>
<span id="more"></span>
<p><br></p>
<h1 id="1-安装配置Git"><a href="#1-安装配置Git" class="headerlink" title="1. 安装配置Git"></a>1. 安装配置Git</h1><p>Linux下安装Git非常的简单，只需要简单的一条命令。<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ sudo apt-get install git</span><br></pre></td></tr></table></figure><br>配置本地Git的用户名和邮箱与Github的一致<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ git config --global user.name <span class="string">&quot;Your Name&quot;</span></span><br><span class="line">$ git config --global user.email <span class="string">&quot;email@example.com&quot;</span></span><br></pre></td></tr></table></figure><br><br></p>
<h1 id="2-Git基本概念"><a href="#2-Git基本概念" class="headerlink" title="2. Git基本概念"></a>2. Git基本概念</h1><h2 id="2-1-What-is-git"><a href="#2-1-What-is-git" class="headerlink" title="2.1 What is git?"></a>2.1 What is git?</h2><p>git是一个管理代码的版本控制系统(Version Control System, VCS)，它能跟踪每一个文件的变化，如果你修改了某个文件，VCS能够记录并且保存这些变化，这使得你可以撤销任何的修改，回溯到任何一个历史版本。<br><strong>注意</strong><br>所有的版本控制系统都只能跟踪文本文件的改动，如TXT文件、网页文件、程序代码等；而对于图片、视频这种二进制文件，虽然也能由版本控制系统管理，但没法跟踪文件的变化，只能把二进制文件每次改动串起来，也就是只能知道图片从100KB改成120KB，但是到底改了什么，无法知道。不幸的是，Microsoft的Word文件也是二进制格式的，因此Git也没法跟踪Word文件的改动。</p>
<h2 id="2-2-Git-workflow"><a href="#2-2-Git-workflow" class="headerlink" title="2.2 Git workflow"></a>2.2 Git workflow</h2><p>在一个Git项目里面，由三个主要的组成元素：</p>
<ul>
<li>版本库(repository)：简称repo，它记录了你项目的所有变化和修改，保存了所有的commit，commit是指在某个时间点，项目所有文件的一个snapshot，可以理解为一次提交，将最新的版本提交到版本库，交由版本库进行保存。</li>
<li>暂存区(index or staging area)：它是工作区和版本库之间的一个bridge，可以理解为一个中转站。暂存区的文件能够被commit，index is where commits are prepared。</li>
<li>工作区(working tree)：就是我们本地工作的目录，我们能够修改文件、增添删除文件的目录。</li>
</ul>
<p>三者可以理解为一个层级关系，版本库是在层级最高的位置，工作区最低，我们在工作区工作，进行文件修改，我们每完成一次修改，我们希望保存到版本库，并且告诉版本库这次修改了什么，好让版本库能够记录下来。<br>下面就是Git的基本工作流程：</p>
<ol>
<li>在工作区修改文件。</li>
<li>将工作区所有修改过的文件add(stage)到暂存区，准备commit。</li>
<li>commit到版本库。</li>
</ol>
<img src="/2018/08/15/Getting-Started-with-Git/git_workflow.png" class="" width="600" height="300">
<p>一个文件的四个状态，可以通过git status命令看得到这些词。</p>
<ul>
<li>Untracked: 当一个文件新创建，在版本库里面没有它，所以无法追踪。</li>
<li>Modified：一个文件在工作区被修改了，但是还没到暂存区。</li>
<li>Staged：修改的文件被stage到暂存区。</li>
<li>Committed：修改的文件在暂存区commit到版本库。</li>
</ul>
<img src="/2018/08/15/Getting-Started-with-Git/git_four_states.png" class="" width="600" height="300">
<p>Git项目的三个组成元素，各自代表一个版本，我们可以查看两两之间的差别：</p>
<ul>
<li>git diff: 查看工作区和暂存区的差别。</li>
<li>git diff —cached: 查看暂存区和版本库的差别。</li>
<li>git diff HEAD: 查看工作区和版本库的差别。</li>
</ul>
<h2 id="2-3-版本回退"><a href="#2-3-版本回退" class="headerlink" title="2.3 版本回退"></a>2.3 版本回退</h2><h3 id="2-3-1-仓库版本回退"><a href="#2-3-1-仓库版本回退" class="headerlink" title="2.3.1 仓库版本回退"></a>2.3.1 仓库版本回退</h3><p>Git既然能够跟踪文件的修改，自然就可以回溯，<code>.git</code>文件有保留每一次commit的信息，每一次commit都有一个唯一的commit id对应，它是经过SHA1计算出来的一个数字，用16进制表示，我们可以通过命令回退命令，回退到想要版本。<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git reset &lt;mode&gt; &lt;你想回退到的版本的commit id的前几位&gt;</span><br></pre></td></tr></table></figure></p>
<p>在Git管理版本的时候，会有一个HEAD指针，指向最近的一次commit，每次的commit会练成一条链，回退的过程实际是将HEAD指针指向之前的节点<br><img src="/2018/08/15/Getting-Started-with-Git/git_reset.png" class="" width="400" height="200"></p>
<p>reset有三种mode：soft, mixed(默认), hard，可以看到无论是哪个模式，HEAD的位置都肯定是会改变的，所以reset的另外一个作用就是撤销commit。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">mode</th>
<th style="text-align:center">HEAD指针的位置</th>
<th style="text-align:center">暂存区</th>
<th style="text-align:center">工作区</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">soft</td>
<td style="text-align:center">改变</td>
<td style="text-align:center">不变</td>
<td style="text-align:center">不变</td>
</tr>
<tr>
<td style="text-align:center">mixed</td>
<td style="text-align:center">改变</td>
<td style="text-align:center">改变</td>
<td style="text-align:center">不变</td>
</tr>
<tr>
<td style="text-align:center">hard</td>
<td style="text-align:center">改变</td>
<td style="text-align:center">改变</td>
<td style="text-align:center">改变</td>
</tr>
</tbody>
</table>
</div>
<p>一种更加简单的方式替换commit id的是用HEAD指针，上一个版本就是<code>HEAD^</code>，上上个版本就是<code>HEAD^^</code>，依次往后可以写成<code>HEAD~n</code>。</p>
<h3 id="2-3-2-工作区修改撤销"><a href="#2-3-2-工作区修改撤销" class="headerlink" title="2.3.2 工作区修改撤销"></a>2.3.2 工作区修改撤销</h3><p>当你在工作区乱改一通，也忘了自己改了什么地方，那么你可以用本地版本库的内容直接替换工作区，丢弃工作区的所有改变，用以下的命令可以使得file回到最近一次commit或者stage时的状态(注意，stage也是可以的，有些人可能提交到暂存区后，又乱改一通，这条命令将暂存区的版本替换工作区)<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git checkout -- &lt;file&gt;</span><br></pre></td></tr></table></figure></p>
<h3 id="2-3-3-暂存区版本回退"><a href="#2-3-3-暂存区版本回退" class="headerlink" title="2.3.3 暂存区版本回退"></a>2.3.3 暂存区版本回退</h3><p>当我们在commit之前发现文件有问题，不能commit，但是修改的文件已经添加到了暂存区了，可以用以下的命令将暂存区的修改撤销(unstage)，重新放回工作区。<code>git reset</code>命令既可以回退版本，也可以把暂存区的修改回退到工作区，用HEAD的时候，表示最新的版本。<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git reset HEAD &lt;file&gt;</span><br></pre></td></tr></table></figure></p>
<p>总结一下：</p>
<ol>
<li>撤销工作区修改，可以直接用本地版本库来替换</li>
<li>不小心stage到暂存区，可以用reset命令撤销，撤销后暂存区该文件恢复原状，但是工作区依旧时修改了的。但是我个人觉得，既然工作区你瞎改了一些东西，并且add到暂存区，那么只需要把修改的内容改回去，再重新add到暂存区，就能恢复原状了。</li>
<li>不小心还commit到版本库了，也可以用reset命令，结合hard mode，将工作区、暂存区全部恢复之前的版本。</li>
</ol>
<h2 id="2-4-创建版本库"><a href="#2-4-创建版本库" class="headerlink" title="2.4 创建版本库"></a>2.4 创建版本库</h2><p>首先，版本库有两种类型：</p>
<ul>
<li>本地版本库(local)，寄托在本地的机器，一般是个人电脑，为个人所用。</li>
<li>远端版本库(remote)，托管在远端的一个服务器上，被多个用户所使用。Github就是全球最大的托管Git版本库的服务器，从名字就可以知道。</li>
</ul>
<p>本地版本库的创建有两种方法：</p>
<ul>
<li>git init, 在本地初始化一个版本库。</li>
<li>git clone, 从远端克隆一个仓库到本地。</li>
</ul>
<p>当我们创建完一个本地版本库后，在本地的目录下就会多一个<code>.git</code>的文件夹，它就是版本库，里面就是管理整个目录的内容。</p>
<h2 id="2-5-版本库的同步"><a href="#2-5-版本库的同步" class="headerlink" title="2.5 版本库的同步"></a>2.5 版本库的同步</h2><p>远端仓库的创建非常简单，只需要在Github上创建即可，但是远端仓库创建之后，仓库内是空的，它只能通过本地版本库的同步来实现初始化。<br>远端仓库使得我们可以和其它用户一起合作，只需要保证远端的版本库永远是最新的版本即可，因此，本地和远端版本库之间就需要频繁地进行同步，主要通过三个动作来完成：push, pull, merge。<br>同步之前，我们需要绑定本地版本库到远端仓库，才能进行以后的同步操作。远端仓库默认叫做origin，当然也可以叫其它名字。<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git remote add origin &lt;remote repo addr&gt;</span><br></pre></td></tr></table></figure></p>
<h3 id="2-5-1-Git-push"><a href="#2-5-1-Git-push" class="headerlink" title="2.5.1 Git push"></a>2.5.1 Git push</h3><p>当我们想将本地版本库更新到远端仓库的时候，我们可以将其push到远端的仓库，这样就能使得远端仓库跟本地保持同步。<br><img src="/2018/08/15/Getting-Started-with-Git/push.png" class="" width="600" height="300"></p>
<h3 id="2-5-2-Git-pull"><a href="#2-5-2-Git-pull" class="headerlink" title="2.5.2 Git pull"></a>2.5.2 Git pull</h3><p>当你的队友将他的本地仓库push到你们共同的远端仓库，你需要对自己本地的仓库进行更新，跟你队友的仓库保持一致，这时候我们就需要将远端仓库pull到自己的本地仓库。<br><img src="/2018/08/15/Getting-Started-with-Git/pull.png" class="" width="600" height="300"></p>
<h3 id="2-5-3-Git-merge"><a href="#2-5-3-Git-merge" class="headerlink" title="2.5.3 Git merge"></a>2.5.3 Git merge</h3><p>无论是上传到远端仓库还是从远端仓库下载，远端仓库永远是最新版本，拥有最高优先级的。当我们想push我们的本地仓库到远程仓库的时候，不幸你的本地库的版本不是最新版本(即你在修改自己的本地版本的同时，有人更新了远程仓库)，这时候push会被拒绝，因为远程仓库的有些更新并不在你的本地仓库。这时候我们需要将远端仓库先pull下来，并和本地仓库进行合并(merge)。<br>merge就是指两个仓库(或者一个仓库的两个分支)进行合并的过程，在合并的过程，Git会自动地将另外一个仓库(分支)的改变更新到当前的仓库(分支)。<br>但是，合并的两个仓库(分支)在某些文件可能都共同地修改了同个地方，这时候就出现了所谓的conflit，Git并不会自动地帮你选择一个版本，而是将这个选择的权利交给了我们，Git会帮我们将conflit的地方标记出来，我们需要手动地进行修改，才能最后完成合并过程。</p>
<p>下面对比一下pull和fetch</p>
<ul>
<li>fetch：fetch是将远程仓库的某条分支的内容拉到本地，但是fetch后是看不到变化，而是在本地新开了一个分支，该分支的指针是<code>FETCH_HEAD</code>，checkout到该分支后可以查看远程分支的最新内容。然后切换到master分支，执行merge，选中<code>FETCH_HEAD</code>，合并后如果出现冲突则解决冲突，最后commit。</li>
<li>pull：pull相当于fetch和merge，自动将远程仓库更新到本地仓库<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git fetch origin master(将远程仓库的master分支拉到本地当前分支)</span><br><span class="line">$ git merge FETCH_HEAD</span><br></pre></td></tr></table></figure>
<br><h1 id="3-分支管理和协作"><a href="#3-分支管理和协作" class="headerlink" title="3. 分支管理和协作"></a>3. 分支管理和协作</h1>分支使得Git变得更加的强大，使得团队协作更加的方便。任何一个Git仓库，都会默认有一个master的分支，无论是本地仓库还是远程仓库，这个分支是在创建Git仓库的时候就会默认创建的。之前提到HEAD指针是指向最近一次commit的，但严格来讲，HEAD指针是指向master指针，master指针才是指向最近一次commit，所以可以理解为HEAD指针是指向当前的分支。<br>如果这样来看的话，一条分支就好比链表，每一次commit就往链表尾部插入新的仓库snapshot。一个仓库可以有好几条分支，当新开一条分支的时候，原有的链表尾部就会开始分叉，同时会有另外一个新的指针指向新的分叉，当我们切换分支的时候，HEAD指针就会指向对应分支的指针。<img src="/2018/08/15/Getting-Started-with-Git/branch.png" class="" width="500" height="250">
</li>
</ul>
<h2 id="3-1-创建、切换分支"><a href="#3-1-创建、切换分支" class="headerlink" title="3.1 创建、切换分支"></a>3.1 创建、切换分支</h2><p>Git创建分支和切换分支都很快，因为无非就是创建一个指针，和改变HEAD指针的指向而已。<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">创建branch：git branch &lt;branch_name&gt;</span><br><span class="line">切换branch：git checkout &lt;branch_name&gt;</span><br><span class="line">创建并切换branch：git checkout -b &lt;branch_name&gt;</span><br><span class="line">查看branch list：git branch</span><br></pre></td></tr></table></figure></p>
<h2 id="3-2-分支合并"><a href="#3-2-分支合并" class="headerlink" title="3.2 分支合并"></a>3.2 分支合并</h2><p>在多人协作的时候，基本上都是每个人都在自己的分支上工作，完成自己的工作之后再把自己的代码合并到master分支上。<br>之前提到，两条分支的merge可能会导致conflit，冲突的原因是两个<strong>已经提交的分支</strong>的相同文件相同位置的不同操作进行了合并。</p>
<p><strong>注意</strong><br>要避免冲突，就是最好每次修改文件之前，先merge别的分支(或者pull远程仓库)，这样就能保证自己是在别人最新版本的基础上修改的，自己修改完后去合并到别人分支(push到远程仓库)都不会产生冲突。<br>这种情况就好比下图，第三个节点是最新版本，然后在dev分支上修改，修改完成后commit，再切换回master分支，然后跟dev分支进行merge，这时候这种merge叫做fast forward，因为是直接将master的指针指向了第四个节点，相当于直接覆盖。<br><img src="/2018/08/15/Getting-Started-with-Git/ff_merge.png" class="" width="500" height="250"></p>
<p>以下的情况会导致冲突，假如我在dev分支修改了a文件的第二行代码，并且提交了。然后我切换到master分支，假如我不知道dev分支改了什么，我在master分支也改了a文件的第二行代码，并且提交了。然后我在这个时候想把dev分支的修改一起merge到master分支上，冲突发生了，因为两个版本都修改了同一行代码，Git会将冲突的位置，用以下的方式告知我们，并要求我们人工进行修改。如果不解决冲突时没法提交或者切换分支的。<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD(Current Change)</span><br><span class="line">other code</span><br><span class="line">========</span><br><span class="line">your code</span><br><span class="line">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; your branch name(Incoming Change)</span><br></pre></td></tr></table></figure></p>
<p>但是假如我在master分支改的时a文件的第三行代码，并且提交了，这时候再去将dev分支merge进来，不会有冲突，而是会自动合并，即使我在修改第三行代码前并没有先将dev的修改merge进来。但是还是建议先merge再做修改。</p>
<h2 id="3-3-与远端仓库的同步"><a href="#3-3-与远端仓库的同步" class="headerlink" title="3.3 与远端仓库的同步"></a>3.3 与远端仓库的同步</h2><p>像之前所说，无论是本地仓库还是远端的仓库，都可以存在不同的分支，还是那句话，在修改代码前，先将远端仓库pull下来，方便以后push的时候，避免产生冲突。并且需要注意，本地仓库是从哪条分支pull下来的，最好就push回哪条分支，不然push不上去。<br>如果本地仓库和远端仓库都只有一条分支，那么情况就简单很多，因为不需要明确地指明哪条分支到哪条分支。</p>
<ul>
<li>pull：如果本地和远端都只有一条分支，直接git pull就好，如果想pull到当前分支，那么本地分支名可以省略。  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git pull [远程仓库名字，一般默认origin] [远端分支名]:[本地分支名]</span><br><span class="line">eg: git pull origin master:master</span><br></pre></td></tr></table></figure></li>
<li>fetch：fetch不需要写本地分支名，因为它还没有merge，只是把远端分支拉到本地并保存到<code>FETCH_HEAD</code>而已。  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git fetch [远程仓库名字] [远端分支名]</span><br></pre></td></tr></table></figure></li>
<li>push：建议写全。<ul>
<li>最常见到的是<code>git push origin master</code>，远程分支被省略，它表示将本地分支推送到与其存在追踪关系的远程分支(通常两者同名)，因为远端仓库肯定存在master分支，因此省略也没有问题。如果该远程分支不存在，则会被新建。</li>
<li>其它形式如<code>git push origin</code>，表示将当前分支push到远端与当前分支存在追踪关系的分支。</li>
<li><code>git push</code>，如果本地和远端都只有一条分支，那么全都可以省略。<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git push [远程仓库名字] [本地分支名]:[远端分支名]</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<h2 id="3-4-Stash"><a href="#3-4-Stash" class="headerlink" title="3.4 Stash"></a>3.4 Stash</h2><p>Stash是一个工作状态保存栈，用于保存/恢复工作区的临时状态。<br><img src="/2018/08/15/Getting-Started-with-Git/stash.png" class="" width="500" height="250"></p>
<p>那么什么时候才需要将工作区的状态暂时保存起来呢？说到暂时，那么肯定就是修改只进行了一半，还没到commit或者stage的地步。比如我在master上进行了一些修改，但是还没有commit(加到暂存区也不行)，现在我需要切换到dev分支进行其它的修改。Git会reject你的分支切换(另一种reject分支切换的情况是出现conflit，conflit没解决之前，不允许切换分支)，并且告诉你要不将修改commit，要不将它放到stash里面，才可以切换分支。这就是stash出现的目的，暂时存储工作区的状态。<br>将工作区临时保存起来可以用<code>git stash</code>命令，实际保存的是工作区的一个snapshot，将工作区stash之后，工作区变回干净状态(从git status可以看出)。因此可以多次stash，相当于将不同的几个snapshot保存起来，stash的地方是一个栈，遵循后进先出。<br><code>git stash list</code>可以查看栈里面的snapshot。如果要恢复工作区可以有两种方法：</p>
<ul>
<li><code>git stash pop</code>：将栈顶元素pop出来，恢复工作区的同时把stash的内容删掉。</li>
<li><code>git stash apply stash@&#123;n&#125;</code> &amp; <code>git stash drop stash@&#123;n&#125;</code>：从stash list选出需要恢复的snapshot，snapshot的命名就是<code>stash@&#123;n&#125;</code>。</li>
</ul>
<p><br></p>
<h1 id="4-GitHub之pull-request"><a href="#4-GitHub之pull-request" class="headerlink" title="4. GitHub之pull request"></a>4. GitHub之pull request</h1><p>当我们在进行团队任务的时候，每个人都会在本地拥有自己的一条分支，同理，在远端也会有一条与之对应的分支，如feature分支、bug fix分支等，当我们在本地完成自己的任务，然后push到远端自己分支之后，我们准备将它merge到master分支(master分支永远是最新的版本)，首先我们不可能直接将自己本地的分支push到远端的master，因为万一出错了就会很麻烦，因此我们只会push到自己的分支，那么怎样能够更新远端的master分支呢？<br>这就是pull request要做的事了，字面上理解pull request是指请求合并代码，或者请求对方pull一下自己的代码。pull request可以发生在一个仓库的不同分支，也可以发生在两个不同的仓库，但是这两个不同的仓库必须存在fork的关系，fork是指将别人的仓库copy到自己的仓库。</p>
<p>假设你想为一些开源项目做一下贡献</p>
<ol>
<li>将这个项目fork到自己的GitHub，并将它clone到本地(注意是自己GitHub上的那个仓库)</li>
<li>创建新分支，进行代码修改</li>
<li>push到自己GitHub上对应的分支</li>
<li>发起pull request，pull request里有base和compare，base就是别人的代码，compare是指自己修改后的代码，在这里base要选择开源项目仓库的master分支，compare选自己仓库的那条分支。</li>
<li>对方进行代码审核，如果通过了，就会进行代码合并。<br><br><img src="/2018/08/15/Getting-Started-with-Git/pr.jpg" class="" width="500" height="250">
</li>
</ol>
<p><br></p>
<h1 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h1><p>Git可以说是每一个开发者必备的工具，而Github更是全世界最活跃的网站之一，无论你是将Github看成是一个项目代码的仓库，还是在公司跟同事合作，掌握Git都会让你受益匪浅，个人推荐用Git Bash，不要依赖Github Desktop，虽然方便，但是沉下心来理解Git的基本概念和操作，也是每个开发者值得做和应该做的一件事。</p>
<p><br></p>
<h1 id="6-参考资料"><a href="#6-参考资料" class="headerlink" title="6. 参考资料"></a>6. 参考资料</h1><ol>
<li><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000">廖雪峰Git教程</a></li>
<li><a href="https://backlog.com/git-tutorial/">backlog git tutorial</a></li>
<li><a href="https://git-scm.com/docs/">Git官网</a>，但是太难懂了，我暂时也没看懂&gt;_&lt;</li>
</ol>
<p><br></p>
]]></content>
      <categories>
        <category>Git</category>
      </categories>
  </entry>
  <entry>
    <title>Get Started With PyTorch</title>
    <url>/2018/11/05/Get-Started-With-PyTorch/</url>
    <content><![CDATA[<img src="/2018/11/05/Get-Started-With-PyTorch/header.jpg" class="" width="600" height="300">
<p>该文章介绍PyTorch的基本数据结构，以及如何用PyTorch搭建一个神经网络</p>
<span id="more"></span>
<p><em>introduction</em></p>
<blockquote>
<p>PyTorch is a Python-based scientific computing package targeted at two sets of audiences: </p>
<ul>
<li>A replacement for NumPy to use the power of GPUs</li>
<li>a deep learning research platform that provides maximum flexibility and speed</li>
</ul>
</blockquote>
<p>PyTorch对比与Tensorflow最大的特点在于它建立的神经网络是动态的，而Tensorflow建立的计算图是静态的，这使得Tensorflow在RNN上会有一点被动。</p>
<p><br></p>
<h1 id="PyTorch的数据基础"><a href="#PyTorch的数据基础" class="headerlink" title="PyTorch的数据基础"></a>PyTorch的数据基础</h1><p>正如官方所说，PyTorch是用来替代Numpy的，自然它就会继承Numpy的一些特点，torch自称是神经网络界的Numpy，因为它能将torch产生的tensor放在GPU中加速计算，正如Numpy可以将array放在CPU中加速运算。</p>
<h2 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h2><p>torch最基本的数据形式就是tensor，是一个多维度的张量，torch中的tensor与Numpy中的array具有很多的相似性。</p>
<ul>
<li>两者在创建上基本一致</li>
<li>两者可以通过函数相互转换</li>
<li>两者都有相同的数学运算</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建</span></span><br><span class="line">numpy_data = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line">torch_data_long = torch.LongTensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line">torch_data_float = torch.FloatTensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 相互转换</span></span><br><span class="line">numpy_data = torch_data_long.numpy()</span><br><span class="line">torch_data = torch.from_numpy(numpy_data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看维度</span></span><br><span class="line">numpy_shape = numpy_data.shape</span><br><span class="line">torch_shape = torch_data.size()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 矩阵相乘</span></span><br><span class="line">numpy_mul = np.dot(numpy_data, numpy_data.T)</span><br><span class="line">torch_mul = torch.mm(torch_data, torch_data.transpose(<span class="number">0</span>,<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p><strong>Note: </strong> <code>torch.dot()</code>只能针对于一维tensor，即内积；而<code>np.dot()</code>可以用来做矩阵乘法</p>
<p>下面列举一些常见的API</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 张量转置，a、b为相互交换位置的维度</span></span><br><span class="line">torch_data.transpose(a,b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 张量压缩和扩张，压缩的意思是指：将维度为1的维度去掉，如(3,1,4) -&gt; (3,4)；而扩张就是正好相反</span></span><br><span class="line">torch_data.squeeze(axe)</span><br><span class="line">torch_data.unsqueeze(axe)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 张量reshape</span></span><br><span class="line">torch_data.view(axe1, axe2, ...)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 张量批乘法，batch matrix multiplication，在神经网络里面，一般采取批训练，</span></span><br><span class="line"><span class="comment">#因此数据大小一般是(batch, a, b)，bmm会无视batch维度进行乘法，</span></span><br><span class="line"><span class="comment">#(batch, a, b) × (batch, b, c) -&gt; (batch, a, c)</span></span><br><span class="line">torch_data.bmm(torch_data1)</span><br></pre></td></tr></table></figure>
<p><br></p>
<h2 id="Variable"><a href="#Variable" class="headerlink" title="Variable"></a>Variable</h2><p><code>Variable</code>来自<code>torch.autograd</code>，Variable是神经网络的基础，它好比神经网络的参数，会变化。而实际上Variable描述的是一个计算的操作，多个Variable之间就构成一个计算图。Variable的属性如下：</p>
<ul>
<li>data：Tensor类型，Variable中存放的数据</li>
<li>grad：梯度，由于Variable之间会通过数学运算进行相连，如神经网络，构成一个计算图，在反向传递的时候，就会计算计算图中每一个节点的梯度，并存放到grad里</li>
<li>creator：创建这个Variable的操作，如加法、矩阵乘法等</li>
</ul>
<p>Tensor和Variable区别在于：tensor只是一个数据形式，而Variable是tensor的container，是变量，会变化</p>
<p><strong>Note：</strong> 最新版的torch里面，已经模糊掉了Variable的概念了，因此tensor和Variable已经没有区别，<code>torch.tensor</code>和<code>torch.autograd.Variable</code>已经是同一个类了，tensor可以追踪历史。</p>
<p><br></p>
<h1 id="PyTorch搭建神经网络"><a href="#PyTorch搭建神经网络" class="headerlink" title="PyTorch搭建神经网络"></a>PyTorch搭建神经网络</h1><p>pytorch搭建和训练神经网络的方式，对比Keras来说，是复杂了一点点，因为Keras是比较high-level的封装，因此有些东西也无法实现，下面讲述一下如何搭建一个神经网络。</p>
<h2 id="搭建神经网络"><a href="#搭建神经网络" class="headerlink" title="搭建神经网络"></a>搭建神经网络</h2><p>对于小型的网络，可以用一个Sequential将网络框住从而实现搭建，Sequential就好比Keras里面的序贯模型，顺序执行每一层。<code>torch.nn</code> 封装了所有的神经网络层，包括全连接层，卷积层，池化层等。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = torch.nn.Sequential(torch.nn.Linear(<span class="number">40</span>, <span class="number">20</span>),</span><br><span class="line">                          torch.nn.Dropout(<span class="number">0.8</span>),</span><br><span class="line">                          torch.nn.ReLu(),</span><br><span class="line">                          torch.nn.Linear(<span class="number">20</span>, <span class="number">5</span>),</span><br><span class="line">                          torch.nn.Softmax()</span><br><span class="line">                         )</span><br></pre></td></tr></table></figure>
<p>而传统的方法是，通过创建一个继承<code>torch.nn.Module</code>的类，来实现搭建工作。<code>torch.nn.functional</code> 封装了所有神经网络中的激活函数，注意区分 <code>torch.nn.ReLu()</code> 和 <code>torch.nn.functinoal.relu(x)</code>，前者是一个层，后者是一个函数。</p>
<p>自定义的类里面，<code>__init__</code> 函数和 <code>forward</code> 函数是必须要重载的，初始化函数定义网络中的参数，以及可以定义一些网络层；<code>forward</code> 函数定义前向传递的过程，返回输出值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(torch.nn.module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_feature, n_hidden, n_output</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.hidden_layer = torch.nn.Linear(n_feature, n_hidden)</span><br><span class="line">        self.output_layer = torch.nn.Linear(n_hidden, n_output)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.relu(self.hidden_layer(x))</span><br><span class="line">        y_preds = F.softmax(self.output_layer(x))</span><br><span class="line">        <span class="keyword">return</span> y_preds</span><br><span class="line">    </span><br><span class="line">net = Net(<span class="number">40</span>, <span class="number">20</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<p><br></p>
<h2 id="训练神经网络"><a href="#训练神经网络" class="headerlink" title="训练神经网络"></a>训练神经网络</h2><p>训练神经网络的步骤基本是</p>
<ol>
<li>定义损失函数和优化器</li>
<li>迭代进行前向传递，以及误差求导，反向传递更新参数</li>
</ol>
<p>在PyTorch里面，这些都需要自己去实现，不像Keras，只需要一个函数即可。优化器封装在<code>torch.optim</code> 里面，常用的有SGD、Adam，我们需要将网络里面需要更新的变量传给优化器，<code>net.parameters()</code> 返回网络中的Variable；损失函数常用的有MSE、CrossEntropy；</p>
<p>每一个epoch的步骤：</p>
<ol>
<li>计算网络输出值，然后计算loss，注意loss本身也是一个Variable；</li>
<li><code>optimizer.zero_grad()</code> 是将网络中的变量的grad都清零；</li>
<li><code>loss.backward()</code> 是进行反向传递，算出计算图里面每个节点的梯度；</li>
<li><code>optimizer.step</code> 是更新网络中的所有变量。</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.1</span>)</span><br><span class="line">loss_func = torch.nn.MSELoss()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    y_preds = net(x)</span><br><span class="line">    loss = loss_func(y_preds, y)</span><br><span class="line">    <span class="comment"># 下面的三步是训练网络必须有的</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>
<p>训练的过程实际上就是不断更新网络中变量参数的过程，训练完成后，可以将网络保存下来，保存的方式有两种，一种是将整个网络连带训练好的参数一起保存下来，另一种是只保留训练好的参数，这种方法我们在load的时候，需要先构造好一个完全一模一样的网络，然后再将参数load进去</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># save</span></span><br><span class="line">torch.save(net, path1)</span><br><span class="line">torch.save(net.state_dict(), path2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># load</span></span><br><span class="line">net = torch.load(path1)</span><br><span class="line">net1 = Net(<span class="number">40</span>, <span class="number">20</span>, <span class="number">5</span>)</span><br><span class="line">net1.load_state_dict(torch.load(path2))</span><br></pre></td></tr></table></figure>
<p><br></p>
<h2 id="搭建高级神经网络"><a href="#搭建高级神经网络" class="headerlink" title="搭建高级神经网络"></a>搭建高级神经网络</h2><p>所谓高级的神经网络，无非就是CNN和RNN，具体搭建方法和上面一样，只是网络的层改变了而已，具体的网络层参数可以参考官方文档。而另外forward函数里面可以进行很多操作，很多网络结构并不是序贯模型，因此forward里面可以进行各种数学运算，如计算attention weight等。</p>
<p>由于CNN和RNN已经很普遍了，我在这里展示一下auto-encoder的实现，auto-encoder是一个无监督的模型，将输入数据进行encoder压缩，得到encoded representation，再通过decoder解压</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">AutoEncoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(AutoEncoder, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 压缩</span></span><br><span class="line">        self.encoder = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">28</span>*<span class="number">28</span>, <span class="number">128</span>),</span><br><span class="line">            nn.Tanh(),</span><br><span class="line">            nn.Linear(<span class="number">128</span>, <span class="number">64</span>),</span><br><span class="line">            nn.Tanh(),</span><br><span class="line">            nn.Linear(<span class="number">64</span>, <span class="number">12</span>),</span><br><span class="line">            nn.Tanh(),</span><br><span class="line">            nn.Linear(<span class="number">12</span>, <span class="number">3</span>)   <span class="comment"># 压缩成3个特征, 进行 3D 图像可视化</span></span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 解压</span></span><br><span class="line">        self.decoder = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">3</span>, <span class="number">12</span>),</span><br><span class="line">            nn.Tanh(),</span><br><span class="line">            nn.Linear(<span class="number">12</span>, <span class="number">64</span>),</span><br><span class="line">            nn.Tanh(),</span><br><span class="line">            nn.Linear(<span class="number">64</span>, <span class="number">128</span>),</span><br><span class="line">            nn.Tanh(),</span><br><span class="line">            nn.Linear(<span class="number">128</span>, <span class="number">28</span>*<span class="number">28</span>),</span><br><span class="line">            nn.Sigmoid()      </span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        encoded = self.encoder(x)</span><br><span class="line">        decoded = self.decoder(encoded)</span><br><span class="line">        <span class="keyword">return</span> encoded, decoded</span><br><span class="line"></span><br><span class="line">autoencoder = AutoEncoder()</span><br><span class="line">optimizer = torch.optim.Adam(autoencoder.parameters(), lr=LR)</span><br><span class="line">loss_func = nn.MSELoss()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(EPOCH):</span><br><span class="line">    <span class="keyword">for</span> step, (x, b_label) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">        b_x = x.view(-<span class="number">1</span>, <span class="number">28</span>*<span class="number">28</span>)   <span class="comment"># batch x, shape (batch, 28*28)</span></span><br><span class="line">        b_y = x.view(-<span class="number">1</span>, <span class="number">28</span>*<span class="number">28</span>)   <span class="comment"># batch y, shape (batch, 28*28)</span></span><br><span class="line"></span><br><span class="line">        encoded, decoded = autoencoder(b_x)</span><br><span class="line"></span><br><span class="line">        loss = loss_func(decoded, b_y)      </span><br><span class="line">        optimizer.zero_grad()               </span><br><span class="line">        loss.backward()                    </span><br><span class="line">        optimizer.step()                    </span><br></pre></td></tr></table></figure>
<p><br></p>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ol>
<li><a href="https://pytorch.org/tutorials/">Official PyTorch Tutorials</a>，官方有很多用torch实现的例子，包括图像和文字方面的应用，以及GAN和RL。</li>
<li><a href="https://morvanzhou.github.io/tutorials/machine-learning/torch/">Morvan PyTorch Tutorials</a></li>
</ol>
<p><br></p>
]]></content>
  </entry>
  <entry>
    <title>Introduction to Conditional GAN</title>
    <url>/2019/08/19/Introduction-to-Conditional-GAN/</url>
    <content><![CDATA[<blockquote>
<p>课程来源：李宏毅2018生成对抗网络课程<br>课程主页：<a href="http://speech.ee.ntu.tw/~tlkagk/courses_MLDS18.html">http://speech.ee.ntu.tw/~tlkagk/courses_MLDS18.html</a><br>文章梗概：本文介绍了条件对抗生成网络，与一般的GAN不同，条件GAN的输入并不是随机向量，而是条件。</p>
</blockquote>
<span id="more"></span>
<p><br></p>
<p><em>preface</em></p>
<p>在最原始的 GAN 的里面，生成器接收一个随机的 noise，然后随机地生成一张图片，再交给判别器去判断该生成图片是否 realistic；假如现在我们不想让生成器随机地生成图片，而是根据我们的输入对应的生成图片，那就需要在生成器的输入加入额外的元素。</p>
<h2 id="Text-to-Image"><a href="#Text-to-Image" class="headerlink" title="Text-to-Image"></a>Text-to-Image</h2><p>传统监督的 text-to-image 的方法，需要找到 text 和 image 的 pair，然后训练一个网络使得生成的 image 和 ground true 的 image 越像越好。而 GAN 的方法则是在原有 GAN 的基础上加入对应的 text。</p>
<img src="/2019/08/19/Introduction-to-Conditional-GAN/3.png" class="" width="550" height="550">
<p>而判别器除了要判别出生成图和真实图之外，还要判断真实图和文字是否 match，因此对于判别器的二分类问题，negative examples 有两种，一种是 text 和 generated image，另一种是 text 和真实图片不 match。</p>
<img src="/2019/08/19/Introduction-to-Conditional-GAN/1.png" class="" width="550" height="550">
<p>而对于判别器的架构，有两种架构：</p>
<ul>
<li>将条件 c 和 object 一起输入到一个网络里面进行判断，输出一个分数。</li>
<li>将两种 negative example 分开，一个网络判断是否 realistic，另一个网络判断是否 match。</li>
</ul>
<img src="/2019/08/19/Introduction-to-Conditional-GAN/2.png" class="" width="550" height="550">
<p><em>In Conclusion</em><br>ConditionalGAN 简单来说就是在生成器和判别器的输入分别加入 condition。</p>
<p><br></p>
]]></content>
  </entry>
  <entry>
    <title>Git常用命令</title>
    <url>/2018/08/18/Git%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[<img src="/2018/08/18/Git%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/header.jpg" class="" width="500" height="250">
<p>该文章总结了最常用的Git命令，包括创建仓库、更新仓库、与远端仓库同步等。<br>参考了文件 <a href="/2018/08/18/Git%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/git-cheatsheet.pdf" title="git-cheatsheet.pdf">git-cheatsheet.pdf</a>，并在此基础上进行了一定的补充。</p>
<span id="more"></span>
<ul>
<li><p>创建仓库</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git init</span><br></pre></td></tr></table></figure>
</li>
<li><p>从远端克隆仓库</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git clone &lt;repo addr&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看工作区和暂存区的状态。该命令会返回相比于暂存区，工作区的状态改变信息，如new file，modified，deleted file等</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git status</span><br></pre></td></tr></table></figure>
</li>
<li><p>将工作区的代码添加到暂存区，为commit做准备</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git add [参数] [路径]</span><br><span class="line">$ git add . : 将修改的文件和新添加的文件stage到暂存区，不包括删除的文件</span><br><span class="line">$ git add --all(-A)：将所有已跟踪的文件的修改与删除和新添加的未跟踪的文件添加到暂存区</span><br></pre></td></tr></table></figure>
</li>
<li><p>将暂存区的代码，提交到本地版本库。-m 参数表示可以直接输入后面的“message”，如果不加 -m参数，那么是不能直接输入message的，而是会调用一个编辑器一般是vim来让你输入这个message</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git commit -m &lt;commit message&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>追加commit，这是直接在上一次的commit基础上进行modify，而不会新开一个commit id，该命令比较少用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git commit --amend</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看不同区域状态的差异</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git diff: 查看工作区和暂存区的差别</span><br><span class="line">$ git diff --cached: 查看暂存区和版本库的差别</span><br><span class="line">$ git diff HEAD: 查看工作区和版本库的差别</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看历史的commit记录，可以添加参数，比如缩写(正常的commit id很长)，还可以让它一条commit显示一行</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git log (--pretty=oneline) --abbrev</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看所有分支的commit记录，包括commit，reset等，回退后的commit在git log是看不到的，而git reflog可以</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git reflog</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看当前仓库的分支，参数<code>-r</code>表示查看远端仓库的分支</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git branch</span><br><span class="line">$ git branch -r</span><br></pre></td></tr></table></figure>
</li>
<li><p>新建一条分支</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git branch &lt;branch name&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>切换分支</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git checkout &lt;branch name&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>新建并切换分支</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git checkout -b &lt;branch name&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>删除分支，参数-d还是-D，取决于这条分支的改变有没有merge到其它分支</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git checkout -d(D) &lt;branch name&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>撤销commit</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git revert &lt;刚刚提交的commit id&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>版本回退，mode一般都会选择hard</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git reset &lt;mode&gt; &lt;commit id或者HEAD指针倒退&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>撤销工作区某个文件的修改</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git checkout -- &lt;file name&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>撤销stage</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git reset HEAD &lt;file&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>合并分支，默认是—ff，fast forward，也可以选择—no-ff，那就会新添加一个commit，名为merged</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git merge &lt;mode&gt; &lt;branch name&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>绑定远端仓库，仓库名一般就叫origin就ok</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git add remote &lt;远端仓库名&gt; &lt;仓库地址&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>更新远端仓库</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git push &lt;远端仓库名&gt; &lt;本地分支名&gt;:&lt;远端分支名&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>更新本地仓库</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git pull &lt;远端仓库名&gt; &lt;远端分支名&gt;:&lt;本地分支名&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>将远端仓库拉到本地</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git fetch &lt;远端仓库名&gt; &lt;远端分支名&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>将工作区暂时保存</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git stash</span><br></pre></td></tr></table></figure>
</li>
<li><p>恢复工作区</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git stash pop</span><br><span class="line">$ git stash apply stash@&#123;n&#125; + git stash drop stash@&#123;n&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><br></p>
]]></content>
      <categories>
        <category>Git</category>
      </categories>
  </entry>
  <entry>
    <title>Incremental Learning with Knowledge Distillation</title>
    <url>/2019/11/30/Incremental-Learning-with-Knowledge-Distillation/</url>
    <content><![CDATA[<p>本文将介绍增量学习 (incremental learning) 和知识蒸馏相结合的文章</p>
<ul>
<li>Large Scale Incremental Learning</li>
<li>End-to-End Incremental Learning</li>
<li>iCaRL: Incremental Classiﬁer and Representation Learning</li>
</ul>
<span id="more"></span>
<p><em>preview</em></p>
<p>增量学习：传统的学习策略是，给定一堆数据集，然后训练一个模型，数据集限定了模型需要学习的东西；但人类的学习策略是渐进的，我们可能会先从一批数据中学习出一些知识，然后再从另外一批数据中学习另外的知识并且不会忘记之前的知识。但是到了深度学习模型，假如我们先让模型学习一个数据集 A，再让模型学习数据集 B（这时候数据集 A 已经不可获取），这时候模型就会彻底忘记数据集 A 中学到的知识，这个称为灾难性遗忘 (catastrophic forgetting)，是增量学习中所要解决的问题。增量学习是一种渐进式的学习策略，后面学习的时候，前面学习用到的数据集不可获取，这是增量学习的基本背景，如何能够在新数据集下引入旧数据集，或者如何能够保存旧数据集下学习到的知识，成为关键。</p>
<p>增量学习又分为数据增量、特征增量和类别增量。数据增量比较好理解，就是增加了额外的数据继续训练；特征增量指在特征数量增多；类别增量指类别数量增多，比如初代模型 $M_0$ 学习到 10 个类别，第二批数据为另外的 10 个类，我们要求第二代模型 $M_1$ 要学习到 20 个类别，既要记住之前学习的 10 个类别，也要学会新的 10 个类别。</p>
<p>我们重点关注的是类别增量，因为标准数据集 CIFAR-100 和 ImageNet 天然就是一个类别增量的数据集，我们可以 10 类一个单位的增加。类增量的算法围绕如何在新数据集学习下引入旧数据的信息，其主要有三类：</p>
<ul>
<li>不采用旧数据</li>
<li>用合成数据，训练一个生成模型去生成一些旧数据，然后混合新数据一起训练。</li>
<li>用少量的旧数据作为 exemplar，从旧数据中挑选出一些代表数据 exemplar，然后混合新数据一起训练。</li>
</ul>
<p>在研究一个增量学习算法时，我们要时刻关注这四个方面：旧数据，新数据，旧模型，新模型。旧数据如何引入到新模型？旧模型的参数如何辅助新模型训练？旧模型和新模型之间是否存在共用参数？</p>
<p>下面我们看看这 3 篇论文如何解决上面提到的这些问题。</p>
<p><br></p>
<h1 id="Large-Scale-Incremental-Learning"><a href="#Large-Scale-Incremental-Learning" class="headerlink" title="Large Scale Incremental Learning"></a>Large Scale Incremental Learning</h1><blockquote>
<p>Conference: CVPR2019<br>Authors: Yue Wu, Yinpeng Chen, et al</p>
</blockquote>
<h2 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h2><p>整体的流程如下，首先我们先看数据方面，可以看出该方法属于上面提到的第三类方法，即保存少部分的旧数据作为 exemplar，然后混合新数据一起训练，数据集分为训练集和验证集，训练的过程作者分为两个阶段，第一阶段新旧数据一起训练，第二阶段为 bias correction，用验证集去训练。</p>
<img src="/2019/11/30/Incremental-Learning-with-Knowledge-Distillation/1.png" class="" width="500" height="500">
<h3 id="Stage-I-baseline-method"><a href="#Stage-I-baseline-method" class="headerlink" title="Stage I: baseline method"></a>Stage I: baseline method</h3><p>我们先看下第一阶段的训练过程，如下图，可以看出新模型和旧模型并不是共享某些层的，新模型需要重新训练，但他们的结构基本相同，除了最后的 FC 层的类别数量，旧模型输出为 n 类的概率值，新模型输出的是 n+m 类的概率值</p>
<img src="/2019/11/30/Incremental-Learning-with-Knowledge-Distillation/2.png" class="" width="500" height="500">
<p>回答上面提到的问题，旧模型和旧数据如何利用？</p>
<ul>
<li>新数据为 $X^m={(x_i, y_i), 1\le i\le M, y_i \in [n+1, …,n+m]}$</li>
<li>旧数据 exemplar 为 $\hat{X}^n={(\hat{x}_j, \hat{y}_j), 1\le j\le N_s, \hat{y}_j \in [1, …,n]}$</li>
<li>新模型的输出 $o^{n+m}(x)=[o<em>1(x), …,x_n(x), o</em>{n+1}(x),…,o_{n+m}(x)]$</li>
<li>旧模型的输出 $\hat{o}^n(x)=[\hat{o}_1(x), …,\hat{o}_n(x)]$\</li>
</ul>
<p>我们把旧模型作为 teacher 模型，进行知识蒸馏，注意这里蒸馏的时候只在前面 n 个类别上进行，并且用到的是全部数据，即新数据和旧数据，有人可能会问为什么新数据也要进行蒸馏，旧模型是在旧类别上训练的，但是将新数据输入到旧模型里面，我们依旧能得到新数据在就类别上的预测概率值，可以知道新图片在旧类别上的概率分布，这对于新模型 FC 层前 n 个神经元的训练还是有所帮助的。</p>
<p>利用蒸馏，我们可以一定程度上缓解灾难性遗忘，把旧数据在旧模型输出的 logit 迁移到新模型 FC 层的前 n 个神经元上</p>
<img src="/2019/11/30/Incremental-Learning-with-Knowledge-Distillation/3.png" class="" width="500" height="500">
<p>除了蒸馏损失，分类损失也是必须的，因为我们现在有新数据还有旧数据的 exemplar，所以直接将它在新模型上训练，这里训练的是 FC 层的所有神经元。</p>
<img src="/2019/11/30/Incremental-Learning-with-Knowledge-Distillation/4.png" class="" width="500" height="500">
<p>我们看一下新模型 FC 层的 n+m 个神经元分别受到哪些监督信息</p>
<ul>
<li>前 n 个神经元：新旧数据在旧模型输出 logit 的知识蒸馏，旧数据 exemplar 的 ground-true 信息</li>
<li>后 m 个神经元：新数据的 ground-true 信息</li>
</ul>
<p>增量学习需要解决的就是如何使得新模型的前 n 个神经元能够保存旧类的分类能力，这里采取的是蒸馏的方法。</p>
<p>上面的方法是把蒸馏用到增量学习的 baseline 方法，但由于新旧类数据量不均衡，新类的数据要远多于旧类的 exemplar，这会导致 FC 层会更多的偏向于后面 m 个神经元，因为后面 m 个神经元受到的监督信息更多，虽然前面 n 个神经元我们已经尽力去补充监督信息，但依旧无法缓解 bias 的问题，从下图可以看出，当在做最后一次增量学习的时候（100 类别，每次增加 20 类），预测的类别多数都在 81-100 类。</p>
<img src="/2019/11/30/Incremental-Learning-with-Knowledge-Distillation/5.png" class="" width="300" height="300">
<p>作者猜测这是由于数据不均衡所导致的，因此做了个小实验，先用上面的这个 baseline 方法训练一个新模型（这个模型 FC 层存在 bias 问题），然后再用全部的新旧数据（该数据不存在类别不均衡问题）去 fine-tune 最后的 FC 层，发现 bias 问题被解决，分类准确率也提升了。但是现实情况，我们肯定是不可能获取得到全部的旧数据的，因此作者提出一种 bias correction (biC) 的方法去调整 FC 层。</p>
<img src="/2019/11/30/Incremental-Learning-with-Knowledge-Distillation/6.png" class="" width="300" height="300">
<h3 id="Stage-II-bias-correction"><a href="#Stage-II-bias-correction" class="headerlink" title="Stage II: bias correction"></a>Stage II: bias correction</h3><p>首先需要把数据集划分为训练集和验证集，注意验证集新旧类别必须均衡，可以看上面的总流程图，由于验证集很小，新旧类别的数据都很少，所以作者在 FC 层后加了一个线性模型来矫正 bias，对于 FC 层前 n 个输出，保持原值，对于后 m 个输出，进行矫正</p>
<img src="/2019/11/30/Incremental-Learning-with-Knowledge-Distillation/12.png" class="" width="400" height="400">
<p> 当进行 bias correction 的时候，前面的特征提取层和 FC 层都是固定的，只训练线性模型，loss 函数如下</p>
<img src="/2019/11/30/Incremental-Learning-with-Knowledge-Distillation/13.png" class="" width="400" height="400">
<p><br></p>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>实验设置基本和 EEIL 和 iCaRL 一样（下面讨论这两篇文章），作者分别在大数据集 ImageNet 和小数据集 CIFAR-100 上测试了结果，实验表明 bias correction 这种方法都比 EEIL 和 iCaRL 要好，但是在大数据集上的提升会更加明显</p>
<h3 id="ImageNet"><a href="#ImageNet" class="headerlink" title="ImageNet"></a>ImageNet</h3><img src="/2019/11/30/Incremental-Learning-with-Knowledge-Distillation/7.png" class="" width="700" height="700">
<img src="/2019/11/30/Incremental-Learning-with-Knowledge-Distillation/8.png" class="" width="300" height="300">
<h3 id="CIFAR-100"><a href="#CIFAR-100" class="headerlink" title="CIFAR-100"></a>CIFAR-100</h3><img src="/2019/11/30/Incremental-Learning-with-Knowledge-Distillation/9.png" class="" width="700" height="700">
<p><br></p>
<h1 id="End-to-End-Incremental-Learning"><a href="#End-to-End-Incremental-Learning" class="headerlink" title="End-to-End Incremental Learning"></a>End-to-End Incremental Learning</h1><blockquote>
<p>Conference: ECCV2018<br>Authors: Francisco M. Castro, et al</p>
</blockquote>
<p>EEIL 同样是基于 exemplar 的方法，在上面的文章，我们侧重说了蒸馏如何用到增量学习上，这篇文章的蒸馏思路和上篇基本类似，这里我们侧重说下 exemplar 的选取。</p>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p>流程图如下，训练分为四步：</p>
<ul>
<li>构造训练集</li>
<li>蒸馏训练</li>
<li>balanced fine-tune</li>
<li>构造 exemplar set</li>
</ul>
<img src="/2019/11/30/Incremental-Learning-with-Knowledge-Distillation/10.png" class="" width="600" height="600">
<h3 id="exemplar"><a href="#exemplar" class="headerlink" title="exemplar"></a>exemplar</h3><p>作者提出了两种策略，分别为固定总的 memory size 和 固定每个类别的 memory size，前者随着增量学习的进行，每个旧类的样本数量会减少，后者随着增量学习的进行，内存会增大。</p>
<p>训练前，先将新数据和旧数据进行融合，构造出训练集，训练完后，将新数据加入到 memory，如何挑取加入到 memory 的样本，作者提出了两种方法</p>
<ul>
<li>随机策略</li>
<li>herding selection：计算该类别样本的均值，计算每个样本距离均值的距离，选出最靠近均值的样本</li>
</ul>
<p>新数据的加入必然会导致旧数据样本数量的减少，因为旧数据也是通过 herding 的方式加入到内存的，在移除旧数据的时候，同样采取该策略，移除距离均值最大的样本。</p>
<h3 id="training-process"><a href="#training-process" class="headerlink" title="training process"></a>training process</h3><p>训练过程，旧模型输出的是 n 个类别的 logit，作为 teacher 模型，新模型输出的是 n+m 个类别的 logit，同样分为两个 loss</p>
<ul>
<li>cross entropy loss：新旧数据同时用上来训练新模型</li>
<li>distillation loss：和上一篇文章唯一不同的是，这里它只用了旧数据，旧数据在旧模型输出的 logit 作为软标签，指导新模型 FC 层的前 n 个神经元。而上篇文章是将新旧数据都输入到旧模型来作为软标签。</li>
</ul>
<h3 id="balanced-fine-tuning"><a href="#balanced-fine-tuning" class="headerlink" title="balanced fine-tuning"></a>balanced fine-tuning</h3><p>为了解决类别不均衡带来的问题，跟上篇文章一样，新旧类选取相同数量的样本作为验证集进行 fine-tune，这里会用更小的学习率</p>
<p><br></p>
<h1 id="iCaRL-Incremental-Classiﬁer-and-Representation-Learning"><a href="#iCaRL-Incremental-Classiﬁer-and-Representation-Learning" class="headerlink" title="iCaRL: Incremental Classiﬁer and Representation Learning"></a>iCaRL: Incremental Classiﬁer and Representation Learning</h1><p>总结前两篇文章，发现增量学习的知识蒸馏的基本围绕以下 4 个损失来设计，分类损失均为数据在新模型下输出 logit 和标签之前的损失，蒸馏损失均为数据在新旧模型输出 logit 之间的损失。</p>
<ul>
<li>新数据的分类损失</li>
<li>新数据的蒸馏损失</li>
<li>旧数据的分类损失</li>
<li>旧数据的蒸馏损失</li>
</ul>
<p>第一篇文章为 4 个损失均有，EEIL 为第1、3、4 个损失的结合；而 iCaRL 为第1、2、4 个损失的组合。</p>
<p>除此之外，iCaRL 在分类上采取的是最近邻分类策略，算出每个类别的平均特征图，计算最近的一个，则为分类结果。</p>
<img src="/2019/11/30/Incremental-Learning-with-Knowledge-Distillation/11.png" class="" width="500" height="500">
<p><br></p>
]]></content>
  </entry>
  <entry>
    <title>Introduction to Reinforcement Learning</title>
    <url>/2019/08/24/Introduction-to-Reinforcement-Learning/</url>
    <content><![CDATA[<blockquote>
<p>课程来源：李宏毅2017机器学习课程<br>课程主页：<a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML17.html">http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML17.html</a><br>课程梗概：简要介绍强化学习的基本概念，以及强化学习两种基本的训练方法：policy-based approach 和 value-based approach。</p>
</blockquote>
<span id="more"></span>
<p><em>preface</em></p>
<p>AlphaGo 的成功使得强化学习进入了研究者的视野，强化学习是不同于监督学习和无监督学习之外的另外一种学习方式，它通过不断地探索，从经验中得到知识，从而使得模型的性能逐步提升。</p>
<p><br></p>
<h2 id="Basic-Idea-of-Reinforcement-Learning"><a href="#Basic-Idea-of-Reinforcement-Learning" class="headerlink" title="Basic Idea of Reinforcement Learning"></a>Basic Idea of Reinforcement Learning</h2><p>强化学习有几个重要的组成部分：</p>
<ul>
<li>Agent</li>
<li>Observation(state)</li>
<li>Action</li>
<li>Environment</li>
<li>Reward</li>
</ul>
<p>Agent 可以理解为整个模型，agent 看到 environment 中的一个 state，可以当作是一次 observation，之后做出反应，此为 action，该反应反馈到 environment 后，agent 会得到回馈 reward；接着 agent 又得到新的 observation，依次类推，直到终止条件。</p>
<p>举个简单的例子，agent 观察到一杯水，于是将其打翻，得到负面的反馈；下一个迭代，agent 观察到打翻掉的水，于是将其收拾好，此时就会得到正面的反馈。依次下去，agent 就会学到不可以打翻水。</p>
<img src="/2019/08/24/Introduction-to-Reinforcement-Learning/1.png" class="" width="500" height="500">
<img src="/2019/08/24/Introduction-to-Reinforcement-Learning/2.png" class="" width="500" height="500">
<p><strong>监督学习和强化学习的不同</strong></p>
<ul>
<li>监督学习可以理解为学生向老师学习，老师代表的是 label，是永远的正确答案，拿围棋举例，监督学习告诉学生，当棋盘为 A 这样的时候，应该走这一步；当棋盘为 B 这样的时候，应该走那一步，但对于围棋以及电玩这种应用，显然不可能枚举所有的情况，也不可能有唯一的一个正确选项，这就催生了强化学习。</li>
<li>强化学习可以理解为从经验中学习，学生通过自己的探索，得出获胜的方法，而这其中有另外一个老师的角色，而这个老师只告诉学生，这样做好不好，即 reward。因此学生探索得越多（训练越多），经验越丰富，模型也就越好。</li>
</ul>
<p><strong>强化学习的目标函数</strong></p>
<p>下面是 space invader 的游戏，黄色的为 alien，下面绿色的为太空船，左上角的为分数，游戏的目的是杀死所有的 alien。</p>
<p>先介绍一个概念 episode，它是 agent 完整的一次游戏（探索）过程，包括多个观察 s 以及动作 a。强化学习的目标函数是要最大化一次 episode 的累积 reward，而不是每一个 action 的 reward，假如目标函数是优化每一个 action 后的 reward，那么 agent 就会学到只射击外星人，而不会学到向左向右移动，因为只有射击才有 reward。</p>
<img src="/2019/08/24/Introduction-to-Reinforcement-Learning/3.png" class="" width="600" height="600">
<p><br></p>
<h2 id="强化学习训练方法"><a href="#强化学习训练方法" class="headerlink" title="强化学习训练方法"></a>强化学习训练方法</h2><p>主要分为两种方法：</p>
<ul>
<li>policy-based approach，学习一个 actor，即直接学习 agent</li>
<li>value-based approach，学习一个 critic，评判 agent 的好坏</li>
</ul>
<p>当然也有两者结合的方法。</p>
<h3 id="Policy-based-Approach——Learning-an-Actor"><a href="#Policy-based-Approach——Learning-an-Actor" class="headerlink" title="Policy-based Approach——Learning an Actor"></a>Policy-based Approach——Learning an Actor</h3><p>actor 的输入为 observation，输出为可能 action 的概率，actor 记为 $\pi_{\theta}(s)$</p>
<img src="/2019/08/24/Introduction-to-Reinforcement-Learning/4.png" class="" width="600" height="600">
<p>上面说到强化学习的目标函数为一次 episode 的总 reward，但是由于随机性，即使是同样的 actor，同样的输入，得到的 episode 也会不一样，因此定义总 reward 的期望作为目标函数。</p>
<img src="/2019/08/24/Introduction-to-Reinforcement-Learning/5.png" class="" width="600" height="600">
<p>一次 episode 可以看成是一次 trajectory $\tau={s<em>1,a_1,r_1,s_2,a_2,r_2,…,s_T,a_T,r_T}$，trajectory 的总 reward 记为 $R(\tau)=\sum</em>{t=1}^Tr<em>t$，固定 actor $\pi</em>{\theta}(s)$，得到某一次 trajectory 的概率为 $P(\tau|\theta)$，$\bar{R_{\theta}}$ 可以近似为 N 次 trajectory reward 的平均。</p>
<img src="/2019/08/24/Introduction-to-Reinforcement-Learning/6.png" class="" width="500" height="500">
<p>我们的目的是要最大化 $\bar{R<em>{\theta}}$，因此只要对 $\bar{R</em>{\theta}}$ 求梯度，再进行梯度上升更新 $\pi_{\theta}(\cdot)$ 即可，下面看一下梯度的计算过程。</p>
<script type="math/tex; mode=display">
\nabla \bar{R_{\theta}}=\sum_{\tau}R(\tau)\nabla P(\tau|\theta)=\sum_{\tau}R(\tau)P(\tau|\theta)\frac{\nabla P(\tau|\theta)}{P(\tau|\theta)}</script><script type="math/tex; mode=display">
=\sum_\tau R(\tau)P(\tau|\theta) \nabla logP(\tau|\theta)</script><script type="math/tex; mode=display">
\approx \frac{1}{N}\sum_{n=1}^{N} R(\tau^n)\nabla logP(\tau^n|\theta)</script><p>假设 $\tau={s_1,a_1,r_1,s_2,a_2,r_2,…,s_T,a_T,r_T}$，那么 $\nabla logP(\tau|\theta)$ 是多少呢？</p>
<script type="math/tex; mode=display">
P(\tau|\theta)=p(s_1)p(a_1|s_1,\theta)p(r_1,s_2|s_1,a_1)p(a_2|s_2,\theta)p(r_2,s_3|s_2,a_2)...</script><script type="math/tex; mode=display">
=p(s_1)\prod_{t=1}^{T}p(a_t|s_t,\theta)p(r_t,s_{t+1}|s_t,a_t)</script><script type="math/tex; mode=display">
\approx \prod_{t=1}^Tp(a_t|s_t,\theta)</script><p>因此</p>
<script type="math/tex; mode=display">
\nabla logP(\tau|\theta)=\sum_{t=1}^T\nabla logp(a_t|s_t,\theta)</script><p>所以，一次迭代的总梯度为</p>
<script type="math/tex; mode=display">
\nabla \bar{R_{\theta}} \approx \frac{1}{N}\sum_{n=1}^NR(\tau^n)\nabla logP(t^n|\theta)=\frac{1}{N}\sum_{n=1}^NR(\tau^n)\sum_{t=1}^{T_n}\nabla logp(a_t^n|s_t^n,\theta)</script><script type="math/tex; mode=display">
=\frac{1}{N}\sum_{n=1}^N\sum_{t=1}^{T_n}R(\tau^n)\nabla logp(a_t^n|s_t^n,\theta)</script><p>前面提到，我们目标函数考虑的是累计的 reward，而不是某个 action 后的瞬间 reward，我们假设在 $\tau^n$ 中，机器看到某个状态 $s_t^n$ 后采取了 $a_t^n$，假如整体的 $R(\tau^n)$ 为正，那么模型就会调整 $\theta$ 从而使得 $p(a_t^n|s_t^n,\theta)$ 增大，反之亦然。</p>
<p>我们将强化学习的梯度和普通分类问题的梯度进行对比</p>
<ul>
<li>强化学习：$\frac{1}{N}\sum<em>{n=1}^N\sum</em>{t=1}^{T_n}R(\tau^n)\nabla logp(a_t^n|s_t^n,\theta)$</li>
<li>分类问题：$\frac{1}{N}\sum<em>{n=1}^N\sum</em>{t=1}^{T_n}yi\nabla logp(a_t^n|s_t^n,\theta)$</li>
</ul>
<p>其实对于 actor 来说，它就是一个分类器，给定一个 state，输出各个 action 的概率，而强化学习的目标函数只是在各个概率前，用 $R(\tau^n)$ 作为其权重而已。</p>
<h3 id="Value-based-Approach——Learning-a-Critic"><a href="#Value-based-Approach——Learning-a-Critic" class="headerlink" title="Value-based Approach——Learning a Critic"></a>Value-based Approach——Learning a Critic</h3><p>Critic 不会用来决定一个 action，但是给定一个 actor，它可以判断它的好坏，因此 actor 可以从 critic 中找出来，经典的算法就是 Q-learning。</p>
<p>State value function $V^\pi(s)$，输入一个 state，输出 cumulated reward，该函数取决于 actor $\pi$，当 actor 较强，输入一个 state，得到的 reward 可能比较大；反之 actor 较弱，reward 则较小。</p>
<h4 id="How-to-estimate-V-pi-s"><a href="#How-to-estimate-V-pi-s" class="headerlink" title="How to estimate $V^\pi(s)$"></a>How to estimate $V^\pi(s)$</h4><p><strong>Monte-Carlo based approach</strong></p>
<p>critic 让 actor 自己玩游戏，当输入状态 s 后，观察 actor 能得到多少 reward。</p>
<img src="/2019/08/24/Introduction-to-Reinforcement-Learning/7.png" class="" width="500" height="500">
<p><strong>Temporal-difference approach</strong></p>
<p>有些应用的 episode 很长，或者甚至不会停止，那么我们只能用中间的一小部分来进行训练，假设只有一长串 episode 的一部分 ${…,s<em>t,a_t,r_t,s</em>{t+1},…}$，易证得：$V^\pi(s<em>t)-r_t=V^\pi(s</em>{t+1})$，</p>
<img src="/2019/08/24/Introduction-to-Reinforcement-Learning/8.png" class="" width="500" height="500">
<p><br></p>
]]></content>
  </entry>
  <entry>
    <title>Introduction to Unsupervised Conditional GAN</title>
    <url>/2019/08/19/Introduction-to-Unsupervised-Conditional-GAN/</url>
    <content><![CDATA[<blockquote>
<p>课程来源：李宏毅2018生成对抗网络课程<br>课程主页：<a href="http://speech.ee.ntu.tw/~tlkagk/courses_MLDS18.html">http://speech.ee.ntu.tw/~tlkagk/courses_MLDS18.html</a><br>文章梗概：本文降介绍非监督的条件GAN，只有两个数据域的数据，并不存在pair关系，其中会介绍最为经典的cycleGAN。</p>
</blockquote>
<span id="more"></span>
<p><br></p>
<p><em>preface</em></p>
<p>之前讲到的 conditional generation 都是 supervised 的，即对于每一个输入都有对应的输出，但是有一些任务，比如风格迁移，从一个 domain 的图像转到另一个 domain 的图像，这种任务是非监督的，我们只有两个 domain 的多张图像，但是他们不存在 pair 的关系，下面来看下 GAN 如何解决这种问题。</p>
<h2 id="Domain-transfer"><a href="#Domain-transfer" class="headerlink" title="Domain transfer"></a>Domain transfer</h2><p>cycleGAN 就是用来解决不同 domain 之间的互转，论文中，作者用了普通马和斑马的例子，将一匹普通马转成斑马，或将斑马“去斑”。</p>
<p>首先先看单向的cycleGAN，domain A 是斑马，domain B 是普通马，先通过一个生成器将 A 域的马转成 B 域，再将生成的 B 域的假马，通过另外一个生成器转回 A 域。这其中有两个损失</p>
<ul>
<li>一是 cycle consistency，即重塑的 A 域的马要和原来 A 域的马要尽可能像。</li>
<li>二是判别损失，生成的 B 域的假马要通过判别器来判断是否真假。</li>
</ul>
<img src="/2019/08/19/Introduction-to-Unsupervised-Conditional-GAN/1.png" class="" width="550" height="550">
<p>而 cycleGAN 本质就是两个镜像对称的 GAN，形成一个环形网络，共有两个判别器，两个生成器，两个判别器用来判断是否为对应的 domain。</p>
<img src="/2019/08/19/Introduction-to-Unsupervised-Conditional-GAN/2.png" class="" width="550" height="550">
<p>cycleGAN 用来解决两个 domain 之间的转换，当涉及多个 domain 之间的转换时，若每两个 domain 之间都用一个生成器来解决的话，那必定会产生巨大的开销，starGAN 用一个生成器和判别器就解决了多个 domain 之间的转换问题</p>
<img src="/2019/08/19/Introduction-to-Unsupervised-Conditional-GAN/3.png" class="" width="550" height="550">
<p>生成器接收输入图像和目标域，生成假的图像后，再通过同一个生成器，将原域和假图像输入，得到重构图像，尽可能使得重构图像和原图接近；而判别器除了需要判别真假外，还需要另外做一个 domain classification。</p>
<p><br></p>
]]></content>
  </entry>
  <entry>
    <title>Introduction to Generative Adversarial Network</title>
    <url>/2018/11/21/Introduction-to-Generative-Adversarial-Network/</url>
    <content><![CDATA[<blockquote>
<p>课程来源：李宏毅2018生成对抗网络课程<br>课程主页：<a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses_MLDS18.html">http://speech.ee.ntu.edu.tw/~tlkagk/courses_MLDS18.html</a><br>文章梗概：该文章分为四个部分：GAN basics, GAN as structured learning, can generator learn by itself, can discriminator generator.</p>
</blockquote>
<span id="more"></span>
<p><br><br><em>preface</em></p>
<blockquote>
<p>Yann LeCun said adversarial training is the coolest thing since sliced bread.<br>He also said GAN and its variation are the most interesting idea in the last 10 years in ML.</p>
</blockquote>
<p><br></p>
<h1 id="Basic-Idea-of-GAN"><a href="#Basic-Idea-of-GAN" class="headerlink" title="Basic Idea of GAN"></a>Basic Idea of GAN</h1><h2 id="Generator"><a href="#Generator" class="headerlink" title="Generator"></a>Generator</h2><p>GAN是用来生成东西的，如图片、文章等，因此GAN会有一个生成器generator，当输入一个随机的向量的时候，就会输出你想要的图片或文章。</p>
<img src="/2018/11/21/Introduction-to-Generative-Adversarial-Network/1.png" class="" width="550" height="550">
<p>上面所说的generator实际上就是一个神经网络，网络的输入是一个随机向量，输出是一个高维向量（图片），输入向量的每一个维度可能会代表不同的特征，改变不同维度的数值，会导致生成的图片的某一个特征的改变。</p>
<img src="/2018/11/21/Introduction-to-Generative-Adversarial-Network/2.png" class="" width="500" height="500">
<h2 id="Discriminator"><a href="#Discriminator" class="headerlink" title="Discriminator"></a>Discriminator</h2><p>GAN中另外一个重要的成分就是判别器discriminator，它用来判断生成的图片的质量，或者用来判断生成的图片是否符合要求。假设我们要生成二次元图片，那么判别器的输入就是一张图片，输出是一个score，score越大代表生成的图片越像二次元图片。</p>
<img src="/2018/11/21/Introduction-to-Generative-Adversarial-Network/3.png" class="" width="550" height="550">
<h2 id="Generator和Discriminator的关系"><a href="#Generator和Discriminator的关系" class="headerlink" title="Generator和Discriminator的关系"></a>Generator和Discriminator的关系</h2><p>在Ian Goodfellow最开始的论文里面，他用假钞制造者和警察来类比generator和discriminator，假钞制造者想要制造出假钞来蒙骗警察，而警察是假钞的鉴定者，因此generator和discriminator之间互相对抗（这也是GAN叫adversarial的原因），并且不断地进化，到最后generator能制造出跟真钞一模一样的钞票，而discriminator也无法判断出真钞还是假钞。下图给出的是image generation的例子，第一代的generator可能生成的图片啥都不是，然后第二代稍微好一点，到最后就能生成二次元图片。</p>
<img src="/2018/11/21/Introduction-to-Generative-Adversarial-Network/4.png" class="" width="550" height="550">
<p>用另外一个视角去看GAN，我们可以把generator看作是学生，而discriminator看作是老师，老师看过很多的二次元图片，知道什么样的图片是好的，而学生学习画二次元图片，老师看学生画的图片，然后<strong>反馈</strong>给学生，让学生画出更好的画。</p>
<p>因此generator和discriminator亦敌亦友</p>
<img src="/2018/11/21/Introduction-to-Generative-Adversarial-Network/5.png" class="" width="550" height="550">
<h2 id="训练流程"><a href="#训练流程" class="headerlink" title="训练流程"></a>训练流程</h2><p>GAN是由两个network组成的，一个是generator，一个是discriminator，两个network需要单独训练，训练流程如下</p>
<ol>
<li>初始化generator和discriminator的参数</li>
<li><p>固定住generator的参数，更新discriminator，discriminator是用来判断生成图片的好坏的，首先我们有一个真实的二次元图片dataset，discriminator实际上是一个二分类器，真实的二次元图片是class 1，生成的图片是class 0，discriminator需要给真实图片高分，而给生成的图片低分</p>
 <img src="/2018/11/21/Introduction-to-Generative-Adversarial-Network/6.png" class="" width="550" height="550">
</li>
<li><p>固定住discriminator的参数，更新generator，generator的目标是生成二次元图片，由于discriminator对于生成的图片一般会给低分，而对于真实图片会给高分，因此我们需要训练generator使得它生成的图片尽可能地能够拿到高分，即蒙骗discriminator</p>
 <img src="/2018/11/21/Introduction-to-Generative-Adversarial-Network/7.png" class="" width="550" height="550">
</li>
</ol>
<p>具体的算法流程如下</p>
<img src="/2018/11/21/Introduction-to-Generative-Adversarial-Network/8.png" class="" width="550" height="550">
<p>简单分析一下两个network的loss function，discriminator实际上是一个二分类器，因此loss function就是cross entropy，discriminator希望真实图片的score尽可能大，而生成图片的score尽可能小；而generator则是希望自己生成的图片被discriminator打的分越高越好。</p>
<p><br></p>
<h1 id="GAN-as-Structured-Learning"><a href="#GAN-as-Structured-Learning" class="headerlink" title="GAN as Structured Learning"></a>GAN as Structured Learning</h1><p>machine learning就是需要找到一个函数能够把对应的X投影到Y上，分类如下</p>
<ul>
<li>classification：输出一个类别</li>
<li>regression：输出一个scalar</li>
<li>structured learning：输出一个sequence，matrix等，如机器翻译，语音识别，text-to-image等</li>
</ul>
<p>structured learning难点在于</p>
<ol>
<li>structured learning可以看作是one-shot learning，one-shot learning指的就是每一个样本只会出现一次，而在传统的分类问题，一个标签往往会对应很多个样本，而structured learning中如果把每一个输出都看作是一个类别的话，可能就会出现很多的类别，比如机器翻译中，一个Y可能就是一个class，而每个Y就可能只会出现一次，testset中的Y在trainset中更是没有出现。</li>
<li>output space会非常大，output space中的绝大多数的类别，更是没有X对应，这里举机器翻译的例子，假如是英译中，output space就是全部可能的中文句子，你不可能找到一个dataset，有所有中文句子的英文翻译。</li>
<li>由于上面的两点，structured learning必须要学会创造新的东西。因为在test中要输出的正确答案，可能是在训练中一次都没见过的。</li>
<li>由于structured learning中机器需要学会创造，因此机器需要学会规划，要有大局观。</li>
</ol>
<p>而GAN的generator就是用bottom-up的方法学着component-by-component地生成object，而discriminator就是用top-down的方法，整体地评估这个object。</p>
<p><br></p>
<h1 id="两个疑问"><a href="#两个疑问" class="headerlink" title="两个疑问"></a>两个疑问</h1><p>我们上面提到，GAN实际是由两个network组成的，一个是generator，一个是discriminator，generator学习生成object，discriminator给出评价，两者缺一不可。两个疑问是：generator能不能自己根据真实的data学习生成object？discriminator既然那么会评价，那么它能不能生成object？</p>
<h2 id="Can-generator-learn-by-itself"><a href="#Can-generator-learn-by-itself" class="headerlink" title="Can generator learn by itself?"></a>Can generator learn by itself?</h2><p>generator是一个输入随机向量，输出object的网络，如果需要学习出这样一个网络，按照监督学习的方法，我们首先需要构造数据集，随机生成X向量对应一张图片Y，但是这里面的问题在于如何去生成每一个图片Y所对应的随机向量X，我们在随机生成向量的时候，很有可能两张非常类似的图片对应了两个非常不一样的X，这样会使得network很难收敛，因为network无法满足让两个非常不一样的X，输出非常接近的Y，而上面提到GAN的generator的输入向量在某种程度上，每个维度是对应某一种特征的，类似于word embedding，因此我们不能随机地生成输入向量X。</p>
<p>由于输入向量在某种程度上表征着生成的图像，因此我们可以用encoder来生成每个图片的code，在训练auto-encoder的时候，我们不难发现，decoder实际上就是我们所要的generator，decoder接收一个code作为输入，然后输出code所对应的图片，这时候我们就能随机生成一些code，然后用decoder来生成图片。</p>
<img src="/2018/11/21/Introduction-to-Generative-Adversarial-Network/9.png" class="" width="550" height="550">
<p>但是auto-encoder训练出来的decoder可能不够鲁棒，因此用VAE(variational auto-encoder)来训练，使得decoder能够允许一定程度上的noise</p>
<img src="/2018/11/21/Introduction-to-Generative-Adversarial-Network/10.png" class="" width="550" height="550">
<p>似乎VAE训练的decoder能够作为generator，然后生成图片，不需要discriminator， 然而我们在训练auto-encoder的时候，我们是希望decoder的输出和输入的图片越接近越好，而一般评判接近与否是用两个图片的Euclidean distance来判断，但是这种评判标准是否合理呢？比如下面的例子，auto-encoder会更加倾向于前两个输出，因为整体的error较少，但是不符合我们的要求，这也意味着auto-encoder的generator它希望做到的是copy，它并不能学到精髓，这也意味着我们需要更加复杂的评判规则，去衡量生成的图片是否和原图片一样好，这也就是discriminator存在的必要性了。</p>
<img src="/2018/11/21/Introduction-to-Generative-Adversarial-Network/11.png" class="" width="550" height="550">
<p>由于auto-encoder是一个fully-connected network，它很难捕获到neuron之间的关系，neuron之间都是相互独立的，但是在生成图片的这件事上，component之间的correlation很重要，而一般network的架构很难将component之间的关系考虑进去，因此就要更大的网络。比如下面的例子中，Layer L是decoder最后一层，每个neuron代表一个pixel，第二个neuron想生成一滴颜色，它想第一个neuron和它一起生成，但是它无法控制它旁边的neuron，因为他们都是相互独立的。</p>
<img src="/2018/11/21/Introduction-to-Generative-Adversarial-Network/12.png" class="" width="550" height="550">
<h2 id="Can-discriminator-generate"><a href="#Can-discriminator-generate" class="headerlink" title="Can discriminator generate"></a>Can discriminator generate</h2><p> 相对于generator，discriminator能够从全局去判断生成的图片好与坏，可以用一个convolution layer轻松检测component之间的关系。</p>
<p>而假设我们有一个训练好的discriminator，它能够判断什么样的图片是好的，那么我们可以通过以下的式子让discriminator生成好的图片。从全局中搜索出能让discriminator打高分的图片。</p>
<script type="math/tex; mode=display">\tilde{x} = arg max_{x\in X} D(x)</script><p>我们姑且不去考虑，这个搜索能不能实现，而关心discriminator应该如何训练，我们有的数据集是真实的图片，即positive example，训练一个discriminator，除了要有positive example，还需要negative example，那么我们该如何生成negative example呢？下面的演算法能够实现discriminator的训练</p>
<p>一开始随机生成一些noise image作为negative example，然后训练discriminator，然后用这个discriminator去生成一些高分图片作为下一轮的negative example，然后再次训练，直到收敛。</p>
<img src="/2018/11/21/Introduction-to-Generative-Adversarial-Network/13.png" class="" width="550" height="550">
<p>下面这个图反映了discriminator如何一步步地收敛，一开始随机生成的数据在左边，真实的数据是中间，第一代discriminator能够把左边图片的评分压低，中间图片的评分拉高，但是这两个区域外的数据，discriminator无法知道该打高分还是低分，因此右边区域的分数可能甚至比真实数据还高；然后用这个discriminator去生成高分图片作为新的negative example，所以在下一轮迭代就会把这一部分的数据的评分压低。不断迭代，最终评分高的部分就只有真实数据。</p>
<img src="/2018/11/21/Introduction-to-Generative-Adversarial-Network/14.png" class="" width="550" height="550">
<p>这个演算法能够使得discriminator产生好的照片，但是问题在于，上面的那个argmax的方程并不容易解决，如果需要解决，就需要引入额外的约束，这又会影响网络的性能。</p>
<h2 id="Generator-vs-Discriminator"><a href="#Generator-vs-Discriminator" class="headerlink" title="Generator vs Discriminator"></a>Generator vs Discriminator</h2><ul>
<li><p>generator</p>
<ul>
<li>pros：擅长生成图片</li>
<li>cons：不能学到component之间的关系，只能浮于表面</li>
</ul>
</li>
<li><p>discriminator</p>
<ul>
<li>pros：考虑大局</li>
<li>cons：生成图片基本不可能，argmax problem不容易解决</li>
</ul>
</li>
<li><p>generator + discriminator</p>
<ul>
<li><p>discriminator的问题在于argmax problem无法解决，即找不到高评分的图片，即找不到下一轮的negative example来进行继续训练。而generator的存在刚好解决了discriminator的这个问题，generator就是努力学习生成能让discriminator评分高的图片，这样generator就可以生成高评分的图片，来作为下一轮discriminator的negative example，或者理解为generator就是学习解决这个argmax的problem。</p>
<script type="math/tex; mode=display">G \rightarrow \tilde{x} \equiv \tilde{x} = argmax_{x \in X} D(x)</script></li>
<li><p>generator的问题在于，它生成的图片是component-by-component的，因此不能考虑到大局观，而discriminator的存在就是从大局上给generator反馈，而不再是VAE的Euclidean distance。</p>
</li>
<li><p>因此generator和discriminator相互促进，各自解决了对方的问题。</p>
</li>
</ul>
</li>
</ul>
<p><br></p>
]]></content>
  </entry>
  <entry>
    <title>NLP预训练模型概况</title>
    <url>/2021/12/07/NLP%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%A6%82%E5%86%B5/</url>
    <content><![CDATA[<img src="/2021/12/07/NLP%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%A6%82%E5%86%B5/titan.png" class="" width="700" height="700">
<span id="more"></span>
<p><br><br><em>preview</em><br>预训练模型在 CV 任务里面是非常常见的，通常是用 ImageNet 数据集在一个网络上进行预训练，利用庞大 ImageNet 数据集让网络尽可能学习到更多的通用语义信息，然后下游任务直接用预训练模型进行微调即可，这是 CV 领域非常成熟的一套方法。</p>
<p>但在 NLP 领域预训练模型迟迟没能面世的原因，可能在于 NLP 的任务多且杂，不同任务的网络设计差异比较大，因此无法做到很好的统一。但这只是我猜测的原因罢了，要说 NLP 的预训练模型，早年的词向量也算是一种预训练模型，把训练好的词向量直接用到下游任务上，但这种预训练模型的效果对比起 CV 领域的预训练模型，效果不太好罢了。直到 transformer 的出现，NLP 的预训练模型才大规模的发展，这篇文章总结一下各科技大公司所提出的 NLP 预训练模型。</p>
<h3 id="ELMO"><a href="#ELMO" class="headerlink" title="ELMO"></a>ELMO</h3><p>讲 ELMO 前必须先讲下词向量，word2vec 等词向量有一个很严重的问题，就是无法区分多义词，比如 play 这个单词，十几种意思只能融合在一个词向量里面，这样的词向量无法应对所有的语境，虽然后面有一些方法解决多义词的问题，但是都没有很好地解决掉。</p>
<p>ELMO 是 embedding from language model 的简称，来自《Deep contextualized word representation》论文，它的基本思想是，通过训练一个双向的 LSTM 语言模型，然后得到三种 embedding，分别是最下层的单词 embedding，第一层 LSTM 输出的 embedding，以及第二层 LSTM 输出的 embedding，利用这三种 embedding 就能很好地解决多义词的问题，因为最原始的 embedding 经过语言模型后，会根据不同语境得到不同的语义信息。</p>
<p>ELMO 训练好后可以很容易的应用到下游任务，整体做法就和词向量作为预训练模型的方法一样，将句子输入到语言模型得到三个 embedding，然后再把这些 embedding 输入到下游任务里面。</p>
<img src="/2021/12/07/NLP%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%A6%82%E5%86%B5/elmo.jpeg" class="" width="500" height="500">
<p><br></p>
<h3 id="GPT"><a href="#GPT" class="headerlink" title="GPT"></a>GPT</h3><p>generative pre-training model，是 OpenAI 2018年提出的模型，利用 transformer 解决各种 NLP 任务，GPT 预训练采用的是单向的语言模型，即根据上文预测下一个可能出现的词，因此 GPT 采用的 transformer 也是单向的 transformer，即将 Encoder 中的 Self-Attention 替换成了 Masked Self-Attention，对于某个 token，只计算与前面 token 的 attention。</p>
<p>而训练的过程其实非常的简单，就是将句子 n 个词的词向量(第一个为 [start] )加上 Positional Encoding 后输入到 Transfromer 中，n 个输出分别预测该位置的下一个词([start] 预测句子中的第一个词，最后一个词的预测结果不用于语言模型的训练)。由于采用的是 masked self-attention，因此对于某个词的预测，他只能看到前面的词，这样保证了模型的合理性。</p>
<img src="/2021/12/07/NLP%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%A6%82%E5%86%B5/gpt.png" class="" width="500" height="500">
<p>训练好单向的语言模型后，就可以应用到下游任务了，运用少量的带标签数据对模型参数进行微调，上一步中最后一个词的输出我们没有用到，在这一步中就要使用这一个输出来作为下游监督学习的输入，将最后一个词的输出连接到一个 linear 层进行下游任务，不同下游任务的改造如下，因为 GPT 是一个单向的语言模型，整体的过程就好像我让它看完一整个句子，然后问它最后的结果，因此用的也是最后一个 token 的输出来连 linear 层。这一点在后面的 GPT 系列体现更加明显。</p>

<p>一些题外话：由于 GPT 是一个单向的语言模型，因此它可以做生成任务，即随机给他一段句子，他能写出一段故事来，甚至是给一个新闻标题生成新闻，GPT 问世的时候曾生成过一篇 unicorn 的 fake news，因此现在都拿 unicorn 去表示 GPT。</p>
<p><br></p>
<h3 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h3><p>Bidirectional Encoder Representation from Transformer，是 Google 2018 年提出的模型，和 GPT 非常类似，将 GPT 的单向语言模型改成了双向的语言模型，即不用采用 masked self-attention，但在预训练的时候，不能再像 GPT 那样预测下一个词，这时候 BERT 采取的是一种 masked language model(MLM) 的预训练任务，本质上和 word2vec 的 cbow 方法一样，即一个句子随机 mask 一些词，然后利用上下文的词去预测出这个 masked 掉的词，因此有人就形象地这个任务称为完形填空。</p>
<p>但是，直接将大量的词替换为 [MASK] 标签可能会造成一些问题，模型可能会认为只需要预测 [MASK] 相应的输出就行，其他位置的输出就无所谓。同时Fine-Tuning阶段的输入数据中并没有 [MASK] 标签，也有数据分布不同的问题。为了减轻这样训练带来的影响，BERT采用了如下的方式：</p>
<ol>
<li>输入数据中随机选择 15% 的词用于预测，这 15% 的词中，</li>
<li>80% 的词向量输入时被替换为 [MASK] </li>
<li>10% 的词的词向量在输入时被替换为其他词的词向量</li>
<li>另外 10% 保持不动</li>
</ol>
<p>BERT 还提出了另外一种预训练方式 NSP(next sentence prediction)，即预测两个句子是否是连着的两句话，与 MLM 同时进行，组成多任务预训练。这种预训练的方式就是往 Transformer 中输入连续的两个句子，左边的句子前面加上一个 [CLS] 标签，两个句子之间使用 [SEP] 标签予以区分，[CLS] 的输出后连一个 linear 层，用来判断两个句子之间是否是连续上下文关系。而其他 masked 的 token 后面也接 linear 层来预测原来被 masked 掉的词。</p>
<img src="/2021/12/07/NLP%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%A6%82%E5%86%B5/bert-1.png" class="" width="700">
<p>为了区分两个句子的前后关系，BERT除了加入了Positional Encoding之外，还两外加入了一个在预训练时需要学习的 Segment Embedding 来区分两个句子。这样一来，BERT的输入就由词向量、位置向量、段向量三个部分相加组成。</p>
<img src="/2021/12/07/NLP%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%A6%82%E5%86%B5/bert-2.png" class="" width="500" height="500">
<p>BERT 的 Fine-Tuning 阶段和 GPT 没有太大区别。将分类预测用的输出向量从 GPT 的最后一个词的输出位置改为了句子开头 [CLS] 的位置了。不同的任务Fine-Tuning的示意图如下：</p>
<img src="/2021/12/07/NLP%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%A6%82%E5%86%B5/bert-3.png" class="" width="700">
<p>总结一下：BERT 借鉴了 ELMO、GPT、CBOW 等方法，本身没有太多的创新，更像是集大成者，但普适性是真的好，在各种任务上都达到了 SOTA 的水平，BERT 和 GPT 给整个 NLP 领域开了个好头，从此之后的预训练模型越做越大，训练的语料越来越多。</p>
<p><br></p>
<h3 id="GPT-2"><a href="#GPT-2" class="headerlink" title="GPT-2"></a>GPT-2</h3><p>GPT-2 沿用 GPT 的思路，依旧是单向的语言模型，即上文推断下文，但实现上完全抛弃 fine-tune 阶段，下游任务转为无监督的方式，要实现这个目标，GPT-2 用了一个更大更深的 transformer，堆叠了 48 层，且用了更大的且更高质量的语料库去预训练这个模型。下游任务无监督的方式是指不需要再用特定的任务数据集去微调，直接输入任务的描述以及问题，GPT-2 就会自动生成答案。举个例子，比如机器翻译任务，直接输入“请将下列中文翻译成英文，中文：我爱你，英文：”，然后 GPT-2 就会用它强大的语言模型，预测后面输出的字是 “ I Love You”，这种强大的能力得益于预训练模型的强大，以及语料库的丰富，这就好比如果一个人博览群书，自然就很轻松地就能完成翻译、问答、摘要等任务。</p>
<p>GPT-2 就好比一个很通用的语言模型，他学习到的知识可能是非常通用的，这才使他能够实现 zero-shot learning（不经过微调直接进行无监督），从 ELMO 到 GPT，再到 BERT，再到 GPT-2、GPT-3，模型规模越发的恐怖，甚至都无法公开因为太大了，如今的发展趋势可能慢慢地从专用模型往通用模型去转变，从少量标签数据到大量高质量无标签数据转变。</p>
<p><em>references</em></p>
<ol>
<li><a href="https://mp.weixin.qq.com/s/IgzOTLsj691mGA5TibLfcQ">从Word Embedding到Bert模型——自然语言处理预训练技术发展史</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/69290203">Transformer结构及其应用详解—GPT、BERT、MT-DNN、GPT-2</a></li>
<li><a href="https://github.com/graykode/nlp-tutorial/blob/master/5-2.BERT/BERT.py">bert代码</a></li>
<li><a href="https://www.bilibili.com/video/BV1Kb4y187G6?spm_id_from=333.999.0.0">bert代码解读</a></li>
<li><a href="https://www.bilibili.com/video/BV1Wv411h7kN?p=55">李宏毅2021机器学习课程</a></li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>VIT预训练模型介绍</title>
    <url>/2021/12/14/VIT%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D/</url>
    <content><![CDATA[<p>Transformer 提出后，一大堆基于 transformer 的 NLP 预训练模型被提出，如 BERT、GPT 等，但 transformer 在 CV 领域上的应用却迟迟没能火起来，最近 VIT (vision transformer) 的提出，给 CV community 注入了一剂强心剂，再一次证明了 transformer 是现今最强的特征提取器，按照 NLP 的发展历程，要训练这种 data-hungry 的大规模网络，就必定需要预训练方法，用监督训练的方式肯定是行不通的，需要利用无标注的数据，采取自监督的方法进行预训练。这篇文章将介绍 BEIT 和 MAE 这两种预训练方法</p>
<blockquote>
<p>BEIT: BERT Pre-Training of Image Transformers<br>Masked Autoencoders Are Scalable Vision Learners</p>
</blockquote>
<span id="more"></span>
<p><br></p>
<h3 id="VIT-介绍"><a href="#VIT-介绍" class="headerlink" title="VIT 介绍"></a>VIT 介绍</h3><p>transformer 擅长处理序列数据，因此 NLP 领域可以很自然地利用 transformer 来构造网络，但是 CV 领域处理的是图片视频这些三维的数据，它不具有序列性。要用 transformer 来处理图片数据，就必须得引入序列信息，而 VIT 采取了一种很简单的方法：将图片或特征图（先用 CNN 得到 feature map）切成一块块的 patch，就能构造出一个从左上角到右下角的序列，加入位置编码，引入分类 token，然后再将这个序列输入到 transformer 里面。transformer 的结构对比标准的 transformer 结构也做了一点修改，将 layernorm 的位置提前了。</p>
<img src="/2021/12/14/VIT%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D/vit.png" class="" width="700">
<p>VIT 整体的训练方式是采取 pre-train 再 finetune 的形式，跟 BERT 一样，VIT 也是有 base、large 和 huge 三种结构，对于这种 data-hungry 的网络，在小数据集（如 cifar10/100）上 train from scratch 肯定很差，因此才需要预训练再微调。VIT 采用的预训练方式是有监督的训练，从上面的网络结构图也能看出，它的预训练任务只能采取分类任务，因此作者采用了三个大分类数据集来做对比，分别是 ImageNet-1k (1.3M)、ImageNet-21k (14M) 和 JFT-18k (303M)，一个数据集比一个数据集大。</p>
<p>由于网络大，只采取监督训练不能完全训练好网络，因此实验结果总会出现，用 ImageNet-1k 预训练的 VIT-Large 比 VIT-Base 还要差的结果，只有用 JFT 这种超大数据集训练才能训得好，但这样对数据集的要求就太高了，因此需要利用自监督的训练方式来对 VIT 进行预训练。</p>
<p><br></p>
<h3 id="BEIT"><a href="#BEIT" class="headerlink" title="BEIT"></a>BEIT</h3><p>BEIT 是参考 BERT 的方式对网络进行预训练的，BERT 是采用 MLM (masked language modeling) 的方式进行预训练，网络结构只用了 encoder，对输入的 sequence 进行随机 mask，然后让网络去预测 mask 掉的 token。</p>
<p>由于 NLP 的输入的句子是由一个个的 token 组成的，而 token 是从有限的集合中来的，也就是 vocabulary，但是 cv 里，将图片切成一个个 patch 后，这些 patch 不可能从一个有限的 vocabulary 里面得到，如果要按照 BERT 那一套预训练方式，首先需要将 patch 映射到 token，因此作者训练了一个 tokenizer 实现了这个功能，作者把这个叫做 visual token，然后其他的部分和 BERT 保持一致，mask 掉某些 patch，输入到网络，网络预测 masked patch 的 visual token。对于 masking 部分，作者 mask 掉 40% 的 patch，并且不是完全随机 mask，而是提出一种 block-wise 的 masking 方法，详细看论文。</p>
<p>预训练部分由于采取的是自监督的方式，因此并不需要像原始 VIT 那样需要那么大的监督数据集，作者只采用了 ImageNet-1k 就能取得比较好的结果了。</p>
<img src="/2021/12/14/VIT%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D/beit.png" class="" width="700">
<p><br></p>
<h3 id="MAE"><a href="#MAE" class="headerlink" title="MAE"></a>MAE</h3><p>MAE 代表 Masked Auto-Encoder，从名字上可以看出，它预训练的方式就是将图片 mask 掉，然后训练一个 auto-encoder 将其复原，这种结构在 cv 里的复原任务里经常能看到，但为什么现在才提出来呢，后面会详细说下 NLP 和 CV 领域的差别。</p>
<p>整体的结构如下图，首先将图片进行随机 mask，不同 BEIT 的是，MAE mask 的比例高达 70-80%，并且 BEIT 做的是预测 masked token，MAE 做的是恢复 masked patch，并且对于 encoder 的输入，没有输入 masked patch，这是为了保持与 finetune 阶段一致，减少预训练和 finetune 的 gap，其实 bert 采取 8:1:1 的比例 mask 掉 token，也是为了减少这个 gap 的，并且只输入 visible patch，也能减少很多的运算量；然后会将 encoder 的输出加上 mask token，再一同输入到 decoder 进行解码恢复，注意 encoder 和 decoder 都是 VIT 结构，但是 encoder 要比 decoder 大，这是因为 decoder 只用于预训练，finetune 阶段 decoder 将会被去掉，下游任务只会用到 encoder。</p>
<p>对比 BERT 和 MAE，我们可以发现，其实 BERT 更像是 MAE 的 decoder 部分，MAE 先通过 encoder 得到高级的语义信息，再和 mask token 拼接在一起，输入到 decoder，而 bert 输入是一个个的词，天然就是一种语义信息很高的 token，对比图片的一个个 patch（高冗余）。</p>
<img src="/2021/12/14/VIT%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D/mae.png" class="" width="500" height="500">
<p>作者思考过 cv 和 nlp 两个领域在预训练 masked 编码器的区别，主要有三个方面导致 transformer 当初在 cv 领域没火起来。</p>
<ol>
<li>CNN 一直是 cv 领域的主流网络，因为图片这种数据适合卷积来操作，而这种 grid 的卷积操作不好加入序列信息，如位置编码或者 mask 信息，这种结构的 gap 直到 VIT 出现才被解决。</li>
<li>文字和图片的信息密度不同，文字是一种高度语义化的信息，因此 bert 采取 MLM 这种方式，并且只要 mask 15% 就能够很好的充当预训练的功能；但是图片是一种高度冗余的信息，它可以很容易地通过邻近的像素去还原 mask，并不需要很多的高级语义，要解决这种问题，作者提出只要 mask 的比例足够大，也能学习到很多语义信息来充当预训练。</li>
<li>对于 bert，预测 missing words 是一种高级的语义任务，encoder 的输出就已经有很多高级语义信息了，因此 bert 的 decoder 就只是一个简单的 MLP；而对于 MAE，它是要恢复 pixel，这个相对来说是不需要很多语义信息的，因此 MAE 专门引入了一个 decoder 来做这件事。</li>
</ol>
<p><br></p>
]]></content>
  </entry>
  <entry>
    <title>Similarity-Based Feature Distillation Overview</title>
    <url>/2019/09/10/Similarity-Based-Feature-Distillation-Overview/</url>
    <content><![CDATA[<p>该文章总结了 CVPR 和 ICCV 中提到的基于特征相似度蒸馏策略，共五篇文章。</p>
<ul>
<li>Structured Knowledge Distillation for Semantic Segmentation</li>
<li>Similarity-Preserving Knowledge Distillation</li>
<li>Correlation Congruence for Knowledge Distillation</li>
<li>Knowledge Distillation via Instance Relationship Graph</li>
<li>Relational Knowledge Distillation</li>
</ul>
<span id="more"></span>
<p><br></p>
<h2 id="Structured-Knowledge-Distillation-for-Semantic-Segmentation"><a href="#Structured-Knowledge-Distillation-for-Semantic-Segmentation" class="headerlink" title="Structured Knowledge Distillation for Semantic Segmentation"></a>Structured Knowledge Distillation for Semantic Segmentation</h2><blockquote>
<p>Authors: Yifan Liu, Ke Chen, Chris Liu, et al.<br>Conference: CVPR 2019</p>
</blockquote>
<p>该论文主要针对的是图像分割领域的知识蒸馏问题，作者共提出了三个 loss</p>
<ul>
<li>Pixel-wise distillation：由于分割任务可以看成是一个在每个像素点上的分类任务，因此可以将原始 KD 的那套方法拿过来，假设模型输出的分割图大小为 $W^\prime \times H^\prime \times N$，$N$ 为类别数，因此 Loss 定义为</li>
</ul>
<script type="math/tex; mode=display">
{\cal L}_{pi}(S)=\frac{1}{W^\prime \times H^\prime}\sum_{i \in R}KL(q_i^s|q_i^t)</script><ul>
<li>Pair-wise distillation：上面的 pixel-wise KD 可以看成是软标签上的蒸馏，而这部分的蒸馏是特征图上的蒸馏，但是作者并不是像 Hint Distillation 那样直接让学生网络 mimick 教师网络的特征图，而是将特征图在空间维度上提取了一个相似度矩阵。假设一个特征图的大小为 $W\times H \times C$，每个 pixel 可以看成是一个 $C$ 维的向量，$a_{ij}$ 表示为 第 $i$ 个像素点 $f_i$ 和第 $j$ 个像素点 $f_j$ 之间的 similarity，定义为</li>
</ul>
<script type="math/tex; mode=display">
a_{ij}=\frac{f_i^T \cdot f_j}{||f_i||_2 \cdot ||f_j||_2}</script><p>​        因此相似度矩阵会是一个 $WH \times WH$ 的对称矩阵，pair-wise distillation 定义为</p>
<script type="math/tex; mode=display">
{\cal L}_{pa}(S)=\frac{1}{(W^\prime \times H^\prime)^2}\sum_{i \in R}\sum_{j\in R}(a_{ij}^s-a_{ij}^t)^2</script><ul>
<li>Holistic Distillation：作者将整个学生网络看成是一个 generator，模型生成的分割图看成是一个 fake sample，然后教师网络的输出看成是 true sample，一起输入到一个 discriminator 里面，希望 discriminator 能够判别出哪些是学生网络生成的，哪些是教师网络生成的，进而再固定判别器，调整生成器使得学生网络生成的分割图尽可能地像 GT，其中 GAN 运用的是 Wasserstein GAN，loss 函数如下</li>
</ul>
<script type="math/tex; mode=display">
{\cal L}_{ho}(S,D)=E_{Q^s\sim p_s(Q^s)}[D(Q^s|I)]-E_{Q^t\sim p_t(Q^t)}[D(Q^t|I)]</script><p>整体的框架如下</p>
<img src="/2019/09/10/Similarity-Based-Feature-Distillation-Overview/1.png" class="" width="500" height="500">
<p>实验结果如下，三个 Loss 一起用的话，实验结果最好。</p>
<img src="/2019/09/10/Similarity-Based-Feature-Distillation-Overview/2.png" class="">
<p><br></p>
<h2 id="Similarity-Preserving-Knowledge-Distillation"><a href="#Similarity-Preserving-Knowledge-Distillation" class="headerlink" title="Similarity-Preserving Knowledge Distillation"></a>Similarity-Preserving Knowledge Distillation</h2><blockquote>
<p>Authors: Frederick Tung,  Greg Mori, et al.<br>Conference: ICCV 2019</p>
</blockquote>
<p>该论文做的是分类任务，上面的论文对于特征的提取方式，采取的是空间相似度的方式，而这篇论文关注的是样本之间的相似性，如下图，得到一个 $b\times b$ 的相似度矩阵。</p>
<img src="/2019/09/10/Similarity-Based-Feature-Distillation-Overview/3.png" class="">
<p>假设特征图经过 reshape 后为 $Q\in R^{b\times chw}$，样本相似度矩阵定义为</p>
<script type="math/tex; mode=display">
\tilde{G}^{(l)}=Q^{(l)} \cdot Q^{(l)T};G^{(l)}_{[i,:]}=\frac{\tilde{G}_{[i,:]}^{(l)}}{||\tilde{G}_{[i,:]}^{(l)}||_2}</script><p>特征蒸馏的 Loss 为</p>
<script type="math/tex; mode=display">
L_{SP}(G_T,G_S)=\frac{1}{b^2}\sum||G_T-G_S||^2</script><p><br></p>
<h2 id="Correlation-Congruence-for-Knowledge-Distillation"><a href="#Correlation-Congruence-for-Knowledge-Distillation" class="headerlink" title="Correlation Congruence for Knowledge Distillation"></a>Correlation Congruence for Knowledge Distillation</h2><blockquote>
<p>Authors: Baoyun Peng,  Xiao Jin, et al.<br>Conference: ICCV 2019</p>
</blockquote>
<p>这篇论文和上面那篇几乎一样，同样是 ICCV 的文章，同样是做分类任务，同样是提取样本之间的相似性，唯一不同的是改了个名字叫做 correlation congruence。</p>
<p>另外不同的是，这篇文章还给出了几个相似度的度量方式</p>
<img src="/2019/09/10/Similarity-Based-Feature-Distillation-Overview/4.png" class="">
<p><br></p>
<h2 id="Knowledge-Distillation-via-Instance-Relationship-Graph"><a href="#Knowledge-Distillation-via-Instance-Relationship-Graph" class="headerlink" title="Knowledge Distillation via Instance Relationship Graph"></a>Knowledge Distillation via Instance Relationship Graph</h2><blockquote>
<p>Authors: Yufan Liu,  Jiajiong Cao, et al.<br>Conference: CVPR 2019</p>
</blockquote>
<p>这篇论文其实跟上面的两篇的核心思想是一样的，都是样本之间的相似性，但是作者用 Graph 进行包装，提出的是 Instance Relationship Grpah(IRG)，图中节点为一个 sample，节点之间的边为 sample 之间的 relationship，如下图，但是这个 graph 可以看出来是一个全连接的 graph，其邻接矩阵不存在 0 项，因此和上面提到的样本相似度矩阵是一个东西。</p>
<img src="/2019/09/10/Similarity-Based-Feature-Distillation-Overview/5.png" class="">
<p>但与前面论文稍微不同的是，作者除了直接蒸馏这个邻接矩阵之外，还提出蒸馏其他信息。首先一个 IRG 可以看成是节点集合和边集合，节点为每个样本的特征图，边为两个样本特征图之间的距离，因此第 $l$ 层的 IRG 定义如下</p>
<img src="/2019/09/10/Similarity-Based-Feature-Distillation-Overview/6.png" class="">
<p>除了定义 IRG 之外，作者参考了 FSP 的思路，考虑了网络中不同层之间 IRG 之间的关系，想看一个 IRG 如何过渡到下一个 IRG，定义为 $IRG-t$，其包含节点的过渡以及边的过渡，“过渡”的定义为 Euclidean 距离。</p>
<img src="/2019/09/10/Similarity-Based-Feature-Distillation-Overview/7.png" class="">
<p>对于特征的蒸馏方式，作者提出了两种 mode，一种是一对一，学生网络和教师网络的 IRG 一对一，另一种是一对多，教师网络只拿最后一层去指导学生网络的后面几层。</p>
<img src="/2019/09/10/Similarity-Based-Feature-Distillation-Overview/8.png" class="">
<p>上面提到了两个图，一个是 IRG，另一个是 IRG-t，两个都是以 graph 的形式出现，有节点有边，对于 IRG，其 Loss 如下，其包含节点的 Loss 和边的 Loss。</p>
<img src="/2019/09/10/Similarity-Based-Feature-Distillation-Overview/9.png" class="">
<p>而作者最后采取的 Loss 如下，第一项为最后一层教师和学生网络之间 IRG 节点的 loss，最后一层 IRG 节点即网络的输出，作者把前面特征图中 IRG 的节点信息没有加入到 Loss 项里；而第二项采用的是一对多的 mode，教师网络只考虑最后一层的边信息。</p>
<img src="/2019/09/10/Similarity-Based-Feature-Distillation-Overview/10.png" class="">
<p>对于 IRG-t，同样存在节点信息和边信息，general 的 Loss 形式如下</p>
<img src="/2019/09/10/Similarity-Based-Feature-Distillation-Overview/11.png" class="">
<p>但是由于边信息太多，作者把第二项去掉了，只保留第一项</p>
<img src="/2019/09/10/Similarity-Based-Feature-Distillation-Overview/12.png" class="">
<p>总的 Loss 如下</p>
<img src="/2019/09/10/Similarity-Based-Feature-Distillation-Overview/13.png" class="">
<p><br></p>
<h2 id="Relational-Knowledge-Distillation"><a href="#Relational-Knowledge-Distillation" class="headerlink" title="Relational Knowledge Distillation"></a>Relational Knowledge Distillation</h2><blockquote>
<p>Authors: Wonpyo Park,  Dongju Kim, et al.<br>Conference: CVPR 2019</p>
</blockquote>
<p>同样是分类任务中，挖掘样本之间的相关性</p>
<img src="/2019/09/10/Similarity-Based-Feature-Distillation-Overview/14.png" class="">
<p>对于相关性度量，作者提出两种方式</p>
<ul>
<li><p>Distance-wise distillation：就是简单的距离公式</p>
<img src="/2019/09/10/Similarity-Based-Feature-Distillation-Overview/15.png" class="" width="400" height="400">
</li>
<li><p>Angle-wise distillation：采用的三元组的方式，两个样本之间先求距离并作归一化，然后再求角度。</p>
</li>
</ul>
<img src="/2019/09/10/Similarity-Based-Feature-Distillation-Overview/16.png" class="" width="400" height="400">]]></content>
  </entry>
  <entry>
    <title>【概率论与数理统计】——事件与概率</title>
    <url>/2018/08/27/%E3%80%90%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E3%80%91%E2%80%94%E2%80%94%E4%BA%8B%E4%BB%B6%E4%B8%8E%E6%A6%82%E7%8E%87/</url>
    <content><![CDATA[<blockquote>
<p>课程名称：概率论与数理统计<br>开设学校：中科大<br>课程平台：icourse<br>第一章：事件与概率，讲述概率论的基本概念</p>
</blockquote>
<span id="more"></span>
<h1 id="一、随机试验和随机事件"><a href="#一、随机试验和随机事件" class="headerlink" title="一、随机试验和随机事件"></a>一、随机试验和随机事件</h1><ul>
<li>随机试验：随机现象的实现和对它某特征的观测<ul>
<li>随机试验的要求：结果至少有两个；每次只得到其中一种结果，且之前不能够预知；在相同条件下能重复试验。</li>
</ul>
</li>
<li>基本事件：随机试验中的每个单一结果。</li>
<li>随机事件：简称事件，在随机试验中我们所关心的可能出现的各种结果，它由一个或若干个基本事件组成。</li>
<li>样本空间：随机试验中所有基本事件所构成的集合，用 $\Omega$ 或 $S$ 表示<ul>
<li>样本空间中的基本事件是有限的，可能是可数或者不可数</li>
<li>必然事件($\Omega$)：在试验中一定会发生的事件 </li>
<li>不可能事件($\Phi$)：在实验中不可能发生的事件</li>
</ul>
</li>
</ul>
<h1 id="二、事件的运算"><a href="#二、事件的运算" class="headerlink" title="二、事件的运算"></a>二、事件的运算</h1><p>事件与集合一一对应，事件是集合（集合元素是基本事件），基本事件是空间中的点；事件的运算转成集合的运算</p>
<ul>
<li>子事件</li>
<li>事件的和 $A\bigcup B$</li>
<li>事件的积 $A\bigcap B$</li>
<li>对立事件(余事件) $\overline{A}$</li>
<li>事件A和事件B的差 $A-B$</li>
<li>$De Morgan$ 对偶法则</li>
</ul>
<h1 id="三、概率的定义及性质"><a href="#三、概率的定义及性质" class="headerlink" title="三、概率的定义及性质"></a>三、概率的定义及性质</h1><ul>
<li>概率的定义1：随机事件发生可能性大小的数学表征，其值在0 - 1之间，换句话说，概率是事件的函数。从集合到实数的一个映射。</li>
<li>概率的定义2：古典概型。<ul>
<li>古典概型的两个基本条件<ul>
<li>有限性：随机试验的结果只有有限个（n个）</li>
<li>等可能性：每个基本事件发生的可能性相同</li>
</ul>
</li>
<li>概率的计算，若事件A包含m个基本事件，则定义A的概率为$\frac{m}{n}$</li>
</ul>
</li>
<li>概率的定义3：统计定义（频率派）<ul>
<li>古典概型的两个条件往往不能满足。</li>
<li>独立重复做n次随机试验，n足够大，用事件A发生的频率作为其概率</li>
</ul>
</li>
<li>概率的定义4：主观概率（Bayer派）<ul>
<li>人们常谈论种种事件出现机会的大小，如某人有80%的可能性办成某事。而另一人则可能认为仅有50%的可能性。 即我们常常会拿一个数字去估计这类事件发生的可能性，而心目中并不把它与频率挂钩。</li>
<li>但是当前用频率来定义概率的频率派仍是数理统计的主流。焦点是频率派认为概率是客观存在，不可能因人而异。</li>
</ul>
</li>
<li>概率的定义5：公理化定义<ul>
<li>仅对概率运算规定一些简单的基本法则<ul>
<li>设A是随机事件，则 $0 \leq P(A) \leq 1$</li>
<li>必然事件的概率为1</li>
<li>若事件AB不相容，则 $P(A+B) = P(A) + P(B)$</li>
<li>若事件AB相容，则 $P(A+B) = P(A) + P(B) - P(AB)$</li>
</ul>
</li>
</ul>
</li>
<li>古典概型的计算要点：<ul>
<li>选择合适的样本空间</li>
<li>运用排列组合</li>
</ul>
</li>
</ul>
<h1 id="四、条件概率"><a href="#四、条件概率" class="headerlink" title="四、条件概率"></a>四、条件概率</h1><ul>
<li>定义：在一次试验中，在某事件B发生的条件下，事件A发生的概率</li>
<li>经典问题：卡片问题，有三张卡片，第一张两面都是黑色，第二张两面都是白色，第三张一黑一白。现在随机抽一张牌，这张牌黑色向上，问黑色向下的概率是多少？答案是2/3，白色向下的概率是1/3</li>
<li>计算：$P(A \mid B) = \frac{P(AB)}{P(B)}$ <ul>
<li>从下面的图可以看出，B发生下A发生的概率是指AB这部分占B的比例。条件概率和无条件概率的区别在于，事件A的样本空间发生了变化。</li>
<li><img src="/2018/08/27/%E3%80%90%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E3%80%91%E2%80%94%E2%80%94%E4%BA%8B%E4%BB%B6%E4%B8%8E%E6%A6%82%E7%8E%87/condition_proba.png" class=""></li>
<li>推出：$P(AB) = P(A\mid B) * P(B)$</li>
<li>乘法公式： <script type="math/tex">P(A_1A_2A_3 ... A_n) = P(A_1) * P(A_2\mid A_1) * P(A_3\mid A_1A_2) * ... * P(A_n\mid A_1A_2...A_{n-1})</script></li>
<li>例题1：一个电话，忘了最后一位，求三次之内打通电话的概率<ul>
<li>设A是事件三次之内打通电话，A的对立事件是前三次都没有打通电话，令$A_i$为第 $i$ 次打通电话，则<script type="math/tex">P(A) = 1- P(\overline{A_1}) * P(\overline{A_2} \mid \overline{A_1}) *P(\overline{A_3} \mid \overline{A_1A_2})  = 1 - \frac{9}{10} * \frac{8}{9} * \frac{7}{8} = 0.3</script></li>
</ul>
</li>
<li>例题2：有a个红球，b个黑球，问第k次抽到红球的概率<ul>
<li>解法1：将问题转变成，有 $a+b$ 个格子，往里面放球，求第 $k$ 个格子是红球的概率，样本空间为 $a+b$ 个格子中球的摆放情况，总情况个数 $(a+b)!$，第 $k$ 个格子为红球的情况：可以往第 $k$ 个格子放一个红球，剩下 $a+b-1$ 个格子的情况为 $(a+b-1)!$ ，因此概率为 $\frac{(a+b-1)!}{(a+b)!} = \frac{a}{a+b}$ </li>
<li>解法2：先简化问题，假设只有1个红球，$b$ 个黑球，求问第 $k$ 次抽到红球的概率，第 $k$ 次抽到红球，意味着前面 $k-1$ 次抽到的都是黑球，利用上面的乘法公式，设 $A<em>i$为第 $i$ 次抽到黑球，$A$ 为第 $k$ 次抽到红球，则$$P(A) = P(A_1) <em> P(A_2 \mid A_1) </em> P(A_3 \mid A_1A_2) <em> … </em> P(\overline{A_k} \mid A_1A_2…A</em>{k-1})<br>= \frac{b}{b+1} <em>\frac{b-1}{b} </em> \frac{b-2}{b-1}<script type="math/tex">$$* ... * \frac{1}{b-k+1} = \frac{1}{b+1}</script>，所以跟 $k$ 无关。<br>现在问题升级为 $a$ 个红球，$b$ 个黑球，我们可以将问题转成第 $k$ 次抽到红球一号或者红球二号或者红球三号或红球 $a$ 号的概率，显然不同红球之间是相互独立，因此整体概率等于各自概率之和，上面已经算过1个红球的情况，因此相加后概率为 $\frac{a}{a+b}$</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="五、全概率公式"><a href="#五、全概率公式" class="headerlink" title="五、全概率公式"></a>五、全概率公式</h1><ul>
<li>partition 定义：分割，或者完备事件群，假设样本空间为 $S$，$B_1 + B_2 + … + B_n = S$，且 $B_i$之间不相容，那么称 ${B_1, B_2, … , B_n}$ 为 $S$ 的一个分割。</li>
<li>一个自然的分割是事件 $B$ 和事件 $\overline{B}$。</li>
<li>全概率公式：$S$ 的一个分割为 ${B_1, B_2, … , B_n}$，某个事件为 $A$，$P(A) = \sum P(A\mid B_i) * P(B_i)$<ul>
<li>推导过程<ul>
<li>$P(A) = P(AS) = P(A\sum B_i ) = P(AB_1 \bigcup AB_2 \bigcup … \bigcup AB_n)$</li>
<li>$P(A) = \sum_1^n P(AB_i)$</li>
<li>$P(A) = \sum_1^n P(A \mid B_i) * P(B_i)$</li>
</ul>
</li>
</ul>
</li>
<li>注意这个不是全概率公式，但是结论确实很显然，$P(A\mid B) + P(\overline{A} \mid B) = 1$</li>
</ul>
<h1 id="六、贝叶斯公式"><a href="#六、贝叶斯公式" class="headerlink" title="六、贝叶斯公式"></a>六、贝叶斯公式</h1><ul>
<li>贝叶斯公式就是<strong>因果关系互换</strong></li>
<li>设${B_1, B_2, … , B_n}$是样本空间的一个分割，$A$ 为 $\Omega$ 中的一个事件，则<script type="math/tex; mode=display">P(B_i \mid A) = \frac{P(A \mid B_i) * P(B_i)}{\sum_1^n P(A \mid B_i) * P(B_i)}</script></li>
<li>由公式可以知道，分母就是事件 $A$ 的概率，而分子和等式左边的条件概率中的条件正好反过来，所以我们知道在因果关系互换时必须用贝叶斯公式。</li>
</ul>
<h1 id="七、事件的独立性"><a href="#七、事件的独立性" class="headerlink" title="七、事件的独立性"></a>七、事件的独立性</h1><ul>
<li>为了计算两个事件同时发生的概率，可以运用乘法定理，$P(AB) = P(A \mid B)P(B)$，什么情况下$P(AB) = P(A)P(B)$？即 $A$ 和 $B$ 同时发生的概率等于两个事件单独发生的概率乘积</li>
<li>因此事件独立的定义为：如果$P(AB) = P(A)P(B)$，事件 $A$ 和 $B$ 相互独立。即$A$ 的发生与 $B$ 没有任何关系</li>
<li>$P(B) = P(B \mid A)$意味着，无条件概率等于条件概率，意味着得到 $A$ 这个消息，对 $B$ 这个事件并没有任何作用</li>
<li>$AB$ 独立的话，$AB$ 的对立事件之间也是相互独立的<ul>
<li>假设 $A_1, A_2, …, A_n$ 是随机试验的 $n$ 个事件，他们之间相互独立，以 $B$ 表示 $A$ 或 $\overline{A}$ 之一，那么下列 $2^n$ 个等式都会成立<script type="math/tex; mode=display">P(B_1B_2...B_n) = P(B_1)P(B_2)...P(B_n)</script></li>
</ul>
</li>
<li>注意独立和不相容是两个完全不同的概念<ul>
<li>不相容是指两个事件没有公共部分，独立肯定是相容的，因为独立的定义就是求两个事件的公共部分</li>
</ul>
</li>
<li>例题：A、B、C三个人打飞机，分别打中飞机的概率为0.4，0.5，0.7，飞机命中一炮，两炮，三炮被击落的概率为0.2、0.6、1。求飞机被击落的概率。<ul>
<li>很直观的想法是用全概率公式，飞机被击中0123炮共4种情况，用全概率公式之前，先确定这四个情况是否是样本空间的一个分割，这显然是的，然后带入全概率公式</li>
<li>设事件 $X$ 为飞机被击落，事件 $B_i$ 为飞机被击中 $i$ 炮</li>
<li><script type="math/tex; mode=display">P(X) = P(X\mid B_0) * P(B_0) + P(X \mid B_1) * P(B_1) + P(X \mid B_2) * P(B_2) + P(X \mid B_3) * P(B_3)</script></li>
<li>$P(X \mid B_0) * P(B_0) = 0$，第一项忽略</li>
<li>求$P(B_1)$，注意$P(B_1) \neq P(A+B+C)$，后者表示的是至少打中一炮，所以<ul>
<li>$P(B_1) = P(A\overline{B}\overline{C} \bigcup\overline{A}B\overline{C} \bigcup\overline{A}\overline{B}C)$</li>
<li>首先ABC三人分别打中与否是相互独立的，因此他们的对立事件也是，<ul>
<li>所以 <script type="math/tex">P(A\overline{B}\overline{C}) = P(A) * P(\overline{B}) * P(\overline{C})</script> </li>
</ul>
</li>
<li>其次$A\overline{B}\overline{C}$  和 $\overline{A}B\overline{C}$两个之间是不相容的，为什么？因为出现事件 $A$ 和事件 $\overline{A}$，所以肯定没有公共部分，所以不相容的话，概率相加<ul>
<li>$P(A\overline{B}\overline{C} \bigcup\overline{A}B\overline{C} \bigcup\overline{A}\overline{B}C) = P(A\overline{B}\overline{C}) + P(\overline{A}B\overline{C}) + P(\overline{A}\overline{B}C)$</li>
</ul>
</li>
</ul>
</li>
<li>求 $P(B_2)$ 一样的道理</li>
</ul>
</li>
</ul>
<p><br></p>
]]></content>
      <tags>
        <tag>概率论</tag>
      </tags>
  </entry>
  <entry>
    <title>Wasserstein GAN</title>
    <url>/2019/08/19/Wasserstein-GAN/</url>
    <content><![CDATA[<blockquote>
<p>课程来源：李宏毅2018生成对抗网络课程<br>课程主页：<a href="http://speech.ee.ntu.tw/~tlkagk/courses_MLDS18.html">http://speech.ee.ntu.tw/~tlkagk/courses_MLDS18.html</a><br>文章梗概：WGAN 作者首先详细证明了为什么原始 GAN 训练困难的问题，然后提出解决方案，用 Wasserstein 距离来衡量两个分布之间的距离。</p>
</blockquote>
<span id="more"></span>
<p><br></p>
<h2 id="Original-GAN-review"><a href="#Original-GAN-review" class="headerlink" title="Original GAN review"></a>Original GAN review</h2><p>2014 年，Ian Goodfellow 提出的生成对抗网络由生成器和判别器两个网络组成，两个网络相互对抗，从而最后实现生成器能够生成出真实照片的功能。对于生成器，它尽可能地去模仿真实分布，而判别器尽可能地区分开真实分布和生成分布。</p>
<p>判别器的目标函数为：</p>
<script type="math/tex; mode=display">
V(G,D)=E_{x\sim P_{data}}[logD(x)]+E_{x\sim P_g}[log(1-D(x))]</script><p>而生成器的目标函数有两种：</p>
<ul>
<li>原始 GAN 论文（公式一）</li>
</ul>
<script type="math/tex; mode=display">
E_{x\sim P_g}[log(1-D(x))]</script><ul>
<li>the - log D trick（公式二）</li>
</ul>
<script type="math/tex; mode=display">
E_{x\sim{P_g}}[-logD(x)]</script><p><br></p>
<h2 id="GAN’s-problems"><a href="#GAN’s-problems" class="headerlink" title="GAN’s problems"></a>GAN’s problems</h2><p>GAN 提出来之后一直面临着很多问题，如训练困难，生成器和判别器的 loss 无法指导训练过程，生成样本缺乏多样性等，WGAN 论文就上面提到的两个生成器的损失函数进行分析，来解释为什么 GAN 会存在这些问题。</p>
<h3 id="公式一"><a href="#公式一" class="headerlink" title="公式一"></a>公式一</h3><p><strong>一句话概括：判别器训练得越好，生成器梯度消失越严重。</strong></p>
<p>在生成器固定参数的时候，最优的判别器是</p>
<script type="math/tex; mode=display">
D^*(x)=\frac{P_{data}(x)}{P_{data}(x)+P_g(x)}</script><p>判别器的评分等于一个样本 $x$ 来自真实分布和生成分布的可能性的相对比例，如果 $P<em>{data}(x) = 0$ 且 $P_g(x)\neq 0$，那就说明该样本来自生成器，这时判别器给出的评分就是0，反之如果 $P</em>{data}(x) \neq 0$ 且 $P_g(x)= 0$，那么判别器给出的评分就是1。</p>
<p>然而 GAN 训练有一个 trick，就是不能把判别器训练得太好，否则在实验中生成器是学不动的（loss 降不下去），为了探究背后的原因，我们可以看看在极端情况下（判别器最优），生成器的损失函数变成什么。</p>
<p>给公式一加上一项不依赖生成器的项，使之变成</p>
<script type="math/tex; mode=display">
E_{x\sim P_{data}}[log(D(x))]+E_{x\sim P_g}[log(1-D(x))]</script><p>当训练生成器的时候，判别器是固定的，假设判别器为最优判别器，那么生成器的目标函数即为：</p>
<script type="math/tex; mode=display">
E_{x\sim P_{data}}log\frac{P_{data}(x)}{\frac{1}{2}[P_{data}(x)+P_g(x)]}+E_{x\sim P_g}\frac{P_g(x)}{\frac{1}{2}[P_{data}(x)+P_g(x)]}-2log2</script><p>根据 JS 散度的定义，目标函数变成</p>
<script type="math/tex; mode=display">
2JS(P_{data}|P_g)-2log2</script><p>当这里，生成器的目标函数为两个分布之间的 JS 散度，训练生成器的目的是最小化该目标函数，即最小化两个分布之间的 JS 散度。判别器越接近最优判别器，生成器的目标函数就越能近似两个分布之间的 JS 散度。</p>
<p>问题就出在这个 JS 散度上，我们会希望如果两个分布之间越接近它们的 JS 散度越小，我们通过优化 JS 散度就能将 $P<em>g(x)$ 拉向 $P</em>{data}(x)$，最终以假乱真。这个希望在两个分布有所重叠的时候是成立的，但是如果两个分布完全没有重叠的部分，或者重叠的部分可忽略，那么 JS 散度就不能衡量他们之间的距离了。</p>
<p>根据 JS 散度和 KL 散度的定义</p>
<script type="math/tex; mode=display">
JS(P_1|P_2)=\frac{1}{2}KL(P_1|\frac{P_1+P_2}{2})+\frac{1}{2}KL(P_2|\frac{P_1+P_2}{2})</script><script type="math/tex; mode=display">
KL(P_1|P_2)=E_{x\sim P_1}log\frac{P_1}{P_2}</script><p>对于任意一个 $x$，存在四种情况</p>
<ul>
<li>$P_1(x)=0$，$P_2(x)= 0$：无 JS 散度。</li>
<li>$P_1(x)\neq 0$，$P_2(x)\neq 0$：由于重叠部分可以忽略，因此对 JS 散度的贡献也为 0。</li>
<li>$P_1(x)=0$，$P_2(x)\neq 0$：JS 散度为 $log2$。</li>
<li>$P_1(x)\neq 0$，$P_2(x)= 0$：JS 散度同为 $log2$。</li>
</ul>
<p>也就是说，无论 $P_{data}$ 和 $P_g$ 有多靠近，只要他们没有重叠，或者重叠部分可忽略，那么 JS 散度就是固定的常数 $log2$，那么对于生成器来说，目标函数的值应该不会变，这也就导致生成器接收不到任何的梯度信息。因此当判别器为最优判别器的时候，生成器接收不到梯度信息，即使判别器不是最优，生成器也极有可能面临梯度消失的问题。</p>
<p>而在绝大多数情况下，$P_{data}$ 和 $P_g$ 是不会有重合的，原因为</p>
<ul>
<li>真实分布和生成分布都只是高维空间下的一个低维流体，就好比二维空间中的两条线，即使有重合，也只是某几个点，可以忽略。</li>
<li>现实中我们只能得到真实分布的采样分布，如果没有足够多的样本，即使两个分布有重合，我们也很难得到真实重合部分。</li>
</ul>
<img src="/2019/08/19/Wasserstein-GAN/1.png" class="" width="500" height="500">
<p>因此，原始 GAN 训练不稳定的原因就是：如果判别器训练得太好，生成器梯度消失，loss 降不下去；如果判别器训练得不好，那么生成器的梯度不准。只有判别器训练得不好不坏才行，这就无形中增加了 GAN 的训练难度。</p>
<h3 id="公式二"><a href="#公式二" class="headerlink" title="公式二"></a>公式二</h3><p><strong>一句话概括：最小化第二种生成器 loss 函数，会等价于最小化一个不合理的距离衡量，导致两个问题，一是梯度不稳定，二是 collapse mode 即多样性不足。</strong></p>
<p>我们已知</p>
<script type="math/tex; mode=display">
E_{x\sim P_{data}}[log(D^*(x))]+E_{x\sim P_g}[log(1-D^*(x))]=2JS(P_{data}|P_g)-2log2</script><p>把 KL 散度变成含有 $D^*(x)$ 的形式</p>
<script type="math/tex; mode=display">
KL(P_g|P_{data})=E_{x\sim P_g}[log\frac{P_g(x)}{P_{data}(x)}]=E_{x\sim P_g}[log\frac{P_g(x)/(P_{data}(x)+P_g(x))}{P_{data}(x)/(P_{data}(x)+P_g(x))}]</script><script type="math/tex; mode=display">
=E_{x\sim P_g}[log\frac{1-D^*(x)}{D^*(x)}]=E_{x\sim P_g}log[1-D^*(x)]-E_{x\sim P_g}logD^*(x)</script><p>因此公式二的目标函数等价于</p>
<script type="math/tex; mode=display">
E_{x\sim P_g}[-logD^*(x)]=KL(P_g|P_{data})-2JS(P_{data}|P_g)+2log2+E_{x\sim P_{data}}logD^*(x)</script><p>注意上式最后两项不依赖于生成器 G，因此等价于最小化</p>
<script type="math/tex; mode=display">
KL(P_g|P_{data})-2JS(P_{data}|P_g)</script><p>这个等价最小化目标存在两个严重的问题。第一是它同时要最小化生成分布与真实分布的 KL 散度，却又要最大化两者的 JS 散度，一个要拉近，一个却要推远！这在直观上非常荒谬，在数值上则会导致梯度不稳定，这是后面那个 JS 散度项的毛病。</p>
<p>第二，即便是前面那个正常的 KL 散度项也有毛病。因为 KL 散度不是一个对称的衡量，$KL(P<em>g|P</em>{data})$ 与 $KL(P_{data}|P_g)$ 是有差别的，以前者为例</p>
<ul>
<li>当 $P<em>g(x) \rightarrow 0$ 而 $P</em>{data}(x) \rightarrow 1$ 时，</li>
</ul>
<script type="math/tex; mode=display">
KL(P_g|P_{data})=E_{x\sim P_g}log\frac{P_g(x)}{P_{data}(x)}=\int P_g(x)log\frac{P_g(x)}{P_{data}(x)}dx \rightarrow 0</script><ul>
<li>当 $P<em>g(x) \rightarrow 1$ 而 $P</em>{data}(x) \rightarrow 0$ 时，</li>
</ul>
<script type="math/tex; mode=display">
KL(P_g|P_{data})\rightarrow+\infty</script><p>换言之，$KL(P<em>g|P</em>{data})$ 对于上面两种错误的惩罚是不一样的，第一种错误对应的是生成分布还未能覆盖真实分布，惩罚微小；第二种错误对应的是“生成器生成了不真实的样本” ，惩罚巨大。第一种错误对应的是缺乏多样性，第二种错误对应的是缺乏准确性。<strong>这一放一打之下，生成器宁可多生成一些重复但是很“安全”的样本，也不愿意去生成多样性的样本，因为那样一不小心就会产生第二种错误，得不偿失。这种现象就是大家常说的 collapse mode。</strong></p>
<p><em>In Conclusion</em></p>
<p><strong>在原始 GAN 的（近似）最优判别器下，第一种生成器 loss 面临梯度消失问题，第二种生成器 loss 面临优化目标荒谬、梯度不稳定、对多样性与准确性惩罚不平衡导致 mode collapse 这几个问题。</strong></p>
<p><br></p>
<h2 id="Wasserstein-距离的优越性"><a href="#Wasserstein-距离的优越性" class="headerlink" title="Wasserstein 距离的优越性"></a>Wasserstein 距离的优越性</h2><p>Wasserstein 距离又叫 Earth-Mover 距离，定义如下</p>
<script type="math/tex; mode=display">
W(P_{data}, P_g)=inf_{\gamma\sim \Pi(P_{data},P_g)}E_{(x,y)\sim\gamma}||x-y||</script><p>$\Pi(P<em>{data},P_g)$ 是 $P</em>{data}$ 和 $P<em>g$ 组合起来的所有可能的联合分布的集合，反过来说，$\Pi(P</em>{data},P<em>g)$ 中每一个分布的边缘分布都是 $P</em>{data}$ 和 $P_g$ 。对于每一个可能的联合分布 $\gamma$ 而言，可以从中采样 $(x,y)\sim\gamma$ 得到一个真实样本 $x$ 和生成样本 $y$，并算出这对样本之间的距离，所以可以计算该联合分布 $\gamma$ 下对距离的期望值。在所有可能的联合分布中能够对这个期望值去到的下界，就定义为 Wasserstein 距离。</p>
<p>直观上可以把这个期望值理解为在 $\gamma$ 这个“路径规划”下把 $P<em>g$ 这堆沙土挪到 $P</em>{data}$ 所需的平均消耗，而 Wasserstein 距离则是“最优路径规划”下的消耗，所有才叫 Earth-Mover（推土机）距离。</p>
<p>Wasserstein 距离相比 KL 散度和 JS 散度的优越性在于，即便两个分布没有重叠，也能反映他们的远近。</p>
<p><br></p>
<h2 id="从-Wasserstein-距离到-WGAN"><a href="#从-Wasserstein-距离到-WGAN" class="headerlink" title="从 Wasserstein 距离到 WGAN"></a>从 Wasserstein 距离到 WGAN</h2><p>既然 Wasserstein 距离能够度量任意两个分布的距离，如果能够把它定义为生成器的 loss，不就可以产生有意义的梯度来更新生成器，使得生成分布被拉向真是分布了吗？</p>
<p>Wasserstein 距离定义中的下界无法直接求解，WGAN 作者将其变换成以下形式</p>
<script type="math/tex; mode=display">
W(P_{data},P_g)=\frac{1}{K}sup_{||f||_L\le K}(E_{x\sim P_{data}}[f(x)]-E_{x\sim P_g}[f(x)])</script><p>首先需要介绍 Lipschitz 连续，它是加在连续函数 $f$ 下的一个约束，要求存在一个常数 $K\ge0$ 使得定义域内的任意两个数 $x_1$ 和 $x_2$ 都满足</p>
<script type="math/tex; mode=display">
|f(x_1)-f(x_2)|\le K|x_1-x_2|</script><p>此时称函数 $f$ 的 Lipschitz 常数为 K。Lipschitz 约束了连续函数的变化幅度，要求函数不能太急，要平缓。</p>
<p>作者希望借助一个神经网络去拟合这样的一个函数 $f$，并且借助梯度上升的方法，取到该目标函数的最大值，至于 Lipschitz 约束，作者采取了一个简单的方法—— gradient clipping，限制神经网络的参数在某个范围。</p>
<p>到此为止，我们可以构造一个参数为 $w$ 、最后一层不是非线性激活层的判别器 $f<em>w$，在限制 $w$ 不超过某个范围的条件下，使得 $L=E</em>{x\sim P<em>{data}}[f_w(x)]-E</em>{x\sim P_g}[f_w(x)]$ 尽可能大，此时 $L$ 就会近似真实分布和生成分布的 Wasserstein 距离。</p>
<p>注意原始 GAN 的判别器是在做二分类问题，所以最后一层为 sigmoid，而 WGAN 的判别器做的是近似 Wasserstein 距离，所以最后一层不需要 sigmoid。</p>
<p>WGAN 的完整算法如下</p>
<img src="/2019/08/19/Wasserstein-GAN/2.png" class="" width="500" height="500">
<p>对于判别器，目标函数如下，上面已经证明过，该目标函数的最大值即为两个分布之间的 Wasserstein 距离，通过梯度上升法更新判别器，使其能够近似 Wasserstein 距离。</p>
<script type="math/tex; mode=display">
E_{x\sim P_{data}}D(x)-E_{x\sim P_g}D(x)</script><p>对于生成器，目标函数如下，找到最优判别器后，下面的目标函数可以等价于 Wasserstein 距离，然后通过梯度下降法更新生成器，从而使得 Wasserstein 距离减少。</p>
<script type="math/tex; mode=display">
-E_{x\sim P_g}D(x)</script><p>对比原始 GAN，WGAN 只改了四点：</p>
<ul>
<li>判别器最后一层去掉 sigmoid</li>
<li>生成器和判别器的 loss 不取 log</li>
<li>判别器更新采用 gradient clipping</li>
<li>不采用基于动量的优化算法，推荐 RMSProp，SGD</li>
</ul>
<p><br></p>
<h2 id="Improved-WGAN-WGAN-GP"><a href="#Improved-WGAN-WGAN-GP" class="headerlink" title="Improved WGAN (WGAN-GP)"></a>Improved WGAN (WGAN-GP)</h2><p>作者们发现 WGAN 有时候也会伴随样本质量低、难以收敛等问题。WGAN 为了保证 Lipschitz 限制，采用了 weight clipping 的方法，然而这样的方式可能过于简单粗暴了，因此他们认为这是上述问题的罪魁祸首。</p>
<p>他们提出的替代方案是给 Critic loss 加入 <strong>gradient penalty (GP)</strong>，这样，新的网络模型就叫 <strong>WGAN-GP</strong>。</p>
<p>作者认为，当且仅当一个可微函数的梯度范数（gradient norm）在任意处都不超过 1 时，该函数满足 1-Lipschitz 条件。</p>
<img src="/2019/08/19/Wasserstein-GAN/3.png" class="" width="500" height="500">
<p>但是我们不可能让所有的 $x$ 的 gradient norm 都小于 1，因此作者将条件放缩为对于所有 $x\sim P<em>{penalty}$，而 $P</em>{penalty}$ 为生成样本和真实样本之间的线性插值。</p>
<img src="/2019/08/19/Wasserstein-GAN/4.png" class="" width="500" height="500">
<p>而在 WGAN-GP 论文中，判别器的目标函数为</p>
<img src="/2019/08/19/Wasserstein-GAN/5.png" class="" width="500" height="500">
<p>至于为什么限制 gradient norm 趋向 1（two-sided penalty）而不是小于 1（one-sided penalty），作者给出的解释是，从理论上最优的 gradient norm 应当处处接近 1，对 Lipschitz 条件的影响不大，同时从实验中发现 two-sided penalty 效果比 one-sided penalty 略好。</p>
<img src="/2019/08/19/Wasserstein-GAN/6.png" class="" width="500" height="500">
<p><br></p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a href="https://arxiv.org/pdf/1701.04862.pdf">Towards principled methods for training generative adversarial networks</a></li>
<li><a href="https://arxiv.org/pdf/1701.07875.pdf">Wasserstein GAN</a></li>
<li><a href="https://arxiv.org/pdf/1704.00028.pdf">Improved training of wasserstein gans</a></li>
</ul>
<p><br></p>
]]></content>
  </entry>
  <entry>
    <title>Transformer简单介绍</title>
    <url>/2021/12/07/Transformer%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/</url>
    <content><![CDATA[<p>Transformer 是 Google 在 2017 年发表的 《Attention is all you need》论文中提出的，为了解决机器翻译等 seq2seq 任务中 RNN 不能并行化等问题，transformer 是一个完全由 self-attention 搭建起来的网络，完全抛弃了 CNN 和 RNN 等传统网络结构，从而实现了并行化，到目前为止已经完全取代 RNN 成为自然语言处理领域最好的特征提取器。</p>
<span id="more"></span>
<p><br></p>
<h3 id="self-attention"><a href="#self-attention" class="headerlink" title="self-attention"></a>self-attention</h3><p>上面提到 transformer 是由 self-attention 搭建起来的网络，这里就先介绍一下 self-attention 机制，NLP 任务的输入往往是一连串的文字，把它看成是一个由多个 token 组成的 sequence，那么 self-attention 想要做的事情就是计算 token 之间的相关性。</p>
<p>每一个 token 会由三个向量所表征，分别是 Q(query)、K(key) 和 V(value) 向量，当我们需要计算 token A 与其他 token 之间的相似度的时候，就用 token A 的 query 向量去和其他 token 的 key 向量去进行相似度计算，这里的相似度计算采用的是内积（因此 transformer 中的 <strong>Q 向量 和 K 向量的维度必须一致</strong>），内积后进行 softmax 操作得到与各个 token 之间的相似度，然后将相似度与各个 token 的 V 向量进行 weighted sum，最终得到 token A 的输出。其他 token 也是进行同样的操作得到各自的输出。</p>
<p>上面提到 transformer 可以并行化是因为，我们可以把上述的操作进行矩阵化，把 sequence 里的 token 进行拼接，我们可以得到三个矩阵 Q K V，下图表示的是两个 token，每个 token 的向量都是 3 维，计算出两个 token 的相似度，除以  $\sqrt d_k$ 是为了减小量级，加速收敛，然后进行 weighted sum 得到每个 token 的输出。</p>
<img src="/2021/12/07/Transformer%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/1.jpeg" class="" width="500" height="500">
<p><br></p>
<h3 id="Multi-head-Attention"><a href="#Multi-head-Attention" class="headerlink" title="Multi-head Attention"></a>Multi-head Attention</h3><p>上面提到的 self-attention 正是下图的 scaled dot-product attention，Mask 表示的是有目标性地选择 token 去进行 softmax 操作，因为为了方便训练，我们会设置一个超参是 maxlen，表示一个 sequence 的最大长度，少于这个长度的 sequence 会被后面填充特殊 [PAD]，在计算相似度的时候，这些 [PAD] 字符是没有意义的，所以我们要把它 mask 掉，计算真正有意义的 token 之间的相似度。</p>
<p>而下面右图的 multi-head attention 是 transformer 真正用到，所谓多头注意力机制类似 group convolution，将空间划分成多个子空间，然后再各个子空间里面计算 token 之间的相似度。比如一个 token 是由 512 维的 QKV 向量组成，那么将这些向量划成 8 个头，每个头为 64 份，然后分别对每一份计算 scaled dot-product attention，再把输出结果合并即可，这样做的目的是为了细粒度化，因为特征空间的每一个维度可能都对应着不同的信息，如果对整一个大的空间去进行相似度，可能会不准确，所以将空间切分进行细粒度的相似度计算。</p>
<img src="/2021/12/07/Transformer%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/2.png" class="" width="500" height="500">
<p><br></p>
<h3 id="positional-encoding"><a href="#positional-encoding" class="headerlink" title="positional encoding"></a>positional encoding</h3><p>上面提到的 self-attention 是可以进行矩阵操作的，因此这也是它可以并行化的原因，但是这同时也失去了时序的信息，我们把一个 sequence 打乱顺序输入，得到的结果会是一样的，因为上面的 self-attention 无法捕捉 token 的位置信息，因此我们需要在词嵌入的基础上加入位置的 embedding。</p>
<p>transformer 论文提出的位置编码如下，pos 表示第几个 token，i 表示的是 token 的第几个维度</p>
<img src="/2021/12/07/Transformer%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/3.png" class="" width="500" height="500">
<p>作者之所以设置这样的位置编码，是利用了 sin 和 cos 函数的性质，下一个位置的编码向量可以由前面的编码向量线性表示，等价于以一种非常容易学会的方式告诉了网络单词之间的绝对位置，让模型能够轻松学习到相对位置信息。</p>
<p><br></p>
<h3 id="Network-Structure"><a href="#Network-Structure" class="headerlink" title="Network Structure"></a>Network Structure</h3><p>transformer 的整体结构如下，和 seq2seq 模型结构一样，分为 encoder 部分和 decoder 部分，输入的 sequence 首先会对每个 token 进行词嵌入再加入位置编码得到输入的 sequence，然后经过 N=6 个的 encoder 得到输出。然后编码器的输出会输入到每一个的 decoder 里面，而 decoder 同样会有输入，这里详细说下训练和预测两个环节的差异。</p>
<p>transformer 在预测的时候，decoder 是串行的，比如机器翻译任务，输入中文句子到 encoder 得到输出，然后把 encoder 的输出和一个 [START] 特殊字符同时 输入到 decoder，表示解码任务的开始，然后解码器输出第一个翻译的 token A，然后将 [START] 和 token A 输入到 decoder 进行第二轮的解码，得到第二个翻译的 token B，依此类推，直到输出的 token 为 [END]。</p>
<p>Transformer 在训练的时候，decoder 是并行的，意味着只进行一轮的解码，假如翻译的句子是“我爱你”，那么翻译的结果应该是“I Love You”，此时 decoder 的输入为 [START] I Love You，decoder 的 ground-true 输出应该是 I Love You [END]，表示我输入 [START] 的时候应该输出 I，输入 I 的时候应该输出 Love。这种做法叫做 teacher-forcing，让训练过程接近预测过程。</p>
<img src="/2021/12/07/Transformer%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/4.png" class="" width="500" height="500">
<h4 id="encoder"><a href="#encoder" class="headerlink" title="encoder"></a>encoder</h4><p>编码器共有 N=6 个，每一个的结构都一样，由 multi-head attention 和 feed forward 两部分组成，add &amp; Norm 分别表示残差连接和 LayerNorm，每一个编码器的输入为上一个编码器的输出，输入之后经过矩阵变换分别得到 QKV 三个矩阵，这里采用的是 self-attention，因此输入都是一致的，和后面的解码器区分开来，解码器的 KV 来自于最后一层编码器的输出，Q 来自于解码器 masked 多头注意力的输出。</p>
<p>layernorm 是对特征空间进行归一化，即对 512 维进行归一化，区别于 batchnorm，是对于每一维的特征在 batch 维度进行归一化。之所以不用 bn，因为 bn 对特征每一个维度进行归一化，显然不同 batch（不同sequence）的某一个特征维度并没有关联，所以 NLP 领域多用 layernorm</p>
<h4 id="decoder"><a href="#decoder" class="headerlink" title="decoder"></a>decoder</h4><p>decoder 的输入有编码器最后一层的输出还有额外的经过位置编码的输入，结构由 maksed 多头注意力模块、encoder-decoder 注意力机制和前向反馈网络组成。</p>
<p><strong>masked multi-head attention</strong></p>
<p>这里要注意的是 <strong>masked</strong>，为什么要强调这个 masked 呢？是因为训练过程要模拟预测过程，即我在预测某一个词的时候我是不知道后面的词的，比如输入是 [START] I Love You，对于 Love，我只知道 [START] 和 I，因此在计算相似度的时候，后面的词要全部 mask 掉，包括 pad 的，只计算与前面 token 的相似度后 softmax。</p>
<p><strong>encoder-decoder attention</strong></p>
<p>这个区别于 self-attention，是计算输入的 token 和输出的某个 token 之间的 attention，因此 Q 向量来自于 masked 多头注意力的输出，KV 向量来自最后一层 encoder 的输出</p>
<h4 id="classifier"><a href="#classifier" class="headerlink" title="classifier"></a>classifier</h4><p>decoder 的输出维度为 [batch, sequence_length, d_ff]，然后需要经过一个 linear 层得到 [batch, sequence_length, vocab_size]，因为本质上是一个分类任务，因此 softmax 后与 GT 计算 cross entropy</p>
<p><em>reference</em></p>
<ol>
<li><a href="https://zhuanlan.zhihu.com/p/308301901">https://zhuanlan.zhihu.com/p/308301901</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/44731789">https://zhuanlan.zhihu.com/p/44731789</a></li>
<li><a href="https://github.com/graykode/nlp-tutorial/blob/master/5-1.Transformer/Transformer.py">transformer pytorch 实现</a>，注意 mask 的实现</li>
<li><a href="https://www.bilibili.com/video/BV1dR4y1E7aL?spm_id_from=333.999.0.0">上面代码的解读</a></li>
<li><a href="https://github.com/DA-southampton/NLP_ability/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/Transformer/%E7%AD%94%E6%A1%88%E5%90%88%E8%BE%91.md">一些面试常问的transformer的问题</a></li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>【数据挖掘比赛】————达观杯文本智能处理挑战赛</title>
    <url>/2018/09/24/%E3%80%90%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E6%AF%94%E8%B5%9B%E3%80%91%E2%80%94%E2%80%94%E2%80%94%E2%80%94%E8%BE%BE%E8%A7%82%E6%9D%AF%E6%96%87%E6%9C%AC%E6%99%BA%E8%83%BD%E5%A4%84%E7%90%86%E6%8C%91%E6%88%98%E8%B5%9B/</url>
    <content><![CDATA[<img src="/2018/09/24/%E3%80%90%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E6%AF%94%E8%B5%9B%E3%80%91%E2%80%94%E2%80%94%E2%80%94%E2%80%94%E8%BE%BE%E8%A7%82%E6%9D%AF%E6%96%87%E6%9C%AC%E6%99%BA%E8%83%BD%E5%A4%84%E7%90%86%E6%8C%91%E6%88%98%E8%B5%9B/header.png" class="">
<p><a href="http://www.dcjingsai.com/common/cmpt/%E2%80%9C%E8%BE%BE%E8%A7%82%E6%9D%AF%E2%80%9D%E6%96%87%E6%9C%AC%E6%99%BA%E8%83%BD%E5%A4%84%E7%90%86%E6%8C%91%E6%88%98%E8%B5%9B_%E7%AB%9E%E8%B5%9B%E4%BF%A1%E6%81%AF.html">“达观杯”文本智能处理挑战赛</a>是达观集团在<a href="http://www.dcjingsai.com/">DataCastle</a>举办的数据挖掘比赛，比赛全程一个人solo，最终排名为：40/3182(top2%)。</p>
<span id="more"></span>
<h2 id="比赛信息"><a href="#比赛信息" class="headerlink" title="比赛信息"></a>比赛信息</h2><h3 id="赛题任务"><a href="#赛题任务" class="headerlink" title="赛题任务"></a>赛题任务</h3><p>自然语言处理一直是人工智能领域的重要话题，而人类语言的复杂性也给 NLP 布下了重重困难等待解决。长文本的智能解析就是颇具挑战性的任务，如何从纷繁多变、信息量庞杂的冗长文本中获取关键信息，一直是文本领域难题。随着深度学习的热潮来临，有许多新方法来到了 NLP 领域，给相关任务带来了更多优秀成果，也给大家带来了更多应用和想象的空间。</p>
<p>此次比赛提供了一批长文本数据和分类信息，我们需要构建文本分类模型，实现精准分类。</p>
<h3 id="赛题数据"><a href="#赛题数据" class="headerlink" title="赛题数据"></a>赛题数据</h3><p>这是一个文本分类的任务，并且针对的是长文本，数据给定了长文本经过分词后的结果，并且数据进行了脱敏处理，无法知道原文本是什么内容，数据特征列如下：</p>
<ul>
<li>word_seg：str，文本分词后的每一个词，用空格间隔，一个词用一个数字代表</li>
<li>article：str，文本中的每一个字，同样用空格间隔，一个数字代表一个字。从这里可以看出文本应该是中文，存在字和词的信息。</li>
<li>class：int，文本所属的类别（共19个类）</li>
</ul>
<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><h3 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h3><p>文本信息不能作为分类模型的输入，我们必须要将文本的词信息转成数字信息，常用的方法有onehot，tf-idf，LDA，word2vec，在本次比赛中，我采用了tf-idf和word2vec这两种特征提取方法</p>
<p>sklearn中有TfidfVectorizer的工具可以直接将文本信息转成t-fidf矩阵，比较重要的参数如下</p>
<ul>
<li>ngram-range：tuple，ngram是指将多个词看成一个词的一种做法，这样做可以将词序考虑进去，而不是将每个词都看成是独立的。</li>
<li>min_df, max_df：int/double，这两个参数都是限定哪些词需要考虑的，出现太多次或者太少次都需要被过滤。</li>
<li>use_idf：默认为true</li>
<li>smooth_idf：默认为true防止分母为0。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">vec = TfidfVectorizer(ngram_range=(<span class="number">1</span>,<span class="number">3</span>),min_df=<span class="number">3</span>, max_df=<span class="number">0.9</span>,use_idf=<span class="number">1</span>,smooth_idf=<span class="number">1</span>, sublinear_tf=<span class="number">1</span>)</span><br><span class="line">x = vec.fit_transform(x)</span><br></pre></td></tr></table></figure>
<h3 id="单模型"><a href="#单模型" class="headerlink" title="单模型"></a>单模型</h3><h4 id="线性模型"><a href="#线性模型" class="headerlink" title="线性模型"></a>线性模型</h4><p>长文本提取出来的tf-idf矩阵维度达到了百万级，远远大于训练集的样本数，这时候选取线性模型的分类效果和训练时间都会比较好，因此我选择了逻辑回归分类器和线性svm作为基本的分类器，并且通过交叉验证和网格搜索找到较好的参数设置。</p>
<h4 id="深度学习模型"><a href="#深度学习模型" class="headerlink" title="深度学习模型"></a>深度学习模型</h4><ul>
<li><p>fasttext是Facebook提出的一个快速文本分类的模型，本质上是一个线性模型，因为它的网络没有用到激活函数，而fasttext重要的一点是用到了character-ngram的trick，但是在这个任务中， 由于数据脱敏了，所以这个trick就没有太大的作用</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">clf = fasttext.supervised(input_file = <span class="string">&#x27;fasttext_trainset.txt&#x27;</span>, output=<span class="string">&#x27;fasttext_model&#x27;</span>, label_prefix = <span class="string">&#x27;__label__&#x27;</span>, bucket = <span class="number">2000000</span>, word_ngrams = <span class="number">1</span>, ws = <span class="number">20</span>, epoch=<span class="number">40</span>, silent = <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>CNN模型用于文本分类，主要是将文本拼成一个矩阵，然后进行卷积操作，网络结构如下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">input_layer = Input(shape = (MAX_WORD_NUM,), dtype = <span class="string">&#x27;int32&#x27;</span>, name = <span class="string">&#x27;input&#x27;</span>)</span><br><span class="line">embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length= MAX_WORD_NUM, trainable = <span class="literal">True</span>)(input_layer)</span><br><span class="line"></span><br><span class="line">conv_output = []</span><br><span class="line"><span class="keyword">for</span> filter_ <span class="keyword">in</span> [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]:</span><br><span class="line">    conv = Conv1D(<span class="number">256</span>, filter_, padding=<span class="string">&#x27;same&#x27;</span>)(embedding_layer)</span><br><span class="line">    conv = Activation(<span class="string">&#x27;relu&#x27;</span>)(conv)</span><br><span class="line">    conv = GlobalMaxPool1D()(conv)</span><br><span class="line">    conv_output.append(conv)</span><br><span class="line"></span><br><span class="line">conv_output = concatenate(conv_output)</span><br><span class="line">full_connected_layer = Dense(<span class="number">256</span>)(conv_output)</span><br><span class="line">full_connected_layer = Dropout(<span class="number">0.3</span>)(full_connected_layer)</span><br><span class="line">full_connected_layer = Activation(<span class="string">&#x27;relu&#x27;</span>)(full_connected_layer)</span><br><span class="line">full_connected_layer = Dense(<span class="number">72</span>)(full_connected_layer)</span><br><span class="line">full_connected_layer = Activation(<span class="string">&#x27;relu&#x27;</span>)(full_connected_layer)</span><br><span class="line">full_connected_layer = Dense(<span class="number">19</span>)(full_connected_layer)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li><p>bi-lstm用于文本分类也很常见，lstm能够解决RNN梯度消失的问题，并且能够捕获文本长时间的依赖关系，网络结构如下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">input_layer = Input(shape = (MAX_WORD_NUM, ), dtype=<span class="string">&#x27;int32&#x27;</span>)</span><br><span class="line">embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=MAX_WORD_NUM, trainable = <span class="literal">True</span>)(input_layer)</span><br><span class="line">x = Dropout(<span class="number">0.3</span>)(embedding_layer)</span><br><span class="line">x = LSTM(units=<span class="number">128</span>, return_sequences= <span class="literal">True</span>)(embedding_layer)</span><br><span class="line">x = Dropout(<span class="number">0.3</span>)(x)</span><br><span class="line">x = Flatten()(x)</span><br><span class="line">x = Dense(units=<span class="number">19</span>)(x)</span><br></pre></td></tr></table></figure>
<h3 id="模型融合"><a href="#模型融合" class="headerlink" title="模型融合"></a>模型融合</h3><p>单模型效果最好的是几个线性模型，深度模型的效果都一般般，但通过最后的stacking，使得整体模型的效果有了一定的提升，stacking过程分5折</p>
<ul>
<li>第一层<ul>
<li>逻辑回归，搭配不同的solver</li>
<li>linear svc</li>
<li>fasttext</li>
<li>lstm</li>
<li>cnn</li>
</ul>
</li>
<li>第二层：lgb</li>
</ul>
</li>
</ul>
<h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><p>比赛代码可以到<a href="https://github.com/Vincent-Hoo/2018-Dc-DataGrand-TextClassification">这里</a>查看，比赛前十的答辩ppt如下：<a href="/2018/09/24/%E3%80%90%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E6%AF%94%E8%B5%9B%E3%80%91%E2%80%94%E2%80%94%E2%80%94%E2%80%94%E8%BE%BE%E8%A7%82%E6%9D%AF%E6%96%87%E6%9C%AC%E6%99%BA%E8%83%BD%E5%A4%84%E7%90%86%E6%8C%91%E6%88%98%E8%B5%9B/%E8%BE%BE%E8%A7%82%E6%9D%AF%E5%8D%81%E5%BC%BA%E5%88%86%E4%BA%AB%E6%B1%87%E6%80%BBppt.pdf" title="达观杯十强分享汇总ppt.pdf">达观杯十强分享汇总ppt.pdf</a></p>
<p><br></p>
]]></content>
  </entry>
  <entry>
    <title>【概率论与数理统计】————随机变量及其分布</title>
    <url>/2018/09/04/%E3%80%90%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E3%80%91%E2%80%94%E2%80%94%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E5%8F%8A%E5%85%B6%E5%88%86%E5%B8%83/</url>
    <content><![CDATA[<blockquote>
<p>课程名称：概率论与数理统计<br>开设学校：中科大<br>课程平台：icourse<br>第二章：随机变量及其分布，讲述离散型和连续型随机变量的概念，离散随机变量分布律，常见的离散随机变量的概率分布；随机变量的分布函数和概率密度函数</p>
</blockquote>
<span id="more"></span>
<h1 id="1-随机变量的概念"><a href="#1-随机变量的概念" class="headerlink" title="1. 随机变量的概念"></a>1. 随机变量的概念</h1><p>之前我们研究的概率，是基于样本空间下的一个随机试验，研究其发生的概率，若我们要研究一连串的随机试验及其它发生的概率，往往这一连串的事件之间有联系，我们就可以引入随机变量，用一个数字或者一个区间来表示这一连串的随机事件，如 $X$ 表示抛 5 次硬币正面向上的次数，那么我们就可以用 $X=3$ 来表示 5 次有 3 次正面向上这一个随机事件；</p>
<p>所以，随机变量就是一个数与一个事件的对应关系，相当于是一个随机变量的一个取值对应一个随机事件，随机变量的引入就是为了更加简洁地表示一个随机事件，不需要用一大串的文字描述。但是随机变量的引入更多是为了研究一连串相关的事件，而不只是随机变量某一取值时刻下的事件，往往是研究随机变量不同取值下的不同随机事件之间的联系。<br><br></p>
<h1 id="2-如何研究随机变量"><a href="#2-如何研究随机变量" class="headerlink" title="2. 如何研究随机变量"></a>2. 如何研究随机变量</h1><p>由于随机变量是表示一连串的事件，首先我们得确定随机变量可以取哪些值，即确定有哪些随机事件。其次我们需要研究随机变量取不同值下所表示的随机事件发生的概率。</p>
<p>之前说到，我们用一个数字或者一个区间来表示一个随机事件，这里面就包含了两种随机变量：离散型随机变量和连续性随机变量，前者随机变量的取值是可数的或者有限的，后者不可数。<br><br></p>
<h1 id="3-离散随机变量的分布"><a href="#3-离散随机变量的分布" class="headerlink" title="3. 离散随机变量的分布"></a>3. 离散随机变量的分布</h1><p>当随机变量取不同取值的时候，所表示的随机事件的概率会不同，对于离散型的随机变量，我们可以用表格的形式，列出不同取值下的概率，这就是随机变量的概率分布，即不同取值下所表示的随机事件的概率值，可以统一用以下的式子来表示。此式子叫做分布律</p>
<script type="math/tex; mode=display">P(X = x_k) = p_k，k = 1,2,...</script><p>且满足</p>
<ol>
<li>$p_k \ge 0，k = 1,2,…$</li>
<li>$\sum_1^{+\infty}p_k = 1$ </li>
</ol>
<p>或者用表格来表示</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">$X$</th>
<th style="text-align:center">$x_1$</th>
<th style="text-align:center">$x_2$</th>
<th style="text-align:center">$x_3$</th>
<th>$…$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$p_k$</td>
<td style="text-align:center">$p_1$</td>
<td style="text-align:center">$p_2$</td>
<td style="text-align:center">$p_3$</td>
<td>…</td>
</tr>
</tbody>
</table>
</div>
<h2 id="3-1-离散均匀分布"><a href="#3-1-离散均匀分布" class="headerlink" title="3.1 离散均匀分布"></a>3.1 离散均匀分布</h2><ul>
<li>分布律：$ P(X = x_k) = \frac{1}{n}$， $k = 1,2,…,n $</li>
<li>其实就是我们之前所说的古典概型，古典概型满足两个特性，有限性和等可能性，实际上描述的就是离散均匀分布，随机变量的每个取值对应一个基本事件，每一个基本事件的概率都是一样。</li>
</ul>
<h2 id="3-2-二项分布"><a href="#3-2-二项分布" class="headerlink" title="3.2 二项分布"></a>3.2 二项分布</h2><ul>
<li><p>在一次试验中事件 $A$ 发生的概率为 $p$ ，把这种试验重复做 $n$ 次，若把 $X$ 记为 $n$ 次试验中事件 $A$ 发生的次数，$X$ 可以取 $0-n$，$P(X = k) = C_n^k p^k (1-p)^{n-k}$ ，称 $X$ 服从二项分布，记为 $X \sim B(n, p)$   $~$ </p>
<img src="/2018/09/04/%E3%80%90%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E3%80%91%E2%80%94%E2%80%94%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E5%8F%8A%E5%85%B6%E5%88%86%E5%B8%83/%E4%BA%8C%E9%A1%B9%E5%88%86%E5%B8%83.png" class="">
</li>
<li><p>伯努利试验：一次试验中可能的结果为 $A$ 和 $A$ 的对立事件，重复做这样的试验</p>
</li>
<li><p>若 $n=1$，则变为 $0-1$ 分布，或叫两点分布或伯努利分布，$X$ 只取0，1两个值，$P(X = 1) = p$, $P(X = 0) = 1 - p$，$X = 1$表示 $A$ 发生，$X = 0$ 表示 $A$ 不发生。两点分布是最简单的一种分布，任何一个只有两种可能结果的随机现象，如明天是否下雨，都属于两点分布</p>
</li>
<li><p>任何概率分布都应该满足 $P(X = x_k) \ge 0$,   $ \sum_1^n P(X = x_k) = 1 $，二项分布也满足，二项式定理展开就可以证明</p>
<ul>
<li>$\sum_1^n P(X = x_k) = \sum_1^n C_n^{k} p^k(1-p)^{n-k} = (p + 1-p)^n = 1$</li>
</ul>
</li>
<li><p>求解 $P(X = x_k)$取最大值的 $k$ 值</p>
<ul>
<li>令 $b_k = P(X = x_k)  = C_n^k p^k(1-p)^{n-k} = \frac{n!}{k!(n-k)!}p^k(1-p)^{n-k}$ </li>
<li>最大值应该满足 $\frac{b<em>k}{b</em>{k-1}} \ge 1$ 且 $\frac{b<em>{k}}{b</em>{k+1}} \ge 1$</li>
<li>解得 $p(n+1)-1 \le k \le p(n+1)$  </li>
<li>若 $p(n+1)$ 为整数，则最大值时k有两个值</li>
</ul>
</li>
<li><p>例题：若100个人射飞机，射中飞机的概率为0.02，假设 $X$ 为射中飞机的人数，那么 $X \sim B(100, 0.02)$ ，100个人本来是相互独立地射飞机，但是可以看成100次重复的随机试验，所以满足二项分布</p>
</li>
</ul>
<h2 id="3-3-泊松分布"><a href="#3-3-泊松分布" class="headerlink" title="3.3 泊松分布"></a>3.3 泊松分布</h2><ul>
<li>分布律：$P(X = x_k) = e^{-\lambda} \frac{\lambda^k}{k!}$ </li>
<li>描述稀有事件发生的概率，交通路口发生事故的次数满足泊松分布</li>
<li>随机变量取值为：$[ 0, +\infty)$ </li>
<li>泊松分布可以作为二项分布的一种近似<ul>
<li>$X \sim B(n, p)$，$n$ 很大，$p$ 很小，$np$ 较小，$P(X = k) = C_n^k p^k (1-p)^{n-k}\approx e^{-\lambda} \frac{\lambda^k}{k!}$, $\lambda = np$ <ul>
<li>$P(X = k) = C_n^k p^k (1-p)^{n-k} = \frac{n(n-1)…(n-k+1)}{k!}p^k(1-p)^{n-k}$  </li>
<li>因为 $p$ 很小，根据泰勒展开，$1 - p \approx e^{-p}$ ，所以 $(1-p)^{n-k} \approx e^{-p(n-k)}$ </li>
<li>由于泊松分布一般描述稀有事件，即 $k$ 较小，由因为 $n$ 很大，所以 $n - k \approx n$ ，$e^{-p(n-k)} \approx e^{-pn} = e^{-\lambda}$ </li>
<li>同样因为 $k$ 较小，$n$ 很大，$\frac{n(n-1)…(n-k+1)}{k!} p^k \approx \frac{(np)^k}{k!} = \frac{\lambda^k}{k!}$   </li>
</ul>
</li>
<li>二项分布不好算，可以用泊松分布近似计算，从上面的推导可以看出 $k$ 不能很大</li>
<li>$n$ 很大，$np$ 很小，准确点说 $n &gt;30$，$np &lt; 5$ 即可<img src="/2018/09/04/%E3%80%90%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E3%80%91%E2%80%94%E2%80%94%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E5%8F%8A%E5%85%B6%E5%88%86%E5%B8%83/%E6%B3%8A%E6%9D%BE%E5%88%86%E5%B8%83.png" class="">
</li>
</ul>
</li>
</ul>
<p>二项分布当n等于1时，可以转成两点分布；当n很大，p很小的时候，可以转成泊松分布。</p>
<p><br></p>
<h1 id="4-分布函数和密度函数"><a href="#4-分布函数和密度函数" class="headerlink" title="4. 分布函数和密度函数"></a>4. 分布函数和密度函数</h1><h2 id="4-1-分布函数"><a href="#4-1-分布函数" class="headerlink" title="4.1 分布函数"></a>4.1 分布函数</h2><p>上面提到的都是离散型随机变量的概率分布 $P(X =x_k) = p_k,  k = 0,1,2,…$ ，离散型随机变量的取值是有限的或者是可数的，离散型变量的一个取值对应一个随机事件，而对于连续型变量，通常用一段区间来表示一个随机事件，如 $P(a &lt; X \le b)$，这个概率又可以表示为 $P(X \le b) - P(X \le a)$。</p>
<ul>
<li><p>定义：$P(X \le x) = F(x)$，$-\infty &lt; x &lt; +\infty$。随机变量 $X$ 的分布函数</p>
</li>
<li><p>分布函数直观来说就是表示随机变量取值小于某个值的概率，它具有以下几个性质</p>
<ul>
<li>分布函数是非递减的，且 $lim<em>{x\to -\infty} F(x) =0$ ，$lim</em>{x\to +\infty} F(x) =1$，从分布函数的定义就可以看出。</li>
<li>离散型随机变量的分布函数不连续，因为离散变量的取值是有限的，因此分布函数会呈现出阶梯状，且是右连续的<ul>
<li>$F(x) = P(X \le x)=\sum<em>{x</em>{k} \le x} P(X = x_k) $ </li>
</ul>
</li>
<li>连续型随机变量的分布函数是连续且递增的，有了分布函数，某个区间的概率值就很容易计算，$P(a &lt; X \le b) = F(b) - F(a)$，<img src="/2018/09/04/%E3%80%90%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E3%80%91%E2%80%94%E2%80%94%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E5%8F%8A%E5%85%B6%E5%88%86%E5%B8%83/%E5%88%86%E5%B8%83%E5%87%BD%E6%95%B0.png" class="">
</li>
</ul>
</li>
</ul>
<ul>
<li>注意区分以下分布律和分布函数，分布律是描述离散随机变量遵循的分布，简单来说就是取不同值的概率；而分布函数是随机变量小于某个值的概率</li>
</ul>
<h2 id="4-2-密度函数"><a href="#4-2-密度函数" class="headerlink" title="4.2 密度函数"></a>4.2 密度函数</h2><p>随机变量的分布函数 $F(x)$ 是描述随机变量取值小于某个值下的概率的函数，而随机变量的密度函数 $f(x)$ 满足以下几个条件，注意并不是所有的随机变量或者分布函数都有对应的密度函数，但是所有的随机变量都有分布函数</p>
<ol>
<li>$f(x)$ 是非负的</li>
<li>$\int_{-\infty}^{+\infty} f(x)dx=1$ </li>
<li>$F(x) = \int_{-\infty}^{x}f(t)dt$  </li>
</ol>
<p>从上面的定义可以看出，密度函数 $f(x)$ 是分布函数 $F(x)$ 的微分，而 $F(x)$ 是 $f(x)$ 的积分，所以计算以下概率可以转变成求密度函数的积分，注意无论是分布函数还是密度函数，还是分布律，他们都是服务于算概率的</p>
<script type="math/tex; mode=display">P(a < X \le b) = F(b) - F(a) = \int_a^b f(x)dx</script><p>上面定义的第二点，也很容易理解，$\int_{-\infty}^{+\infty} f(x)dx=F(+\infty)-F(-\infty)=1-0=1$  </p>
<p>对于连续型随机变量 $P(X = a) = 0$，因为 $P(X = a) = lim<em>{h\to 0} \int</em>{a-h}^{a}f(x)dx=0$ </p>
<p>分布函数连续不一定是连续型随机变量的分布函数，连续不一定可微，只有密度函数存在，才能说是连续随机变量</p>
<h2 id="4-3-总结一下"><a href="#4-3-总结一下" class="headerlink" title="4.3 总结一下"></a>4.3 总结一下</h2><p>对于离散型的随机变量，只可以取有限个或者可数个值，我们可以用分布律去描述它，$P(X = x_k) = p_k$   </p>
<p>对于连续型的随机变量，我们用一个区间去表示一个随机事件，但由于区间涉及两个边界值，我们引入分布函数 $F(x)$ 描述随机变量取值小于某个值的概率，进而引入密度函数 $f(x)$，然后概率值就可以转化为求密度函数的积分值</p>
<p>但是对于离散型随机变量，我们也可以研究它的分布函数，但是它没有概率密度函数，因为它的分布函数不可微</p>
<p><br></p>
<h1 id="5-连续随机变量的分布"><a href="#5-连续随机变量的分布" class="headerlink" title="5. 连续随机变量的分布"></a>5. 连续随机变量的分布</h1><p>对于离散随机变量，我们可以用分布律来描述，对于连续型随机变量，我们可以用分布函数或密度函数来描述，分布律、分布函数、密度函数都是为了算概率而存在的</p>
<h2 id="5-1-均匀分布"><a href="#5-1-均匀分布" class="headerlink" title="5.1 均匀分布"></a>5.1 均匀分布</h2><ul>
<li><p>若一个随机变量的密度函数为以下的形式，则该随机变量服从均匀分布，$X \sim U(a,b)$ </p>
<script type="math/tex; mode=display">f(x)= \left\{\begin{array}{cc} 
\frac{1}{b-a}, & a \le x \le b\\ 
0, & other\ values 
\end{array}\right.</script></li>
<li><p>均匀分布可以用示性函数 $I$ 来表示，$f(x) = \frac{1}{b-a}I_{a\le x\le b}(x)$ </p>
</li>
</ul>
<h2 id="5-2-指数分布"><a href="#5-2-指数分布" class="headerlink" title="5.2 指数分布"></a>5.2 指数分布</h2><ul>
<li><p>指数分布的随机变量，密度函数满足 $f(x) =\lambda e^{-\lambda x} I_{x \ge 0}(x)$，$\lambda &gt; 0$，$X \sim exp(\lambda)$  </p>
<script type="math/tex; mode=display">f(x)= \left\{\begin{array}{cc} 
\lambda e^{-\lambda x}, & x \ge 0\\ 
0, & x < 0 
\end{array}\right.</script></li>
<li><p>$\lambda$ 越大，密度函数下降得越快</p>
<img src="/2018/09/04/%E3%80%90%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E3%80%91%E2%80%94%E2%80%94%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E5%8F%8A%E5%85%B6%E5%88%86%E5%B8%83/%E6%8C%87%E6%95%B0%E5%88%86%E5%B8%83.png" class="">
</li>
<li><p>指数分布最大的特点是无后效性，通常用来描述元件的寿命，即寿命是无老化的，假如我知道一个元件已经用了 $s$ 个小时，想知道原件能用 $s + t$ 个小时的概率，它就等于原件用 $t$ 个小时的概率</p>
<ul>
<li>$P(X &gt; s+t \mid X&gt;s) = P(X &gt;t)$  </li>
<li>证明：<ul>
<li>令事件 $A$ 为 $X &gt; s+t$，事件 $B$ 为 $X &gt; s$，则 $P(A \mid B) = \frac{P(AB)}{P(B)}$  </li>
<li>可以看出 $B \subset A$，所以 $P(AB) = P(A)$ </li>
<li>$P(A) = \int_{s+t}^{+\infty} \lambda e^{-\lambda x}dx$ </li>
<li>令 $\lambda x=u$，则 $P(A) = \int<em>{\lambda(s+t)}^{+\infty} e^{-u}du = -e^{-u} \mid</em>{\lambda(s+t)}^{+\infty} = e^{-\lambda(s+t)}$  </li>
<li>同理，$P(B)=e^{-\lambda s}$  </li>
<li>因此，$P(A \mid B) = \frac{P(A)}{P(B)} = e^{-\lambda t} = P(X &gt; t)$  </li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="5-3-正态分布"><a href="#5-3-正态分布" class="headerlink" title="5.3 正态分布"></a>5.3 正态分布</h2><ul>
<li>密度函数满足 $f(x) = \frac{1}{\sqrt{2\pi} \sigma} exp(-\frac{(x-\mu)^2}{2\sigma^2})$，$-\infty &lt; x &lt; +\infty$，记为 $X \sim N(\mu, \sigma^2)$    </li>
<li>密度函数的曲线有以下性质<ul>
<li>对称函数，对称轴为 $x = \mu$，对称轴左边单调递增，对称轴右边单调递减，极值落在 $x = \mu$ 上，为$\frac{1}{\sqrt{2\pi} \sigma}$</li>
<li>$\sigma$ 的大小决定了密度函数的陡峭程度</li>
</ul>
</li>
<li>当 $\mu = 0$，$\sigma = 1$ 的时候，称为标准正态分布，密度函数记为 $\phi(x)$，分布函数记为 $\Phi(x)$ <ul>
<li>分布函数 $\Phi(x)$ 是密度函数 $\phi(x)$ 的积分，$\Phi(x) = \int_{-\infty}^x \phi(t)dt$ </li>
<li>$\Phi(x) = 1 - \Phi(-x)$，可以从密度函数 $\phi(x)$ 算面积很容易证出。</li>
</ul>
</li>
<li>一般的正态分布 $X \sim N(\mu, \sigma^2)$ 都可以转成标准正态分布 $X \sim N(0, 1)$，其分布函数 $F(x) = \Phi(\frac{x-\mu}{\sigma})$ ，然后通过查标准正态分布表格来算出概率。</li>
</ul>
<p><br></p>
<h1 id="6-多维随机变量"><a href="#6-多维随机变量" class="headerlink" title="6. 多维随机变量"></a>6. 多维随机变量</h1><p>多维随机变量也称随机向量，跟一维随机变量一样，多维随机变量就是多个随机事件的并集，或者说是多个事件的联合概率，像一维随机变量，我们同样关注多维随机向量的分布。</p>
<h2 id="6-1-二维离散型随机向量的分布"><a href="#6-1-二维离散型随机向量的分布" class="headerlink" title="6.1 二维离散型随机向量的分布"></a>6.1 二维离散型随机向量的分布</h2><p>$P(X=x<em>i, Y=y_j)=p</em>{i,j}$ </p>
<p> 同样应该满足密度分布的基本条件</p>
<ol>
<li>$p_{i,j}&gt;0$</li>
<li>$\sum\sum p_{ij}=1$ </li>
</ol>
<h2 id="6-2-二维连续型随机向量的分布"><a href="#6-2-二维连续型随机向量的分布" class="headerlink" title="6.2 二维连续型随机向量的分布"></a>6.2 二维连续型随机向量的分布</h2><p>类似一维随机变量，我们用分布函数来描述它的概率分布，$F(x,y)$ 称为随机向量 $(X,Y)$ 的联合分布函数。</p>
<p>$F(x, y)=P(X \le x, Y \le y)$ </p>
<p>将二维随机变量 $(X,Y)​$ 看成是平面上随机点的坐标，那么，分布函数 $F(x,y)​$ 在点 $(x,y)​$ 处的函数值就是下图无穷矩形域内的面积占整个平面的概率。<br><img src="/2018/09/04/%E3%80%90%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E3%80%91%E2%80%94%E2%80%94%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E5%8F%8A%E5%85%B6%E5%88%86%E5%B8%83/%E4%BA%8C%E7%BB%B4%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F.png" class=""></p>
<p>$F(x,y)$ 有如下性质：</p>
<ol>
<li>给定 $y$，$F(x,y)$ 是 $x$ 的非减函数；给定 $x$, $F(x,y)$ 是 $y$ 的非减函数</li>
<li>$\lim<em>{x \rightarrow -\infty} F(x,y)=0$，$\lim</em>{y \rightarrow -\infty} F(x,y)=0$，$\lim_{x \rightarrow +\infty, y \rightarrow +\infty} F(x,y)=1$ </li>
<li>$P(a&lt;X \le b, c &lt; Y \le d) = F(b,d) - F(a,d) - F(b,c) + F(a,c)$     </li>
<li>$P(X \le x_i) = P(X \le x_i, Y \le +\infty) = F(x_i, +\infty)$ </li>
<li>$F(x,y)$ 分别是关于 $x$ 和 $y$ 右连续</li>
</ol>
<p>同样从分布函数引出概率密度函数，若对分布函数 $F(x,y)$，存在 $f(x,y) \ge 0$，使得对任一 $(x,y) \in R^2$，有 $F(x,y)=\int<em>{-\infty}^{x} \int</em>{-\infty}^{y} f(u,v)dudv$，则称 $F(x,y)$ 为连续型联合分布函数，$f(x,y)$ 为联合密度函数。 $f(x,y)$ 同样具有以下性质：</p>
<ol>
<li>$f(x,y) \ge 0$  </li>
<li>$\int<em>{-\infty}^{+\infty} \int</em>{-\infty}^{+\infty} f(x,y)dxdy=1$ </li>
<li>$\frac{\partial F(x,y)}{\partial x \partial y} = f(x,y)$ </li>
</ol>
<p>几何上，$z=f(x,y)$ 表示空间的一个曲面，$\int<em>{-\infty}^{+\infty} \int</em>{-\infty}^{+\infty} f(x,y)dxdy=1$ 表示介于 $f(x,y)$ 与 $xOy$  平面之间的空间区域的面积。<br><img src="/2018/09/04/%E3%80%90%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E3%80%91%E2%80%94%E2%80%94%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E5%8F%8A%E5%85%B6%E5%88%86%E5%B8%83/%E4%BA%8C%E7%BB%B4%E6%AD%A3%E6%80%81.png" class=""></p>
<p><br></p>
<h1 id="7-边缘分布-marginal-distribution"><a href="#7-边缘分布-marginal-distribution" class="headerlink" title="7. 边缘分布(marginal distribution)"></a>7. 边缘分布(marginal distribution)</h1><ul>
<li><p>定义：已知 $(X,Y)$ 的联合分布函数为 $F(x,y)$，则 $X$ 的分布 $F_1(x)$ 和 $Y$ 的分布 $F_2(y)$ 称为 $F(x,y)$ 的边缘分布</p>
<ul>
<li>$F_1(x)=P(X \le x)=P(X \le x, Y \le +\infty)=F(x, +\infty)$ </li>
<li>$F_2(y)=P(Y \le y)=P(Y \le y, X \le +\infty) = F(+\infty, y)$ </li>
</ul>
</li>
<li><p>二维离散型随机变量的边缘分布</p>
<ul>
<li>二维离散随机变量的分布一般以联表的形式表示</li>
<li>固定某个 $x<em>i$，$P(X=x_i) = \sum</em>{j=1}^{m}p<em>{ij}$，联表每一行的求和就是随机变量 $X$ 的边缘分布，可记为 $p</em>{i\bullet}$ </li>
<li>固定某个 $y<em>i$ ，$P(Y=y_j) = \sum</em>{i=1}^{n}p<em>{ij}$ ，联表每一列的求和就是随机变量 $Y$ 的边缘分布，可记为 $p</em>{\bullet j}$ </li>
</ul>
</li>
<li><p>二维连续型随机变量的边缘分布</p>
<ul>
<li>二维连续随机变量的分布一般以概率密度函数的形式表示</li>
<li>从联合密度函数，求 $X$ 的密度函数，就需要将随机变量 $Y$ 从积分中去掉，$f<em>X(u) = \int</em>{-\infty}^{+\infty}f(u,v)dv$ <ul>
<li>$F<em>X(x) = F(x, +\infty)=\int</em>{-\infty}^x[\int<em>{-\infty}^{+\infty}f(x,y)dy]dx=\int</em>{-\infty}^{x}f_X(x)dx$ </li>
</ul>
</li>
<li>同理，求 $Y$ 的密度函数，，$f<em>Y(v) = \int</em>{-\infty}^{+\infty}f(u,v)du$    </li>
</ul>
</li>
<li><p>二维正态分布的边缘分布就是一维正态分布</p>
</li>
<li><p>注意：联合分布可以推出边缘分布，但是边缘分布不能推出联合分布，例子：一维正态分布推不出二维正态分布，二维正态分布还有一个参数 $\rho$</p>
</li>
</ul>
<p><br></p>
<h1 id="8-条件分布"><a href="#8-条件分布" class="headerlink" title="8. 条件分布"></a>8. 条件分布</h1><p>假设有两个随机变量 $X,Y$，在给定 $Y$ 取某个或某些值的条件下，$X$ 的概率分布就叫做 $Y = y_j$ 条件下 $X$ 的条件分布律。</p>
<h2 id="8-1-离散型随机变量的条件分布"><a href="#8-1-离散型随机变量的条件分布" class="headerlink" title="8.1 离散型随机变量的条件分布"></a>8.1 离散型随机变量的条件分布</h2><ul>
<li>定义：设 $(X,Y)$ 为二维离散型随机变量，若对固定的 $j$ ，称 $P(X=x<em>i \mid Y=y_j) = \frac{p</em>{ij}}{p<em>{\bullet j}}$ 为 $Y = y_j$ 条件下的 $X$ 的条件分布律；同理，对固定的 $i$ ，称 $P(Y=y_j \mid X=x_i) = \frac{p</em>{ij}}{p_{i\bullet}}$  为 $X = x_i$ 条件下的 $Y$ 的条件分布律；</li>
<li>条件分布是一种概率分布，它具有概率分布的一切性质，正如条件概率是一种概率，具有概率的一切性质一样。<ul>
<li>$P(X=x_i \mid Y=y_j) \ge 0$</li>
<li>$\sum_{i=1}^{\infty} P(X=x_i \mid Y=y_j) =1$ </li>
</ul>
</li>
<li>离散型随机变量的条件分布，就是在联合分布的联表的某一行或者某一列的分布。</li>
<li>由条件分布和边缘分布，可以根据乘法公式，求得联合分布<ul>
<li>$P(X=x_i, Y=y_j) = P(X=x_i)P(Y=y_j \mid X= x_i)$ </li>
</ul>
</li>
</ul>
<h2 id="8-2-连续型随机变量的条件分布"><a href="#8-2-连续型随机变量的条件分布" class="headerlink" title="8.2 连续型随机变量的条件分布"></a>8.2 连续型随机变量的条件分布</h2><ul>
<li><p>定义：设 $X$ 和 $Y$ 的联合概率密度为 $f(x,y)$，$(X,Y)$ 关于 $Y$ 的边缘概率密度为 $f<em>Y(y)$，若对于固定的 $y$，$f_Y(y) &gt;0$，则称 $\frac{f(x,y)}{f_Y(y)}$ 为在 $Y = y$ 的条件下 $X$ 的条件概率密度，记为 $f</em>{X \mid Y}(x \mid y) = \frac{f(x,y)}{f<em>Y(y)}$，同理，$f</em>{Y \mid X}(y \mid x) = \frac{f(x,y)}{f_X(x)}$ </p>
</li>
<li><p>由条件密度和边缘密度，可以得到联合密度</p>
<ul>
<li>$f(x,y)=f<em>X(x)f</em>{Y \mid X}(y \mid x)$ </li>
</ul>
</li>
<li>例题：从 [0,1] 中随机取一个数 $X$， 则 $X \sim U(0,1)$，当 $X=x$ 时，再从 $(0,x)$ 中随机取一个数 $Y$，求 $Y$ 的分布。<ul>
<li>题目给定 $X$ 的分布，以及在 $X = x$ 下 $Y$ 的条件分布，自然可以算出 $(X,Y)$ 的联合分布，再从联合分布就可以算出 $Y$ 的边缘分布</li>
<li>$f<em>X(x)=I</em>{(0,1)}(x)$，$f<em>{Y \mid X}(y \mid x) = \frac{1}{x}I</em>{(0,x)}(y)$ </li>
<li>联合分布：$f(x,y) = I<em>{(0,1)}(x) \frac{1}{x}I</em>{(0,x)}(y)$ </li>
<li>边缘分布：$f_Y(y)=\int f(x.y)dx$，算这种积分最重要的就是搞清楚，谁变谁不变，以及积分的上下限是多少，显然被积的是 $x$，所以 $x$ 是变量，$y$ 是常数，所以我们固定某一个 $y$，可以得出 $x$ 的范围是 $[y,1]$，因此 $f_Y(y)=-ln(y)$ ，最后限定 $y$ 的范围是 $(0,1)$ </li>
</ul>
</li>
</ul>
<p><strong>有了联合分布，边缘分布和条件分布都能求</strong><br><br></p>
<h1 id="9-随机变量的独立性"><a href="#9-随机变量的独立性" class="headerlink" title="9. 随机变量的独立性"></a>9. 随机变量的独立性</h1><ul>
<li>回忆两个随机事件独立的定义是：$P(AB)= P(A)P(B)$</li>
<li>随机变量之间的独立好比，两个或多个随机变量的任意取值所代表的随机事件都相互独立<ul>
<li>离散型随机变量：$P(X = x_i, Y = Y_j) = P(X = x_i)P(Y = y_j)$ </li>
<li>连续型随机变量独立即联合分布等于边缘分布的乘积，$F(x,y) = F_X(x)F_Y(y)$；或联合密度等于边缘密度的乘积，$f(x,y)=f_X(x)f_Y(y)$ </li>
</ul>
</li>
<li>两个随机变量 $(X,Y)$ 相互独立就是指 $X$ 有关的任何事件发生与否都与 $Y$ 有关的任意时间发生与否无关。</li>
<li>随机变量的独立是随机事件独立的延申，从两个事件的独立，延申至一系列事件的独立。</li>
<li>如果多个随机变量相互独立，且分布一致，称为独立同分布，简称 independent identical distribution(iid).</li>
</ul>
<p><br></p>
<h1 id="10-随机变量的函数的概率分布"><a href="#10-随机变量的函数的概率分布" class="headerlink" title="10. 随机变量的函数的概率分布"></a>10. 随机变量的函数的概率分布</h1><ul>
<li>随机变量函数：已知随机变量 $X$ 的分布，另一个随机变量 $Y$ 满足 $Y = g(X)$，这就是随机变量的函数，我们可以从自变量(随机变量 $X$ )的分布求出随机变量 $Y$ 的分布<ul>
<li>连续型：$F(y) = P(Y \le y) = \int_{g(x)\le y} f(x)dx$ </li>
<li>离散型：$P(Y = y<em>j) = \sum</em>{g(x_i)=y_j}P(X = x_i)$ </li>
</ul>
</li>
<li>推广至随机向量的函数：已知随机向量 $(X,Y)$ 的分布，另一个随机变量 $Z = g(X, Y)$，求 $Z$ 的分布<ul>
<li>连续型： $F(Z) = P(Z \le z)= \int\int_{g(x,y) \le z}f(x,y)dxdy$ </li>
<li>离散型：$P(Z = z<em>q) = \sum</em>{g(x_i,y_j) = z_q}P(X = x_i, Y = y_j)$，若 $X$ 和 $Y$ 独立，那么后面的概率可以拆成两个单独概率想乘。</li>
</ul>
</li>
<li>推广至随机向量的多个函数： 已知随机向量 $(X,Y)$ 的分布，另一个随机变量 $Z_1 = g_1(X, Y)$，另一个随机变量 $Z_2 = g_2(X, Y)$，求 $(Z_1, Z_2)$ 的联合分布<ul>
<li>$F(Z<em>1, Z_2) = P(Z_1 \le z_1, Z_2 \le z_2) = \int\int</em>{Z_1 = g_1(X, Y) \le z_1, Z_2 = g_2(X, Y) \le z_2} f(x,y)dxdy$  </li>
<li>设 $u = g_1(x,y)$，$v = g_2(x,y)$，若反函数存在，即 $x = \phi(u, v)$，$y = \psi(u,v)$， </li>
<li>$\int\int<em>{Z_1 = g_1(X, Y) \le z_1, Z_2 = g_2(X, Y) \le z_2} f(x,y)dxdy = \int\int</em>{u \le z_1, v \le z_2} f(\phi(u,v), \psi(u,v)) \frac{dxdy}{dudv}dudv$ <ul>
<li>$\frac{dxdy}{dudv} = |J|$，为 $(x,y)$ 关于 $(u,v)$ 的 Jacobi行列式</li>
<li>$J = \left|\begin{array}{cccc}  \frac{\partial x}{\partial u} &amp;    \frac{\partial x}{\partial v} \<br>\frac{\partial y}{\partial u} &amp;    \frac{\partial y}{\partial v}\<br>\end{array}\right|  $     </li>
</ul>
</li>
<li>所以 $(Z<em>1,Z_2)$ 的分布函数等于 $F(Z_1, Z_2) = \int</em>{-\infty}^{z<em>1}\int</em>{-\infty}^{v_1} f(\phi(u,v), \psi(u,v)) |J|dudv$，</li>
<li>同理，$(Z_1,Z_2)$ 的密度函数为 $ f(\phi(u,v), \psi(u,v)) |J|$ </li>
</ul>
</li>
<li><p>例题：在直角坐标平面随机选取一点，分别以随机变量 $X$ 和 $Y$ 表示其横纵坐标，可以认为 $X$ 和 $Y$ 相互独立，且都服从正态分布 $N(0,1)$，求极坐标 $(R, t)$ 的分布 </p>
<ul>
<li>题意是已知 $X$ 和 $Y$ 的分布，求 $(R,t)$ 的联合分布</li>
<li>反函数很容易得：$x = Rcost$，$y = Rsint$，$R \in [0, +\infty]$，$t \in [0, 2 \pi)$ </li>
<li><p>$J = \left|\begin{array}{cccc} \frac{\partial x}{\partial R} &amp;    \frac{\partial x}{\partial t} \<br>\frac{\partial y}{\partial R} &amp;    \frac{\partial y}{\partial t}\<br>\end{array}\right| = \left|\begin{array}{cccc} cost &amp;    -Rsint \<br>sint &amp;   Rcost\<br>\end{array}\right| = R$</p>
</li>
<li><p>$(X,Y)$ 的联合密度为其边缘密度的乘积($X$ 和 $Y$ 相互独立)，$f(x,y) = \frac{1}{2\pi}exp(-\frac{x^2+y^2}{2})$ </p>
</li>
<li>$f(R,t) =  f(Rcost, Rsint) |J| = \frac{1}{2\pi}Rexp(-\frac{R^2}{2})$，$R &gt; 0$，$t \in [0, 2 \pi)$ </li>
<li>$f(R,t) = f<em>1(R)f_2(t)$，其中 $f_1(R) = Rexp(-\frac{R^2}{2})I</em>{R&gt;0}(R)$，$f<em>2(t) = \frac{1}{2\pi}I</em>{[0,2\pi)}(t)$，由于f(R,t) 可以写成两个密度函数的乘积，所以 $R$ 和 $t$ 相互独立，且 $t$ 服从 $[0,2\pi)$ 的均匀分布 </li>
</ul>
</li>
<li><p>注意：两个服从正态分布的随机变量的和也服从正态分布，$X \sim N(\mu_1, \sigma_1^2)$，$Y \sim N(\mu_2, \sigma_2^2)$，且 $X$ 和 $Y$ 相互独立，则 $X + Y  \sim N(\mu_1 +\mu_2, \sigma_1^2 + \sigma_2^2)$ </p>
</li>
<li><p>设 $X$ 和 $Y$ 是两个相互独立的随机变量，求 $M = max(X,Y)$ 的分布</p>
<ul>
<li>$F_M(z) = P(max(X,Y) \le z) = P(X \le z, Y \le z) = P(X \le z)P(Y \le z) = F_X(z)F_Y(z)$  </li>
<li>密度函数：$f_M(z) = \frac{\partial F_M(z)}{\partial z} = f_X(z)F_Y(z) + F_X(z)f_Y(z)$ </li>
<li>推广至n个随机变量的最大值也是同样的道理，最大值小于 $z$，即每个随机变量都小于 $z$</li>
</ul>
</li>
<li><p>设 $X$ 和 $Y$ 是两个相互独立的随机变量，求 $N = min(X,Y)$ 的分布</p>
<ul>
<li><p>$N$ 等于 $X$ 和 $Y$ 的最小值，$N \le z$，即 $X$ 和 $Y$ 的任意一个小于 $z$，求它的对立事件，即 $X$ 和 $Y$ 都大于 $z$</p>
</li>
<li><p>$F_N(z) = P(N \le z) = 1-P(N &gt; z) = 1- P(X &gt; z, Y &gt; z) = 1 - P(X &gt; z)P(Y &gt; z) $ </p>
<p>$= 1 - (1-F_X(z))(1-F_Y(z))$  </p>
</li>
<li><p>密度函数：$f_N(z) = \frac{\partial F_N(z)}{\partial z} = -f_X(z)(1-F_Y(z)) + (1-F_X(z))f_Y(z)$  </p>
</li>
</ul>
<p>关键就是改变积分的区域，通过随机变量函数，约定积分区域</p>
</li>
</ul>
<p><br></p>
<h1 id="11-统计三大分布"><a href="#11-统计三大分布" class="headerlink" title="11. 统计三大分布"></a>11. 统计三大分布</h1><p>$\Gamma(x)=\int_0^\infty e^{-t}t^{x-1}dt$，$x &gt; 0$</p>
<p>性质：</p>
<ol>
<li>$\Gamma(x+1) = x\Gamma(x)$，$\Gamma(1)=1$，$\Gamma(n+1) = n!$ </li>
<li>$\Gamma(\frac{1}{2}) = \sqrt{\pi}$ </li>
</ol>
<h2 id="11-1-卡方分布-chi-squared-distribution"><a href="#11-1-卡方分布-chi-squared-distribution" class="headerlink" title="11.1 卡方分布(chi-squared distribution)"></a>11.1 卡方分布(chi-squared distribution)</h2><ul>
<li><p>定义：设 $X_1, X_2, …, X_n$ 独立同分布，均服从标准正态分布，$\chi^2 = \sum_1^nX_i^2$，则称 $\chi^2$ 是自由度为 $n$ 的卡方分布，记为 $\chi_n^2$，自由度是指能够自由变换的自变量的个数，如果受到一个约束，自由度就要减一，依次类推。</p>
</li>
<li><p>卡方分布的密度函数为</p>
<ul>
<li><p>$f_n(x) = \frac{x^{\frac{x-2}{2}}e^{-\frac{x}{2}}}{2^{\frac{n}{2}}\Gamma(\frac{n}{2})}$ </p>
</li>
<li><p>当 $n=1$，$f_1(x) = \frac{x^{-\frac{1}{2}}e^{-\frac{1}{2}}}{2^{\frac{1}{2}} \sqrt{\pi}}$，为单调递减函数；当 $n=2$，$f_2(x) = e^{-x}$，同为单调递减，当 $n \ge 3$，曲线有单峰，从 $0$ 开始单调上升，再单调下降趋向于0；由于是密度函数，其积分值为1。</p>
<img src="/2018/09/04/%E3%80%90%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E3%80%91%E2%80%94%E2%80%94%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E5%8F%8A%E5%85%B6%E5%88%86%E5%B8%83/%E5%8D%A1%E6%96%B9%E5%88%86%E5%B8%83.png" class="">
</li>
<li><p>若 $X \sim \chi_n^2$，记 $p(X &gt; c) = \alpha$，则 $c = \chi_n^2(\alpha)$ 称为卡方分布的上侧分位数，给定 $\alpha$，可以通过查卡方分布表格，求出 $c$。</p>
<img src="/2018/09/04/%E3%80%90%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E3%80%91%E2%80%94%E2%80%94%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E5%8F%8A%E5%85%B6%E5%88%86%E5%B8%83/%E5%8D%A1%E6%96%B9%E5%88%86%E5%B8%83alpha.png" class="">
</li>
</ul>
</li>
</ul>
<h2 id="11-2-t-分布"><a href="#11-2-t-分布" class="headerlink" title="11.2 t 分布"></a>11.2 t 分布</h2><ul>
<li><p>又称 student‘s distribution</p>
</li>
<li><p>定义：设 $X \sim N(0,1)$，$Y \sim \chi_n^2$，且 $X$ 和 $Y$ 相互独立，令 $T = \frac{X}{\sqrt{\frac{Y}{n}}}$，则称 $T$ 的分布为自由度为 $n$ 的t分布，记为 $T \sim t_n$。</p>
</li>
<li><p>概率密度函数为</p>
<ul>
<li><p>$t_n(x) = \frac{\Gamma(\frac{n+1}{x})}{\sqrt{n\pi}\Gamma(\frac{n}{2})}(1+\frac{x^2}{n})^{-\frac{n+1}{2}}$ </p>
<img src="/2018/09/04/%E3%80%90%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E3%80%91%E2%80%94%E2%80%94%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E5%8F%8A%E5%85%B6%E5%88%86%E5%B8%83/t%E5%88%86%E5%B8%83.png" class="" width="500" height="250">
</li>
<li><p>与标准正态分布的关系</p>
<ul>
<li>t分布的密度函数与标准正态分布的密度很相似，都是关于y轴对称，偶函数</li>
<li>t分布密度函数的峰值要低于标准正态的峰值，但尾部要比正态要高，这很正常，因为密度函数积分都要等于1，中间高度低了，两边的高度就要上升</li>
<li>当 $n$ 趋于无穷的时候，$t_n$ 会趋于标准正态。</li>
</ul>
</li>
<li><p>若 $T \sim t_n$，$P(|T| &gt; c) = \alpha$，则 $c = t_n(\alpha / 2)$ 称为t分布的双侧 $\alpha$ 分位数</p>
<img src="/2018/09/04/%E3%80%90%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E3%80%91%E2%80%94%E2%80%94%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E5%8F%8A%E5%85%B6%E5%88%86%E5%B8%83/t%E5%88%86%E5%B8%83alpha.png" class="">
</li>
</ul>
</li>
</ul>
<h2 id="11-3-F-分布"><a href="#11-3-F-分布" class="headerlink" title="11.3 F 分布"></a>11.3 F 分布</h2><ul>
<li><p>定义：$X \sim \chi<em>n^2$， $Y \sim \chi_m^2$，$X$ 和 $Y$ 相互独立，令 $Z = \frac{X/n}{Y/m}$，则称 $Z$ 为自由度是 $n$ 和 $m$ 的 $F$ 分布，记为 $F \sim F</em>{n,m}$ </p>
</li>
<li><p>$t$ 分布的平方为一个自由度为1和n的F分布，$t_n^2 = \frac{X^2}{Y/n}$，$X$ 为标准正态，$X^2$ 为自由度为1的卡方分布</p>
</li>
<li><p>同样有上测 $\alpha$ 分位数的概念</p>
<img src="/2018/09/04/%E3%80%90%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E3%80%91%E2%80%94%E2%80%94%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E5%8F%8A%E5%85%B6%E5%88%86%E5%B8%83/F%E5%88%86%E5%B8%83.png" class="">
</li>
<li><p>性质：$F<em>{m,n}(1-\alpha) = \frac{1}{F</em>{n,m}(\alpha)}$<br><br></p>
<h1 id="12-总结"><a href="#12-总结" class="headerlink" title="12. 总结"></a>12. 总结</h1><p>这一章将随机事件过渡为随机变量，进而描述一系列随机事件发生的概率，也就是所谓的随机变量的分布，很多第一章提到的概念，如条件概率，在这一章都会以分布的形式出现，都是描述一系列的概率值，牢记，无论是分布函数还是密度函数，还是积分，它们都是为算概率服务的，最终目的都是算概率。</p>
</li>
</ul>
<p><br></p>
]]></content>
      <tags>
        <tag>概率论</tag>
      </tags>
  </entry>
  <entry>
    <title>【概率论和数理统计】——参数估计</title>
    <url>/2018/11/09/%E3%80%90%E6%A6%82%E7%8E%87%E8%AE%BA%E5%92%8C%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E3%80%91%E2%80%94%E2%80%94%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1/</url>
    <content><![CDATA[<blockquote>
<p>课程名称：概率论与数理统计<br>开设学校：中科大<br>课程平台：icourse<br>第四章：数理统计入门，从概率论过渡到数理统计，讲述数理统计的几个基本概念，以及参数估计，包括点估计和区间估计。</p>
</blockquote>
<span id="more"></span>
<p><em>preface</em></p>
<p>统计分为描述性统计和数理统计，描述性统计主要对数据进行简单化的分析，如加减乘除，算均值中位数等；而数理统计是以数学和概率论为工具，研究如何有效收集随机性数据、如何分析数据、以及在给定的统计模型下进行统计推断。下面具体讲解数理统计的这三个研究方向。</p>
<ul>
<li>数据收集<ul>
<li>主要通过抽样和设计试验，抽样一般针对社会科学，进行抽样调查</li>
<li>抽样得到的随机数据要具有代表性，要反映整个样本空间</li>
<li>试验要有对照</li>
</ul>
</li>
<li>数据分析<ul>
<li>涉及统计分析的方法</li>
</ul>
</li>
<li>统计推断<ul>
<li>估计（estimate）：点估计，区间估计</li>
<li>检验（test）：参数检验，非参数检验</li>
</ul>
</li>
</ul>
<p>数理统计不同于一般的资料统计，它更侧重于应用随机现象本身的规律性进行资料的收集、整理和分析。由于大量的随即现象必然呈现出它的规律性，因而从理论上讲，只要对随机现象进行足够多次观察，被研究的随机现象的规律性一定能够清楚地呈现出来。但在客观上只允许我们对随机现象进行次数不多的观察实验，也就是说，我们获得的只是局部观察资料。</p>
<p>数理统计的基本思想是：从所要研究对象的全体中，抽取一小部分进行观测和试验，以取得信息，从而对整体做出推断。每个推断必须伴随一定的概率，以表明推断的可靠性（数理推断）。</p>
<p>数理统计的基本任务是：以大数定律、中心极限定理为理论基础，根据实际掌握的部分信息对有关主体试验的分布、数字特征做出估计并加以检验的数理推断。</p>
<p><br></p>
<h1 id="数理统计的几个基本概念"><a href="#数理统计的几个基本概念" class="headerlink" title="数理统计的几个基本概念"></a>数理统计的几个基本概念</h1><h2 id="总体"><a href="#总体" class="headerlink" title="总体"></a>总体</h2><ul>
<li>总体（population）：研究对象的全体，总体中的每个成员称为个体，总体中所包含的个体的个数称为总体的容量。根据总体容量，可以将总体分为有限总体和无限总体。<ul>
<li>在数理统计研究中，人们往往研究有关对象的某一项数量指标，为此，对这一指标进行随机试验，观察试验结果的全部观察值，从而考察该数量指标的分布情况，这时我们研究的对象就是总体的某个数量指标。</li>
<li>由于每个个体的出现是随机的，所以相应的数量指标的出现也是随机的，因此可以把这种数量指标看作一个随机变量，随机变量的分布就是该数量指标在总体中的分布</li>
<li>总体可以用一个随机变量及其分布来描述，因此在理论上可以把总体和概率分布等同起来。</li>
</ul>
</li>
</ul>
<h2 id="样本"><a href="#样本" class="headerlink" title="样本"></a>样本</h2><ul>
<li>样本（sample）：总体的分布一般是未知的，或只知道包含未知参数的分布。因此为了推断总体的分布及其各种特征，按照一定规则从总体中抽取若干个体进行观察试验，以获得有关总体的信息，这一抽取的过程称为<strong>抽样</strong>，所抽取的部分个体称为<strong>样本</strong>，样本所包含的个体数量称为<strong>样本容量</strong>。<ul>
<li>总体可以看成是一个随机变量 $X$，对总体 $X$ 在相同的条件下，进行  $n$ 次重复、独立的观察，其结果记为 $X_1, X_2, …, X_n$。这样得到的随机变量 $X_1, X_2, …, X_n$ 是来自总体 $X$ 的一个<strong>简单随机样本</strong>，与总体随机变量具有相同的分布。可以看出，样本也可以看成是一个随机变量。</li>
<li>抽样的样本应具有：<ul>
<li>代表性：$X_1, X_2, …, X_n$ 中每一个与所观察的总体有相同的分布。</li>
<li>独立性：$X_1, X_2, …, X_n$ 是相互独立的随机变量。</li>
</ul>
</li>
<li>简单随机样本：是一种理想化的样本。<ul>
<li>对于有限总体，若采用有放回抽取，则可得到简单样本；若采用无放回抽取，则无法保证每次抽取的独立性。但若有限总体的容量远大于样本容量，则无放回抽取近似有放回抽取。</li>
<li>对于无限总体，抽取部分个体后放回与否对总体成分影响不大，因此可采用无放回抽取获得简单样本。</li>
</ul>
</li>
<li>若总体的分布函数为 $F(x)$，概率密度函数为 $f(x)$<ul>
<li>则简单随机样本的联合分布函数为：$F(x_1, x_2, …,x_n) = F(x_1)F(x_2)…F(x_n)$ </li>
<li>则简单随机样本的联合概率密度函数为：$f(x_1, x_2, …,x_n) = f(x_1)f(x_2)…f(x_n)$  </li>
</ul>
</li>
</ul>
</li>
<li>样本值：一旦给定一组样本 $X_1, X_2, …, X_n$，得到 $n$ 个具体的数值 $(x_1, x_2, …, x_n)$，则称为样本的一次观察值，简称样本值。 </li>
<li>三者的关系：事实上，我们抽样后得到的资料都是具体的、确定的值，我们只能观察到随机变量取的值，而见不到随机变量。<ul>
<li>统计就是从手中已有的资料（样本值），去推断总体的分布情况，而样本是联系总体和样本值的桥梁。总体分布决定了样本取值的概率规律，也就是样本取到样本值的规律，因而可以由样本值去推断总体。</li>
</ul>
</li>
<li>样本二重性：另外一套理论是，不区分样本和样本值，而是认为样本具有二重性。<ul>
<li>当决定了抽样方案，但是没有实施时，样本是一组随机变量</li>
<li>当抽样方案实施后，样本是一组数</li>
</ul>
</li>
</ul>
<h2 id="统计量"><a href="#统计量" class="headerlink" title="统计量"></a>统计量</h2><ul>
<li>统计量是为了刻画总体某个特征，而对样本进行某种加工。换句话说，统计量是样本的函数，且不带任何参数。</li>
<li>由于样本具有二重性，统计量也具有二重性，它可以是一个随机变量，也可以是一个数。所以在研究统计量的时候，需要研究它的两种形态，一般理论上我们研究的统计量是把它看成随机变量。</li>
<li>常见的统计量<ul>
<li>样本均值：$\bar{X} = \frac{1}{n}\sum X_i$，由大数定律得出，$\bar{X} \rightarrow E(X)$ </li>
<li>样本方差：$s^2 = \frac{1}{n-1}\sum(X_i - \bar{X})^2$，同样由大数定律得出，$s^2 \rightarrow Var(X)$ </li>
<li>k阶样本中心矩：$v_k = \frac{1}{n}\sum(X_i - \bar{X})^k$</li>
<li>k阶样本原点矩：$\mu_k = \frac{1}{n}\sum X_i^k$</li>
</ul>
</li>
<li>顺序统计量：设样本为 $X<em>1, X_2, …, X_n$，构造统计量 $X</em>{(i)}$ 等于样本 $X<em>1, X_2, …, X_n$ 中第 $i$ 小的数，显然 $X</em>{(i)}$ 是一个统计量，因为它是样本的函数。因此可以得到 $X<em>{(1)} \le X</em>{(2)} \le …\le X_{(n)}$ </li>
<li>构造了顺序统计量之后，就可以得到其它的统计量，如极差 $R = X<em>{(n)} - X</em>{(1)}$，中位数</li>
</ul>
<p><strong>Note:  </strong> $\mu$ 和 $\sigma^2$ 是未知常数，不会变化；$\bar{X}$ 和 $S^2$ 或是随机变量或是一个数，但他们都是一个统计量。</p>
<p><br></p>
<h1 id="参数估计-parametric-estimation"><a href="#参数估计-parametric-estimation" class="headerlink" title="参数估计(parametric estimation)"></a>参数估计(parametric estimation)</h1><ul>
<li>定义：参数估计问题是利用从总体抽样得到的部分信息来估计总体的某些参数或者总体的某些数字特征。</li>
<li>参数估计的两个研究方向：<ul>
<li>在已知总体分布类型的前提下，由样本信息估计出总体未知参数的近似值，从而近似估计总体分布。</li>
<li>有时关心的不是总体服从具体什么分布，而是关注总体的某些数字特征，如均值、方差等。</li>
</ul>
</li>
<li>估计量：设总体 $X$ 的分布函数为 $F(x,\theta)$，其中 $\theta$ 为未知参数，$X_1, X_2, …, X_n$ 为来自总体的样本，由样本构造一个统计量 $\hat{\theta} = g(X_1, X_2, …, X_n)$，若以这个样本函数去估计参数 $\theta$，则称 $\hat{\theta}$ 为参数 $\theta$ 的估计量；若样本试验已经实施，那么 $\hat{\theta}$ 为参数 $\theta$ 的估计值。</li>
<li>参数估计主要分为：点估计和区间估计</li>
</ul>
<h2 id="点估计"><a href="#点估计" class="headerlink" title="点估计"></a>点估计</h2><p>点估计分为两种：矩估计和极大似然估计</p>
<h3 id="矩估计"><a href="#矩估计" class="headerlink" title="矩估计"></a>矩估计</h3><ul>
<li>原理：依据大数定理，用样本矩代替总体矩</li>
<li>假设总体分布有 $p$ 个参数，计算 $p$ 个样本矩 $J_1$ 到 $J_p$，然后根据矩估计原理，总体矩等于样本矩，总体矩是关于参数的函数，所以反解方程算出参数</li>
<li>总体均值等于样本均值，因为总体一阶原点矩可以用样本一阶原点矩代替；同理，总体方差等于样本方差</li>
<li>矩估计不唯一，由于估计量也是一个统计量，因此也是随机变量；而总体的真实参数是一个未知常数，非随机变量。</li>
</ul>
<h3 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h3><ul>
<li><p>思想：极大似然估计法，是建立在最大似然原理的基础上的求点估计量的方法，最大似然原理的直观想法是：在试验中概率最大的事件最有可能发生。在已得试验结果的条件下，应该寻找使得该结果出现可能性最大的那个参数值作为真正参数的估计。</p>
</li>
<li><p>似然函数：设 $X_1, X_2, …, X_n$ 是取自总体 $X$ 的一个样本，总体的密度函数为 $f(x, \theta)$，样本的联合密度为 $f(x_1, x_2, …,x_n, \theta) = \prod f(x_i,\theta)$，记 $L(x, \theta) = \prod f(x_i, \theta)$</p>
<ul>
<li>若给定 $\theta$，则 $L(x,\theta)$ 是密度函数</li>
<li>若给定 $x$，则 $L(x,\theta)$ 是似然函数 </li>
</ul>
</li>
<li><p>极大似然估计指：给定样本值，估计参数 $\theta$，使得似然函数取到最大值，似然函数实际上可以看成是一个概率值，因为 $f(x)dx$ 就可以看成是概率，$L(x,\theta)dx$ 相当于抽样得到这一组样本值的概率，现在要求一个参数 $\theta$ 使得这个概率最大；把 $dx$ 去掉，转成求似然函数 $L(x,\theta)$ 最大值。</p>
</li>
<li><p>对于离散型随机变量，似然函数为 </p>
<script type="math/tex; mode=display">L(\theta) = P(X_1 = x_1, X_2 = x_2, ..., X_n = x_n; \theta) = \prod P(X = x_i; \theta)</script><p>对于连续型随机变量，似然函数为 </p>
<script type="math/tex; mode=display">L(\theta) = \prod f(x_i; \theta)</script></li>
<li><p>极大似然估计的步骤</p>
<ul>
<li>求出似然函数，$L(x, \theta) = \prod f(x_i, \theta)$ </li>
<li>若似然函数可导，求出 $ln L(x,\theta)$ 及似然方程 $\frac{dlnL(x,\theta)}{d\theta_i} = 0, (i = 1,2,…,m)$ </li>
<li>解似然方程得到极大似然估计值</li>
<li>最后得到极大似然估计量</li>
</ul>
</li>
<li><p>例题</p>
<ol>
<li>设 $X$ 服从参数为 $\lambda$ 的泊松分布，$\lambda$ 是未知常数，$X_1, X_2, …., X_n$ 是来自 $X$ 的一个样本，求 $\lambda$ 的极大似然估计量<ul>
<li>$X$ 的分布律为：<script type="math/tex; mode=display">P(X = x) = \frac{\lambda^x}{x!}e^{-\lambda}</script></li>
<li>似然函数：<script type="math/tex; mode=display">L(\lambda) = \prod(\frac{\lambda^{x_i}}{x_i!}e^{-\lambda}) = e^{-n\lambda}\frac{\lambda^{\sum x_i}}{\prod(x_i!)}</script></li>
<li>对数似然函数： <script type="math/tex; mode=display">lnL(\lambda) = -n\lambda + (\sum x_i)ln\lambda - \sum ln(x_i!)</script></li>
<li>解似然方程：<script type="math/tex; mode=display">\frac{dlnL(\lambda)}{d\lambda} = -n + \frac{\sum x_i}{\lambda} = 0</script></li>
<li>似然估计值：<script type="math/tex; mode=display">\hat{\lambda} = \frac{1}{n}\sum x_i = \bar{x}</script></li>
<li>似然估计量：<script type="math/tex; mode=display">\hat{\lambda} = \frac{1}{n}\sum X_i = \bar{X}</script><br></li>
</ul>
</li>
<li>设总体 $X$ 具有分布列如下，已知取得样本值1、2、1、2、2、3、3，求 $\theta$ 的极大似然估计值<ul>
<li>$P(X=1) = \theta^2, P(X=2)=2\theta(1-\theta), P(X=3)=(1-\theta)^2$</li>
<li>$L(\theta) = P(X_1 = 1, X_2 = 2, …, X_7 = 3) = 8\theta^7(1-\theta)^7$ </li>
<li>$\frac{dlnL(\theta)}{d\theta} = \frac{7}{\theta} - \frac{7}{1-\theta} = 0$</li>
<li>$\hat{\theta} = 0.5$<br><br></li>
</ul>
</li>
<li><p>设总体 $X \sim N(\mu, \sigma^2)$，$X_1, …, X_n$ 为来自总体的样本，求 $\mu$ 和 $\sigma^2$ 的极大似然估计量</p>
<ul>
<li>$X$ 的分布函数为：<script type="math/tex; mode=display">f(x;\mu, \sigma^2) = \frac{1}{\sqrt{2\pi}\sigma} exp(-\frac{(x-\mu)^2}{2\sigma^2})</script></li>
<li>似然函数：<script type="math/tex; mode=display">L(\mu, \sigma^2) = \prod \frac{1}{\sqrt{2\pi}\sigma} exp(-\frac{(x_i-\mu)^2}{2\sigma^2}) = (\frac{1}{2\pi\sigma^2})^{\frac{n}{2}}exp(-\frac{\sum (x_i - \mu)^2}{2\sigma^2})</script></li>
<li>对数似然函数：<script type="math/tex; mode=display">lnL(\mu, \sigma^2) = -\frac{n}{2}ln(2\pi) - \frac{n}{2}ln\sigma^2 -\frac{\sum (x_i - \mu)^2}{2\sigma^2}</script></li>
<li>令偏导等于0<script type="math/tex; mode=display">\frac{\partial lnL(\mu, \sigma^2)}{\partial \mu} = \frac{1}{\sigma^2}(\sum x_i - n\mu) = 0</script><script type="math/tex; mode=display">\frac{\partial lnL(\mu, \sigma^2)}{\partial \sigma^2} = -\frac{n}{2\sigma^2} + \frac{1}{2(\sigma^2)^2} \sum (x_i - \mu)^2 = 0</script></li>
<li>解得：<script type="math/tex; mode=display">\hat{\mu} = \frac{1}{n} \sum x_i = \bar{x}$，$\hat{\sigma}^2 = \frac{1}{n}\sum (x_i - \bar{x})^2</script><br></li>
</ul>
</li>
<li><p>从鱼塘里随机捕获500条鱼，做好标记后重新放入鱼池里，充分混合后再捕获1000条，发现其中有72条带标记，问鱼塘里可能有多少条鱼。</p>
<ul>
<li>假设鱼塘有 $N$ 条鱼，$P(X = 72) = \frac{C<em>{500}^{72} C</em>{N-500}^{1000-72}}{C_N^{1000}}$ </li>
<li>可以将这个问题看成极大似然估计的问题，估计参数 $N$，使得上面的概率最大。</li>
</ul>
</li>
</ol>
</li>
</ul>
<h3 id="点估计优良性准则"><a href="#点估计优良性准则" class="headerlink" title="点估计优良性准则"></a>点估计优良性准则</h3><p>使用矩估计方法和极大似然估计得到的参数估计量可能相同，也可能不同，那么如何评判估计量的好坏呢？主要通过无偏性、有效性和一致性判断。</p>
<h4 id="无偏性"><a href="#无偏性" class="headerlink" title="无偏性"></a>无偏性</h4><ul>
<li>定义：设 $\hat{\theta}$ 是未知参数 $\theta$ 的估计量，若 $E\hat{\theta} = \theta$，则称 $\hat{\theta}$ 为 $\theta$ 的无偏估计量，或者称估计量 $\hat{\theta}$ 具有无偏性，$\theta - E\hat{\theta}$ 称为系统误差。</li>
<li>估计量在试验实施前是一个随机变量，无偏性要求多次试验实施的参数平均值就等于真实的参数值。</li>
<li>定理：<ul>
<li>样本均值 $\bar{X}$ 是总体均值 $\mu$ 的无偏估计量，反过来，总体均值 $\mu$ 的无偏估计不唯一，估计量 $\hat{\mu}_1 = \sum w_i X_i, \sum w_i=1$，这也是总体均值的无偏估计</li>
<li>样本方差 $S_n^2$ （二阶中心矩）不是总体方差 $\sigma^2$ 的无偏估计量，证明如下：<ul>
<li><img src="/2018/11/09/%E3%80%90%E6%A6%82%E7%8E%87%E8%AE%BA%E5%92%8C%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E3%80%91%E2%80%94%E2%80%94%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1/esn2.png" class=""></li>
</ul>
</li>
<li>因为二阶中心矩对总体方差的估计偏小，因此修正样本方差为：$S^2 = \frac{1}{n-1}\sum (X_i - \bar{X})^2$  </li>
</ul>
</li>
<li>不是无偏的估计量可以通过修正得到无偏估计量。</li>
</ul>
<h4 id="有效性（最小方差无偏性）"><a href="#有效性（最小方差无偏性）" class="headerlink" title="有效性（最小方差无偏性）"></a>有效性（最小方差无偏性）</h4><ul>
<li>定义：设 $\hat{\theta}_1$ 和 $\hat{\theta}_2$ 都是未知参数 $\theta$ 的无偏估计量，若 $Var(\hat{\theta}_1) \le Var(\hat{\theta}_2)$，则称 $\hat{\theta}_1$ 比 $\hat{\theta}_2$ 更有效</li>
<li>注意：有效性是以无偏性为前提的</li>
</ul>
<h4 id="相合性（一致性）"><a href="#相合性（一致性）" class="headerlink" title="相合性（一致性）"></a>相合性（一致性）</h4><ul>
<li>定义：设 $\hat{\theta}$ 都是未知参数 $\theta$ 的无偏估计量，若 $\hat{\theta}$ 依概率收敛于 $\theta$，即对于任意 $\epsilon &gt; 0$，有 $lim_{n \rightarrow \infty}P(|\hat{\theta} - \theta| \ge \epsilon) = 0$，则称 $\hat{\theta}$ 为 $\theta$ 的相合估计量</li>
<li>样本方差虽然不是无偏估计量，但是它依旧相合 </li>
</ul>
<h4 id="渐近正态性"><a href="#渐近正态性" class="headerlink" title="渐近正态性"></a>渐近正态性</h4><ul>
<li>如果估计量经过标准化的极限分布为标准正态分布，则称该统计量具有渐近正态性</li>
<li>渐近正态性能保证样本量较大时，估计量的渐近分布已知， 这样当估计量的分布难以得出或者过于复杂时候，可以使用其近似分布来进行计算。</li>
</ul>
<p><br></p>
<h2 id="区间估计"><a href="#区间估计" class="headerlink" title="区间估计"></a>区间估计</h2><p>点估计是用样本算得的一个值去估计未知参数，但是点估计仅仅是未知参数的一个近似值，它没有反映出这个近似值的精确度以及误差范围。而区间估计正好弥补了点估计的这个缺陷，我们希望确定一个区间，使我们能以比较高的可靠程度相信它包含真参数值，这里所说的可靠程度是用概率来度量的，称为置信水平或置信度。</p>
<h4 id="几个基本概念"><a href="#几个基本概念" class="headerlink" title="几个基本概念"></a>几个基本概念</h4><ul>
<li>置信区间：设 $\theta$ 是一个待估参数，给定 $\alpha &gt; 0$，若由样本 $X_1, X_2, … , X_n$ 确定的两个统计量 $\underline{\theta}$ 和 $\overline{\theta}$ ($\underline{\theta} &lt;\overline{\theta}$) 满足 $P(\underline{\theta} &lt; \theta &lt; \overline{\theta}) \ge 1- \alpha$，则称区间 $(\underline{\theta} ,\overline{\theta})$ 是 $\theta$ 的置信度为 $1-\alpha$ 的置信区间，$\underline{\theta}$ 和 $\overline{\theta}$ 分别称为置信下限和置信上限。</li>
<li>可信度：$1-\alpha$ </li>
<li>精度：区间的长度，$\overline{\theta} - \underline{\theta}$ </li>
</ul>
<h4 id="区间估计的基本要求"><a href="#区间估计的基本要求" class="headerlink" title="区间估计的基本要求"></a>区间估计的基本要求</h4><ul>
<li>寻找两个统计量，$\underline{\theta}$ 和 $\overline{\theta}$ 使得区间 $(\underline{\theta} ,\overline{\theta})$ 以指定概率包含真值 $\theta$。</li>
<li>要求 $\theta$ 以很大的可能被包含在区间 $(\underline{\theta} ,\overline{\theta})$ 内，就是说，概率 $P(\underline{\theta} &lt; \theta &lt; \overline{\theta})$ 要尽可能的大，即要求估计尽量可靠。</li>
<li>估计的精度要尽可能地高，即要求区间长度尽可能地短。</li>
</ul>
<h4 id="一些注意点"><a href="#一些注意点" class="headerlink" title="一些注意点"></a>一些注意点</h4><ul>
<li>可靠度和精度是一对矛盾，一般是保证可靠度的条件下尽可能地提高精度。</li>
<li>被估计的参数 $\theta$ 虽然未知，但是它是一个常数，没有随机性，而区间 $(\underline{\theta} ,\overline{\theta})$ 是随机的，因此置信区间的本质是，随机区间 $(\underline{\theta} ,\overline{\theta})$ 以 $1-\alpha$ 的概率包含参数 $\theta$ 的真值，而不能说参数 $\theta$ 以 $1-\alpha$ 的概率落入随即区间 $(\underline{\theta} ,\overline{\theta})$。</li>
<li>若反复抽样多次，每个样本值确定一个区间 $(\underline{\theta} ,\overline{\theta})$，每个这样的区间可能包含 $\theta$ 的真值，也可能不包含，按照伯努利大数定理，在这样多的区间中，包含 $\theta$ 真值的约占 $1-\alpha$，不包含的约占 $\alpha$ </li>
</ul>
<h4 id="置信区间求解"><a href="#置信区间求解" class="headerlink" title="置信区间求解"></a>置信区间求解</h4><ol>
<li>寻找待估参数 $\theta$ 的一个良好的点估计 $\hat{\theta}$</li>
<li>寻找一个待估参数 $\theta$ 和点估计量 $\hat{\theta}$ 的函数 $U(\hat{\theta}, \theta)$，且其分布为已知。</li>
<li>对于给定的置信水平 $1-\alpha$，根据 $U(\hat{\theta}, \theta)$ 的分布，确定常数 $a$ 和 $b$，使得 $P(a \le U(\hat{\theta}, \theta) \le b) = 1-\alpha$</li>
<li>对 $a \le U(\hat{\theta}, \theta) \le b$ 作等价变形，得到 $\underline{\theta}(\hat{\theta}) &lt; \theta &lt; \overline{\theta}(\hat{\theta})$，置信区间为 $(\underline{\theta} ,\overline{\theta})$</li>
</ol>
<p><strong>Note</strong></p>
<ul>
<li>区间估计是基于点估计的</li>
<li>确定区间估计很关键的是要寻找一个待估参数 $\theta$ 和点估计量 $\hat{\theta}$ 的函数 $U(\hat{\theta}, \theta)$，且其分布为已知，不依赖任何未知参数。而这与总体分布有关，所以总体分布的形式是否已知，是怎样的类型，至关重要。</li>
<li>上面的第三步求解常数 $a$ 和 $b$，会有很多个解，我们需要从里面挑出精度最高的，即 $b-a$ 最小的。而一般根据待估参数不同，精度最高的常数 $a$ 和 $b$ 满足的条件不同，如果 $U(\hat{\theta}, \theta)$ 为单峰且对称的情况，那么当 $a = -b$ 的时候，置信区间长度最短。</li>
</ul>
<h4 id="单正态总体的区间估计"><a href="#单正态总体的区间估计" class="headerlink" title="单正态总体的区间估计"></a>单正态总体的区间估计</h4><p>设 $X_1, X_2, …, X_n$ 取自总体 $X\sim N(\mu, \sigma^2)$，样本均值 $\bar{X} = \frac{1}{n}\sum X_i $，样本修正方差 $S^2 = \frac{1}{n-1} \sum (X_i - \bar{X})^2$，$\mu$ 的区间估计结构为 $[\bar{X} - d, \bar{X} + d]$，2d为精度；$\sigma^2$ 的区间估计结构为 $[S^2 / c, S^2 / d]$  </p>
<ul>
<li>总体方差 $\sigma^2$ 已知，估计总体均值 $\mu$ 的置信区间<ol>
<li>寻找总体均值较优的点估计量，$\hat{\mu} = \bar{X}$</li>
<li>构造不依赖待估参数且具有已知分布的统计量 $\frac{\bar{X} - \mu}{\sigma / \sqrt{n}} \sim N(0,1)$，标准化的随机变量 $\bar{X}$ </li>
<li>令 $a = -b$，计算<script type="math/tex; mode=display">P(|\frac{\bar{X} - \mu}{\sigma / \sqrt{n}}| < a) = 1-\alpha</script><script type="math/tex; mode=display">P(\frac{\bar{X} - \mu}{\sigma / \sqrt{n}} > a) = \alpha / 2</script>解得： $a = u_{\alpha /2}$ </li>
<li>因此置信区间 <script type="math/tex; mode=display">(\bar{X} - u_{\alpha/2} \frac{\sigma}{\sqrt{n}}, \bar{X} + u_{\alpha/2} \frac{\sigma}{\sqrt{n}})</script><br></li>
</ol>
</li>
<li>总体方差 $\sigma^2$ 未知，用样本方差 $S^2$ 代替，估计总体均值 $\mu$ 的置信区间<ul>
<li>总体方差未知，用样本方差代替，构造统计量 $\frac{\bar{X} - \mu}{S/ \sqrt{n}} \sim t_{n-1}$，t分布</li>
<li>置信区间<script type="math/tex; mode=display">(\bar{X} - (n-1)t_{\alpha/2} \frac{S}{\sqrt{n}}, \bar{X} + (n-1)t_{\alpha/2} \frac{S}{\sqrt{n}})</script><br></li>
</ul>
</li>
<li>估计总体方差 $\sigma^2$<ul>
<li>构造统计量 $\frac{(n-1)S^2}{\sigma^2} \sim \chi^2_{n-1}$  </li>
<li>计算 <script type="math/tex; mode=display">P(\frac{(n-1)S^2}{\sigma^2} \ge K_2) = \alpha / 2</script>以及 <script type="math/tex; mode=display">P(\frac{(n-1)S^2}{\sigma^2} \le K_2) = \alpha / 2</script></li>
<li>解得：<script type="math/tex; mode=display">K_2 = \chi^2_{\alpha/2}, K_1 = \chi^2_{1-\alpha/2}</script></li>
<li>置信区间：<script type="math/tex; mode=display">(\frac{(n-1)S^2}{\chi^2_{\alpha/2}}, \frac{(n-1)S^2}{\chi^2_{1-\alpha/2}})</script></li>
</ul>
</li>
</ul>
<p><strong>Note：</strong></p>
<ul>
<li>上面的例子，总体 $X$ 都是服从正态分布，当给定 $\alpha$ 时，查正态分布表或者t分布表，确定临界值，从而确定置信区间</li>
<li>但是如果总体 $X$ 分布是未知的，样本均值 $\bar{X}$ 就不是正态，单这时只要 $n$ 充分的大，根据中心极限定理，$\frac{\bar{X} - \mu}{\sigma / \sqrt{n} }$ 会近似于标准正态分布，$\frac{\bar{X} - \mu}{S/ \sqrt{n} }$ 是t分布，而t分布也会趋于标准正态分布</li>
</ul>
<p><br></p>
]]></content>
      <tags>
        <tag>概率论</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文复现】——Attention-Fused Deep Matching Network</title>
    <url>/2018/11/05/%E3%80%90%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0%E3%80%91%E2%80%94%E2%80%94Attention-Fused-Deep-Matching-Network/</url>
    <content><![CDATA[<blockquote>
<p>title: Attention-Fused Deep Matching Network for Natural Language Inference<br>conference: 2018, IJCAI<br>authors: Chaoqun Duan, Lei Cui, etc</p>
</blockquote>
<span id="more"></span>
<p><br></p>
<h1 id="Natural-Language-Inference"><a href="#Natural-Language-Inference" class="headerlink" title="Natural Language Inference"></a>Natural Language Inference</h1><p>NLI是NLP中一个比较核心的任务，主要用来判断两个句子是否有一定的关联，两个句子分为称为premise sentence和hopothesis sentence，NLI通常被看为是一个分类任务，经典的数据集包括SNLI、MultiNLI和Quora，这个分类任务的标签为entailment、contradiction和neural，一般基于神经网络的NLI模型包含三个层：encoder层、matching层、prediction层。</p>
<ul>
<li>encoder层：给定两个句子 $p = (p<em>1, …, p_m)$，$q = (q_1, …, q_n)$，经过embedding之后为 $p = (e</em>{p<em>1}, …, e</em>{p<em>m})$，$q = (e</em>{q<em>1}, …, e</em>{q<em>n})$，encoder层就是将两个句子分别通过一个Bi-LSTM进行压缩，得到 $H_p = (h</em>{p<em>1}, …, h</em>{p<em>m})$，$H_q = (h</em>{q<em>1}, …, h</em>{q_n})$，$H_p$ 和 $H_q$ 作为句子 $p$ 和 $q$ 的encoded representation。</li>
<li>matching层：匹配层将两个句子联系起来，$g()$ 是一个匹配函数，$V_p = g(H_p, H_q)$，$V_q = g(H_q, H_p)$，得到新的representation</li>
<li>prediction层：这一层将两个句子 $V_p$ 和 $V_q$ 各自的重要信息提取出来，提取方法采用pooling的方法，分别用GlobalMaxPooling和AveragedPooling进行提取，提取后的信息进行拼接后接入全连接网络进行预测。 </li>
</ul>
<p><br></p>
<h1 id="Attention-Fused-Deep-Matching-Network-AFDMN"><a href="#Attention-Fused-Deep-Matching-Network-AFDMN" class="headerlink" title="Attention-Fused Deep Matching Network(AFDMN)"></a>Attention-Fused Deep Matching Network(AFDMN)</h1><p>作者提出了一个基于注意力机制的匹配网络，来得到更好的interaction效果，注意力机制可以计算一个词与另一个句子的相关性，即attention weight。作者提出的网络结构大致如下，作者提出的matching层分为四个部分：cross attention层，fusion层，self attention层，fusion层，一个matching layer成为一个computational block，整个matching layer可以有多个blocks。</p>
<img src="/2018/11/05/%E3%80%90%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0%E3%80%91%E2%80%94%E2%80%94Attention-Fused-Deep-Matching-Network/afdmn.JPG" class="" width="600" height="300">
<ol>
<li><p>cross attention<br>给定两个句子来自上一个block的representation，$H<em>p^{t-1} = (h</em>{p<em>1}^{t-1}, …, h</em>{p<em>m}^{t-1})$ 和 $H_q^{t-1} = (h</em>{q<em>1}^{t-1}, …, h</em>{q<em>n}^{t-1})$，首先计算一个相关性矩阵 $A$，$A</em>{i,j}$ 表示 句子 $p$ 的第 $i$ 个词和句子 $q$ 的第 $j$ 个词的相关性，$<a, b>$ 表示内积。</p>
<script type="math/tex; mode=display">A_{i,j}^t = h_{p_i}^{t-1} W h_{q_j}^{t-1} + <U_l^t, h_{p_i}^{t-1}> + <U_r^t, h_{q_j}^{t-1}></script><p>$A$ 矩阵的第 $i$ 行做softmax就可以得到句子 $p$ 的第 $i$ 个词对于句子 $q$ 的attention weight，<br>$A$ 矩阵的第 $j$ 列做softmax就可以得到句子 $q$ 的第 $j$ 个词对于句子 $p$ 的attention weight。</p>
<script type="math/tex; mode=display">a_{p_i}^t = softmax(A_{i:}^t)，a_{q_j}^t = softmax(A_{:j}^t)</script><p>然后句子 $p$ 可以用句子 $q$ 和attention weight来表示，同理句子 $q$。</p>
<script type="math/tex; mode=display">\tilde{h}_{p_i}^t = H_q^{t-1} \cdot a_{p_i}^t，\tilde{h}_{q_j}^t = H_p^{t-1} \cdot a_{q_j}^t</script></li>
<li><p>fusion for cross attention<br>对于句子 $p$，我们有两个representation，一个是原始的表示，一个是经过cross attention用句子 $q$ 表示的句子 $p$；同理句子 $q$；fusion层将这两种表示拼接，再经过Bi-LSTM</p>
<script type="math/tex; mode=display">\bar{f}_{p_i}^t = [h_{p_i}^t;\tilde{h}_{p_i}^t;h_{p_i}^t - \tilde{h}_{p_i}^t;h_{p_i}^t \odot \tilde{h}_{p_i}^t]</script><script type="math/tex; mode=display">\tilde{f}_{p_i}^t = Relu(W_f^t \bar{f}_{p_i}^t) + b_f^t</script><script type="math/tex; mode=display">f_{p_i}^t = BiLSTM(\tilde{f}_p^t)</script></li>
<li><p>self attention<br>cross attention是计算两个句子的词的相关性，self attention同理，但是是计算一个句子内部句子之间的相关性，同样计算完attention weight后，可以通过weight算出一个新的representation，跟cross attention唯一不同的是，weight的计算直接采取内积的方式</p>
</li>
<li><p>fusion for self attention<br>跟上面的fusion一样</p>
</li>
</ol>
<p><br></p>
<h1 id="PyTorch实现"><a href="#PyTorch实现" class="headerlink" title="PyTorch实现"></a>PyTorch实现</h1><h2 id="cross-attention"><a href="#cross-attention" class="headerlink" title="cross attention"></a>cross attention</h2><p>实现的时候需要注意的是：</p>
<ol>
<li>weight的前半部分实际上是一个简单的神经网络，后半部分的 $U$ 是一个向量，是一个参数，因此要用 <code>nn.Parameter()</code></li>
<li>能量的后半部分计算比较复杂，下面提供了两个版本，分别是vectorized的和non-vectorized。</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CrossAttn_Layer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, hidden_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(CrossAttn_Layer, self).__init__()</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.W = nn.Linear(<span class="number">2</span>*hidden_size, <span class="number">2</span>*hidden_size)</span><br><span class="line">        self.Ul = nn.Parameter(torch.FloatTensor(<span class="number">0.02</span>*(np.random.rand(<span class="number">2</span>*hidden_size)-<span class="number">0.5</span>)))</span><br><span class="line">        self.Ur = nn.Parameter(torch.FloatTensor(<span class="number">0.02</span>*(np.random.rand(<span class="number">2</span>*hidden_size)-<span class="number">0.5</span>)))</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, hp, hq</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            hp: representation of the 1st sentence from the previous block</span></span><br><span class="line"><span class="string">                dim: [batch_size, max_length_1, 2*hidden_size]</span></span><br><span class="line"><span class="string">            hq: representation of the 2nd sentence from the previous block</span></span><br><span class="line"><span class="string">                dim: dim: [batch_size, max_length_2, 2*hidden_size]</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            output_after_crossAttn_1: [batch_size, max_length_1, 2*hidden_size]</span></span><br><span class="line"><span class="string">            output_after_crossAttn_2: [batch_size, max_length_2, 2*hidden_size]</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        </span><br><span class="line">        batch_size = hp.size()[<span class="number">0</span>]</span><br><span class="line">        sentence_length1 = hp.size()[<span class="number">1</span>]</span><br><span class="line">        sentence_length2 = hq.size()[<span class="number">1</span>]</span><br><span class="line">        </span><br><span class="line">        attn_energy = hp.bmm(self.W(hq).transpose(<span class="number">1</span>,<span class="number">2</span>))    </span><br><span class="line">        energy1 = torch.mm(hp.contiguous().view(-<span class="number">1</span>, <span class="number">2</span>*self.hidden_size), self.Ul.unsqueeze(<span class="number">1</span>)).view(batch_size,sentence_length1, <span class="number">1</span>).expand(batch_size,sentence_length1, sentence_length2)       </span><br><span class="line">        energy2 = torch.mm(hq.contiguous().view(-<span class="number">1</span>, <span class="number">2</span>*self.hidden_size), self.Ur.unsqueeze(<span class="number">1</span>)).view(batch_size, <span class="number">1</span>, sentence_length2).expand(batch_size,sentence_length1, sentence_length2) </span><br><span class="line">        attn_energy = attn_energy + energy1 + energy2</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line"><span class="comment"># =============================================================================</span></span><br><span class="line"><span class="comment">#         for b in range(batch_size):</span></span><br><span class="line"><span class="comment">#             for m in range(sentence_length1):</span></span><br><span class="line"><span class="comment">#                 for n in range(sentence_length2):</span></span><br><span class="line"><span class="comment">#                     attn_energy[b, m, n] += self.Ul.dot(hp[b, m, :]) + self.Ur.dot(hq[b, n, :])</span></span><br><span class="line"><span class="comment">#         </span></span><br><span class="line"><span class="comment"># =============================================================================        </span></span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="keyword">assert</span>(attn_energy.size() == (batch_size, sentence_length1, sentence_length2))</span><br><span class="line">        </span><br><span class="line">        attn_weights_p = F.softmax(attn_energy, dim=<span class="number">2</span>)</span><br><span class="line">        attn_weights_q = F.softmax(attn_energy, dim=<span class="number">1</span>)</span><br><span class="line">        output_after_crossAttn_1 = attn_weights_p.bmm(hq)</span><br><span class="line">        output_after_crossAttn_2 = attn_weights_q.transpose(<span class="number">1</span>,<span class="number">2</span>).bmm(hp)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">assert</span>(hp.size() == output_after_crossAttn_1.size())</span><br><span class="line">        <span class="keyword">assert</span>(hq.size() == output_after_crossAttn_2.size())</span><br><span class="line">         </span><br><span class="line">        <span class="keyword">return</span> output_after_crossAttn_1, output_after_crossAttn_2</span><br></pre></td></tr></table></figure>
<p><br></p>
<h2 id="self-attention"><a href="#self-attention" class="headerlink" title="self attention"></a>self attention</h2><p>实现细节：</p>
<ol>
<li>attention weight matrix不需要用for循环，直接用矩阵乘法实现内积</li>
<li>softmax的维度需要注意</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SelfAttn_Layer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SelfAttn_Layer, self).__init__()</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, output_after_fusion</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            output_after_fusion: [batch_size, max_length, 2*hidden_size]</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            output_after_selfAttn: [batch_size, max_length, 2*hidden_size]</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        </span><br><span class="line">        similarity_matrix = output_after_fusion.bmm(output_after_fusion.transpose(<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line">        attn_weight = F.softmax(similarity_matrix, dim=<span class="number">2</span>)</span><br><span class="line">        output_after_selfAttn = attn_weight.bmm(output_after_fusion)</span><br><span class="line">        <span class="keyword">assert</span>(output_after_fusion.size() == output_after_selfAttn.size())</span><br><span class="line">        <span class="keyword">return</span> output_after_selfAttn</span><br></pre></td></tr></table></figure>
<p><br></p>
]]></content>
  </entry>
  <entry>
    <title>【概率论与数理统计】———— 随机变量的数字特征</title>
    <url>/2018/10/22/%E3%80%90%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E3%80%91%E2%80%94%E2%80%94%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E7%9A%84%E6%95%B0%E5%AD%97%E7%89%B9%E5%BE%81/</url>
    <content><![CDATA[<blockquote>
<p>课程名称：概率论与数理统计<br>开设学校：中科大<br>课程平台：icourse<br>第三章：随机变量的数字特征，研究随机变量的几种重要的数字特征，如期望、方差、协方差，以及讲述连接概率论和数理统计的两大重要定理：大数定理和中心极限定理。</p>
</blockquote>
<span id="more"></span>
<p><em>preface</em></p>
<p>概率论主要研究随机事件发生的概率，最开始我们研究单独的一个随机事件，后来研究一串事件，我们用随机变量来研究它，通过研究随机变量的分布，可以知道随机事件能取哪些值，以及取不同值的概率。知道了随机变量的分布，就能对随机变量的全部概率特征有了全面的理解，它是最高境界，但是随机变量的分布不容易得到，因为分布是最宏观的理解。而有时候我们也不一定需要去了解随机变量的整个分布，我们关心的可能只是一些数字特征，如均值。数字特征分为两类，位置参数和刻度参数，位置参数包括数学期望、中位数、众数，刻度参数包括方差、标准差。另外的包括矩、协方差、相关系数。</p>
<p><br></p>
<h1 id="数学期望"><a href="#数学期望" class="headerlink" title="数学期望"></a>数学期望</h1><h2 id="离散型随机变量"><a href="#离散型随机变量" class="headerlink" title="离散型随机变量"></a>离散型随机变量</h2><ul>
<li>定义：若随机变量 $X$ 为离散型随机变量，其概率分布为 $P(X = x<em>i) = p_i, i = 1,2,3,…$，若 $\sum</em>{i=1}^n p<em>ix_i &lt; \infty$，则称 $\sum</em>{i=1}^n p_ix_i &lt; \infty$ 为随机变量 $X$ 的数学期望，记为 $EX$ 或 $E(X)$ 或 $\mu$ </li>
<li>注意：$EX$ 是一个数，非随机</li>
<li>数学期望又叫均值，是 $X$ 的可能取值与其概率乘积的累加，看成加权平均</li>
<li>几个经典分布的数学期望<ul>
<li>$X \sim B(n, p)$，$E(X) = np$</li>
<li>$X \sim P(\lambda)$，$E(X) = \lambda$ </li>
</ul>
</li>
</ul>
<h2 id="连续型随机变量"><a href="#连续型随机变量" class="headerlink" title="连续型随机变量"></a>连续型随机变量</h2><ul>
<li>定义：若随机变量 $X$ 为连续型随机变量，概率密度函数为 $p(x)$，若积分 $\int<em>{-\infty}^{+\infty} xp(x)dx$ 绝对收敛，则称积分 $\int</em>{-\infty}^{+\infty} xp(x)dx$ 的值为随机变量 $X$ 的数学期望</li>
<li><p>设 $(X,Y)$ 为二维连续型随机变量，则</p>
<ul>
<li>$E(X) = \int<em>{-\infty}^{+\infty} xf_X(x)dx = \int</em>{-\infty}^{+\infty}\int_{-\infty}^{+\infty} xf(x,y)dxdy$ </li>
<li>$E(Y) = \int<em>{-\infty}^{+\infty} yf_Y(y)dx = \int</em>{-\infty}^{+\infty}\int_{-\infty}^{+\infty} yf(x,y)dxdy$ </li>
</ul>
</li>
<li><p>几个经典分布的数学期望</p>
<ul>
<li>$X \sim U(a,b)$，$E(X) = \frac{a+b}{2}$，均分分布的数学期望位于区间的中点。</li>
<li>$X \sim Exp(\lambda)$，$E(X) = \frac{1}{\lambda}$</li>
<li>$X \sim N(\mu, \sigma^2)$，$E(X) = \mu$</li>
</ul>
</li>
</ul>
<h2 id="随机变量函数的数学期望"><a href="#随机变量函数的数学期望" class="headerlink" title="随机变量函数的数学期望"></a>随机变量函数的数学期望</h2><ul>
<li>$X$ 为离散型随机变量，$Y = g(X)$，$E(Y) = \sum_{k=1}^{\infty} g(x_k)p_k$ </li>
<li>$X$ 为连续型随机变量，$Y = g(X)$，$E(Y) = \int_{-\infty}^{+\infty} g(x)f(x)dx$，积分就是求和的极限，$f(x)dx$ 就相当于某一点附近的概率</li>
<li>$(X,Y)​$ 是二维随机变量，$Z = g(X,Y)​$<ul>
<li>若是离散型随机变量，$E(Z) = \sum<em>{i=1}^{\infty}\sum</em>{j=1}^{\infty} g(x<em>i, y_j) p</em>{ij}$ </li>
<li>若是连续型随机变量，$E(Z) = \int\int g(x,y)f(x,y)dxdy$ </li>
</ul>
</li>
</ul>
<h2 id="数学期望的性质"><a href="#数学期望的性质" class="headerlink" title="数学期望的性质"></a>数学期望的性质</h2><ol>
<li>$C$ 是常数，$E(C) = C$</li>
<li>$k$ 是常数，$E(kX) = kE(X)$ </li>
<li>$E(X + Y) = E(X) + E(Y)$，$E(X-Y) = E(X )-E(Y)$</li>
<li>若 $X$ 和 $Y$ 相互独立，则 $E(XY) = E(X)E(Y)​$，反过来不能推独立 </li>
</ol>
<p><br></p>
<h1 id="方差和标准差"><a href="#方差和标准差" class="headerlink" title="方差和标准差"></a>方差和标准差</h1><ul>
<li>定义：设 $X$ 为随机变量，称 $E((X-EX)^2)$ 为随机变量 $X$ 的方差，其算术平方根称为标准差，分别记为 $Var(X)$ 和 $\sigma$ ，$D(X) = Var(X) = \sigma^2$ </li>
<li>方差是刻画随机变量在其中心位置（期望）附近波动程度的一种数字特征<ul>
<li>$EX$ 为随机变量 $X$ 的均值，$X - EX$ 为随机变量距离均值的距离，为了防止距离正负抵消，所以平方处理，但其实所表示的数学意义和 $E(|X- EX|)$ 是一样的</li>
<li>当我们讨论数学性质的时候，用方差比较多；在实际应用时，用标准差比较多，因为标准差与 $X$ 有相同的量纲</li>
</ul>
</li>
<li>计算：$Var(X) = E((X-\mu)^2) = E(X^2 - 2\mu X + \mu^2) = E(X^2) - 2\mu E(X) + \mu^2$，由于 $E(X) = \mu$，所以 $Var(X) = E(X^2)-(E(X))^2$ ；或者用方差的定义算，将 $(X-EX)^2$ 看成一个新的随机变量，再算其数学期望。</li>
<li>常见分布的方差<ul>
<li>$X \sim B(n, p)$，$Var(X) = np(1-p)$</li>
<li>$X \sim P(\lambda)$，$Var(X) = \lambda$</li>
<li>$X \sim Exp(\lambda)$，$Var(X) = \frac{1}{\lambda^2}$</li>
<li>$X \sim N(\mu, \sigma^2)$，$Var(X) = \sigma^2$</li>
</ul>
</li>
<li>方差的性质：<ul>
<li>设 $C$ 是常数，则 $Var(C) = 0$，常数没有波动，所以方差为0</li>
<li>若 $a$，$b$ 是常数，则 $Var(aX+b) = a^2Var(X)$<ul>
<li>$E(aX+b) = aE(X) + b$</li>
<li>$Var(aX+b) = E[aX+b - E(aX+b)]^2 = E[aX+b-aE(X) - b]^2$<br>​                         $=E[a(X-E(X))]^2=a^2E(X-E(X))^2=a^2Var(X)$ </li>
</ul>
</li>
<li>若 $X$，$Y$ 相互独立，$Var(X+Y)=Var(X) + Var(Y)$ <ul>
<li>注意区分期望的这条性质，在期望中，$E(X+Y)=E(X)+E(Y)$，是不需要独立条件</li>
<li>$Var(X+Y) = E[X+Y-(E(X)+E(Y))]^2 = E[(X-E(X)) + (Y-E(Y))]^2$<br>​                        $=E[(X-EX)^2 + (Y-EY)^2-2(X-EX)(Y-EY)]$<br>​                        $=Var(X) + Var(Y)-2E[(X-EX)(Y-EY)]$ </li>
<li>因为随机变量$X$ 和 $Y$ 相互独立，$X-EX$ 和 $Y-EY$ 也相互独立，因此<ul>
<li>$E[(X-EX)(Y-EY)] = E(X-EX)E(Y-EY) = 0*0 = 0$</li>
</ul>
</li>
<li>所以，$Var(X+Y) = Var(X) + Var(Y)$</li>
</ul>
</li>
</ul>
</li>
<li>随机变量的标准化<ul>
<li>假设随机变量 $X$ 的期望是 $\mu$，方差是 $\sigma^2$，则 $X^n = \frac{X-\mu}{\sigma}$ 为 $X$ 的标准化随机变量，易见 $E(X^n) = 0$，$Var(X^n) = 1$</li>
</ul>
</li>
</ul>
<p><br></p>
<h1 id="马尔科夫不等式"><a href="#马尔科夫不等式" class="headerlink" title="马尔科夫不等式"></a>马尔科夫不等式</h1><ul>
<li>假设 $X$ 是一个非负的随机变量，则对任意的 $\epsilon &gt; 0$，$P(X \ge \epsilon) \le \frac{E(X)}{\epsilon}$ ，马尔科夫不等式将概率和期望联系在一起</li>
<li>证明：$E(X) = \int<em>0^{+\infty} xf(x)dx &gt; \int</em>{\epsilon}^{+\infty}xf(x)dx &gt; \epsilon\int_{\epsilon}^{+\infty}f(x)dx = \epsilon P(X \ge \epsilon)$ <ul>
<li>所以，$P(X \ge \epsilon) \le \frac{E(X)}{\epsilon}$ </li>
</ul>
</li>
<li>特例：令随机变量 $Y = (X - EX)^2$，$Y$ 满足非负的条件，令 $\epsilon = \epsilon_1^2$<ul>
<li>则，$P(Y \ge \epsilon) = P(|X-EX| \ge \epsilon_1) \le \frac{E(Y)}{\epsilon} = \frac{Var(X)}{\epsilon_1^2}$  </li>
<li>这是马尔科夫不等式的一个特例，叫做<strong>切比雪夫不等式</strong>：$P(|X-EX| \ge \epsilon) \le  \frac{Var(X)}{\epsilon^2}$</li>
</ul>
</li>
<li>例子：需要调查吸烟率 $p$ ，需要调查多少人才能保证频率和与真正吸烟率的差不超过 $0.005$ 的概率不低于 $0.95$<ul>
<li>假设调查的人数为 $n$，吸烟的人数为 $n_a$，那么频率 $f=\frac{n_a}{n}$，题意要求 $P(|f-p| &lt; 0.005) \ge 0.95$，则 $P(|n_a-np| &lt; 0.005n) \ge 0.95$</li>
<li>$n_a \sim B(n, p)$，二项分布的期望为 $E(n_a) = np$，则上面要求的可以看成$P(|n_a-E(n_a)| &lt; 0.005n) \ge 0.95$，其对立事件为 $P(|n_a-np| \ge 0.005n) \le 0.05$ </li>
<li>利用马尔科夫不等式，$P(|n_a-E(n_a)| \ge 0.005n)  &lt; \frac{np(1-p)}{0.005^2n^2}$ </li>
<li>$p(1-p)$ 的最大值为 0.25，所以 $\frac{np(1-p)}{0.005^2n^2} \le \frac{1}{4*0.005^2n} \le 0.05$，所以 $n = 40000$</li>
</ul>
</li>
</ul>
<p><br></p>
<h1 id="矩"><a href="#矩" class="headerlink" title="矩"></a>矩</h1><ul>
<li>设 $X​$ 为随机变量，若 $E(X^k), k = 1,2,3,…​$ 存在，则称它为 $X​$ 的k阶原点矩</li>
<li>设 $X$ 为随机变量，若 $E((X - EX)^k), k = 1,2,3,…$ 存在，则称它为 $X$ 的k阶中心矩</li>
<li>数学期望是一阶原点矩，方差是二阶中心矩</li>
<li>在实际应用中，高于4阶的矩很少使用，3阶中心距用来衡量随机变量分布是否有偏差，4阶中心矩用来衡量随机变量分布在均值附近的陡峭程度</li>
</ul>
<p><br>  </p>
<h1 id="协方差与相关系数"><a href="#协方差与相关系数" class="headerlink" title="协方差与相关系数"></a>协方差与相关系数</h1><p>之前提到的方差、期望、矩都是描述一个随机变量的数字特征，反应随机变量的某一特性。而协方差和相关系数是描述两个随机变量关系的数字特征</p>
<ul>
<li>协方差定义：$Cov(X,Y) = E[(X-EX)(Y-EY)]$，若 $X=Y$，$Cov(X,Y) = Var(X)$，所以协方差是方差的一个推广</li>
<li>协方差的性质<ul>
<li>$Cov(aX+b, cY+d) = acCov(X,Y)$ <ul>
<li>$Cov(aX+b, cY+d) = E[(aX+b - aEX - b)(cY+d - cEY - d)] $<br>$= E[ac(X-EX)(Y-EY)] = acCov(X,Y)$  </li>
</ul>
</li>
<li>$Cov(aX+bY, cX+dY) = acVar(X)+adCov(X,Y) + bcCov(Y,X) + bdVar(Y)$ </li>
<li>若随机变量 $X$ 和 $Y$ 独立，那么 $Cov(X,Y) = 0$，协方差为0，则两个随机变量不相关<ul>
<li>随机变量 $X$ 和 $Y$ 独立，则随机变量 $X-EX$ 和 $Y-EY$ 独立</li>
<li>$E[(X-EX)(Y-EY)] = E(X-EX)E(Y-EY) = 0 \times 0=0$ </li>
<li>两个随机变量独立，必定不相关；但是不相关，不一定独立<ul>
<li>$Cov(X,Y) = E[(X-EX)(Y-EY)] = E(XY) - EXEY$ </li>
<li>若两个随机变量不相关，则 $E(XY) = EXEY$，但是这推不出随机变量独立</li>
</ul>
</li>
</ul>
</li>
<li>$Cov^2(X,Y) \le Var(X)Var(Y)$，等号成立，当且仅当 $X$ 和 $Y$ 存在严格的线性关系，如 $Y=aX+b$ </li>
</ul>
</li>
</ul>
<p>但是协方差描述的两个随机变量的关系，并没有考虑随机变量本身的量纲，如身高和体重之间的关系，单位不一样，因此又引入了相关系数 </p>
<ul>
<li>相关系数的定义：$Corr(X,Y)= \rho_{x,y}= \frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}}$  </li>
<li>相关系数的性质<ul>
<li>$|\rho_{x,y}| \le 1$，因为 $Cov^2(X,Y) \le Var(X)Var(Y)$ </li>
<li>若 $|\rho_{x,y}| = 0$，则两个随机变量不相关，严格来说，是线性不相关，即随机变量 $X$ 和 $Y$ 不存在严格的线性关系，但可能存在其他的函数关系，下面是一个反例<ul>
<li>$X \sim U(-\pi,\pi)$，$Y = cos(X)$，$EX=0$，$EY=0$，$Cov(X,Y) = 0$</li>
<li>协方差为0，证明不相关，但只是说明，两个随机变量没有线性相关性</li>
</ul>
</li>
<li>若 $\rho_{x,y} = 1$，则 $Y = aX + b$，且 $a &gt; 0$，称两个随机变量是严格的线性关系</li>
<li>若 $\rho_{x,y} = -1$，则 $Y = aX + b$，且 $a &lt; 0$，称两个随机变量是严格的线性关系</li>
<li>若 $\rho_{x,y} \in (0,1)$，则两个随机变量正相关，称两个随机变量存在相关关系 </li>
<li>若 $\rho_{x,y} \in (-1,0)$，则两个随机变量负相关，称两个随机变量存在相关关系 </li>
<li>若两个随机变量独立，则相关系数为0；但相关系数为0，不能推出两个随机变量独立<ul>
<li>特例：二元正态分布，$(X,Y) \sim N(\mu_1,\mu_2, \sigma_1^2, \sigma_2^2, \rho)$，其中 $\rho$ 为相关系数，则 $X$ 和 $Y$ 独立当且仅当 $\rho = 0$ </li>
</ul>
</li>
</ul>
</li>
</ul>
<p><br></p>
<h1 id="大数定律"><a href="#大数定律" class="headerlink" title="大数定律"></a>大数定律</h1><ul>
<li>如果对于任何 $\epsilon &gt; 0 $，都有 $lim_{n \rightarrow \infty} P(|\xi_n - \xi| &gt; \epsilon) = 0$，那么我们称随机序列 $\xi_n$ 依概率收敛到 $\xi$，记为 $\xi_n \rightarrow^p \xi$ </li>
<li>（弱）大数定律：{$X<em>n$} 是一列独立同分布的随机变量序列，具有共同的数学期望 $\mu$，共同的方差 $\sigma^2$，则 $(\bar{X} = \frac{1}{n}\sum</em>{k=1}^n X_k) \rightarrow^p \mu$，称 {$X_n$} 服从（弱）大数定律    <ul>
<li>大数定律描述的事情就是：$lim_{n \rightarrow \infty} P(|\bar{X} - \mu| &gt; \epsilon) = 0$ </li>
<li>利用切比雪夫不等式：$P(|X-EX| \ge \epsilon) \le  \frac{Var(X)}{\epsilon^2}$</li>
<li>$E(\bar{X}) = \mu$，$Var(\bar{X}) = \frac{\sigma^2}{n}$ </li>
<li>所以，$P(|\bar{X} - E(\bar{X})| \ge \epsilon) \le \frac{Var(X)}{\epsilon^2} = \frac{\sigma^2}{n\epsilon^2} \rightarrow 0$，当 $n \rightarrow \infty$ </li>
</ul>
</li>
<li>推论：如果以 $\xi_n$ 表示 $n$ 重伯努利实验中的成功次数，则 $f_n = \frac{\xi_n}{n} \rightarrow p$，即频率（依概率）收敛到概率。简单来说就是当抽样足够多的时候，频率趋近于真实的概率</li>
<li>在大量重复试验的基础上，随机事件多次发生的平均结果具有稳定性，不再具有随机性；大数定律表达了大量重复试验中随机现象所呈现的稳定性</li>
<li>几个常见的大数定理<ul>
<li><strong>切比雪夫大数定理</strong>：{$X_n$} 是相互独立的随机变量，他们都有有限的方差，且方差有共同的上界，即 $Var(X_i) \le C$，则对于任意的 $\epsilon &gt; 0$，$limP(|\frac{1}{n}\sum X_i - \frac{1}{n} \sum E(X_i) | &lt; \epsilon) = 1$，切比雪夫大数定理表明：当 $n$ 充分的大，$n$ 个独立随机变量平均值的离散程度较小，即经过算术平均以后的随机变量 $\frac{1}{n}\sum X_i $ 将比较密集地聚集在它的期望 $E(\frac{1}{n}\sum X_i ) =\frac{1}{n} \sum E(X_i) $  附近</li>
<li><strong>辛钦大数定理</strong>： {$X<em>n$} 是独立同分布的随机变量，且 $E(X_i) = \mu$，则对于任意的 $\epsilon &gt; 0$，$lim</em>{n \rightarrow \infty}P(|\frac{1}{n}\sum X_i  - \mu| &lt; \epsilon ) = 1$，辛钦大数定理表明：当随机变量序列独立同分布时，随机变量序列的算术平均依概率收敛于整体均值。</li>
<li><strong>伯努利大数定理</strong>：设 $n<em>A$ 是 $n$ 次独立重复试验中事件 $A$ 发生的次数，$p$ 是事件 $A$ 在每次试验中发生的概率，则对于任意的 $\epsilon &gt; 0$，有 $lim</em>{n \rightarrow \infty}P(|\frac{n_A}{n} - p| &lt; \epsilon ) = 1$，伯努利大数定理表明：在大量试验的基础上，频率依概率收敛于概率。</li>
</ul>
</li>
</ul>
<p><br></p>
<h1 id="中心极限定理"><a href="#中心极限定理" class="headerlink" title="中心极限定理"></a>中心极限定理</h1><ul>
<li>定义：{$X_n$} 是一列独立同分布的随机变量序列，具有共同的数学期望 $\mu$，共同的方差 $\sigma^2$，则 $P(\frac{X_1 + X_2 + \dots +X_n - n\mu}{\sigma \sqrt{n}} \le x) = \Phi(x)$ <ul>
<li>令 $S_n = X_1 + X_2 + \dots + X_n$，则 $E(S_n)= n\mu$，$Var(S_n) = n\sigma^2$</li>
<li>$\frac{X_1 + X_2 + \dots +X_n - n\mu}{\sigma \sqrt{n}} $ 就是随机变量 $S_n$ 的标准化</li>
<li>中心极限定理描述的就是，当 $n$ 趋于无穷时，序列和的标准化趋向于标准正太分布，即$\frac{X_1 + X_2 + \dots +X_n - n\mu}{\sigma \sqrt{n}} \rightarrow N(0,1) $ </li>
<li>注意：{$X_n$} 的随机变量不必是正态分布，可以是离散型随机变量，也可以是连续型的</li>
</ul>
</li>
<li>用中心极限定理逼近二项分布<ul>
<li>假设 {$X_n$} 都是0-1分布，那么 $S_n \sim B(n,p)$，可以将二项分布标准化成标准正太分布，从而计算二项分布的概率</li>
<li>$E(S_n) = np$，$Var(S_n) = np(1-p) = npq$，令 $q = 1-p$ </li>
<li>$P(k_1 \le X \le k_2) = P(\frac{k_1 - np}{\sqrt{npq})} \le \frac{X-np}{\sqrt{npq}} \le \frac{k_2 - np}{\sqrt{npq}}) = \Phi(\frac{k_2 - np}{\sqrt{npq}}) - \Phi(\frac{k_1 - np}{\sqrt{npq}})$ </li>
<li>$P(X=k)=P(k - \frac{1}{2} \le X \le k +\frac{1}{2}) = P(\frac{k - 0.5 - np}{\sqrt{npq})} \le \frac{X-np}{\sqrt{npq}} \le \frac{k + 0.5 - np}{\sqrt{npq}})$<br>$ = \Phi(\frac{k + 0.5 - np}{\sqrt{npq}}) - \Phi(\frac{k - 0.5- np}{\sqrt{npq}}) $<br>再根据微分中指定理<br>$=\phi(\frac{k-np}{\sqrt{npq}})\frac{1}{\sqrt{npq}} = \frac{1}{\sqrt{2\pi npq}}exp(-\frac{(k-np)^2}{2npq})$  </li>
</ul>
</li>
</ul>
<p><br></p>
<p><em>In Conclusion</em><br>大数定理和中心极限定理是连接概率论和数理统计的重要定理</p>
<p><br></p>
]]></content>
      <tags>
        <tag>概率论</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文阅读】—— OverFeat, Integrated Recognition, Localization and Detection using Convolutional Networks</title>
    <url>/2018/11/27/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94%20OverFeat,%20Integrated%20Recognition,%20Localization%20and%20Detection%20using%20Convolutional%20Networks/</url>
    <content><![CDATA[<blockquote>
<p>Title：OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks<br>Authors：Pierre Sermanet，Yann LeCun，et al<br>Session：ICLR, 2014<br>Abstract：本文改进了AlexNet，通过一个卷积网络来同时进行分类，定位和检测三个计算机视觉任务，并在ILSVRC 2013 中获得了很好的结果，定位任务冠军，分类任务亚军。 </p>
</blockquote>
<span id="more"></span>
<p><br><br><em>Preface</em></p>
<blockquote>
<p>Along with this paper, we release a feature extractor named “OverFeat” </p>
</blockquote>
<p>作者在论文中提到 OverFeat 是一个特征提取器，因为作者充分利用了卷积神经网络特征提取的功能，通过分类任务来训练卷积神经网络，再将卷积层提取到的特征应用于定位和检测的任务上，只需要改变网络最后的几层，即可实现不同任务间的切换。</p>
<p>论文的motivation为：卷积神经网络对于标签数据非常依赖，而得益于大数据集，如 ImageNet，才取得了突破。但是 ImageNet 数据集上的分类图片，物体大致分布在图片中心,但是感兴趣的物体常常在尺寸和位置（以滑窗的方式）上有变化。而解决这个问题传统上有几种方法：</p>
<ol>
<li>在不同位置和不同缩放比例上应用卷积网络，即使用一个大小变化的滑窗来遍历图片，但是这种滑窗的可视窗口可能只包涵物体的一个部分，而不是整个物体；对于分类任务是可以接受的，但是对于定位和检测有些不适合。 </li>
<li>训练一个卷积网络不仅产生类别分布，还产生一个物体位置的预测和bounding box的尺寸。</li>
<li>积累在每个位置和尺寸对应类别的置信度。</li>
</ol>
<p>上面提到了该论文通过一个网络解决了三个视觉任务：分类，定位和检测。</p>
<ul>
<li>分类任务：给定一张图片，模型需要输出这张图片的class。</li>
<li>定位任务：给定一张图片，模型不仅需要输出这张图片的class，还要定位出物体的位置，用一个bounding box的形式。</li>
<li>检测任务：给定一张图片，图片中有多个 object，需要把它全部找出来，包括位置和类别。</li>
</ul>
<p><br></p>
<h1 id="AlexNet-as-Prerequisite"><a href="#AlexNet-as-Prerequisite" class="headerlink" title="AlexNet as Prerequisite"></a>AlexNet as Prerequisite</h1><p>这篇论文的网络结构跟 AlexNet 基本类似，AlexNet 的网络结构在这里不再赘述，因为这篇论文对 AlexNet 改动最大的地方在于测试阶段，这里回顾一下AlexNet 的训练和测试</p>
<ul>
<li>训练阶段：每张训练图片的维度为 $256 \times 256$，裁剪出 $224 \times 224$ 大小的图片作为网络的输入。</li>
<li>测试阶段：输入图片为 $256 \times 256$，从图片的四个角和中心裁剪出 5 张 $224 \times 224$ 的图片，然后对这 5 张图片进行水平镜像操作，共得到 10 张图片。将这 10 张图片输入到网络，得到 10 个预测结果，最后取平均。</li>
</ul>
<p>OverFeat 在训练阶段和 AlexNet 完全一样，但是在测试阶段有很大的差别。</p>
<p><br></p>
<h1 id="Fully-Convolution-Networks"><a href="#Fully-Convolution-Networks" class="headerlink" title="Fully Convolution Networks"></a>Fully Convolution Networks</h1><p>FCN 为全卷积神经网络，顾名思义就是整个网络结构全是卷积层，没有全连接层，那么如何将一个普通的网络（卷积网络 &amp; 全连接网络）转成一个全卷积神经网络呢？</p>
<ol>
<li>将卷积层到全连接层，看成是对整张图片的卷积操作，即 filter size = feature map size。</li>
<li>将全连接层到全连接层，看成是采用 $1\ast1$ 大小的卷积核的卷积层。</li>
</ol>
<p>如下图，pooling 之后的 feature map 大小为 $5 \times 5$，通过一个 $5 \times 5$ 的卷积核就等同于将图片 flatten 之后再做全连接。而后面通过 $1 \times 1$ 的卷积核也等同于在做全连接网络的前向传播。</p>
<img src="/2018/11/27/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94%20OverFeat,%20Integrated%20Recognition,%20Localization%20and%20Detection%20using%20Convolutional%20Networks/fcn1.png" class="">
<p>假如将上面例子的输入图片改成 $16 \times 16$，会有什么结果呢？如下图，网络最后的输出为 $2 \times 2$ 的 feature map，相当于有一个 $14 \times 14$ 的 sliding window 在 $16 \times 16$ 的图片上滑动，stride 为 2，所以输出的左上角对应原图像左上角的 $14 \times 14$ patch 通过网络的结果。</p>
<img src="/2018/11/27/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94%20OverFeat,%20Integrated%20Recognition,%20Localization%20and%20Detection%20using%20Convolutional%20Networks/fcn2.png" class="">
<p>从这里，我们可以看出 FCN 相对于传统网络（卷积层 &amp; 全连接层）的优势，FCN 的输入的维度并不固定，而传统网络的输入维度必须固定，因为卷积层转全连接层的时候，全连接层的输入维度是固定的。</p>
<p>AlexNet在测试阶段的时候，采用了对输入图片的四个角落进行裁剪，进行预测，分别得到结果，最后的结果就是类似对应于上面 $2\ast2$ 的预测图（实际大小为 $2 \ast 2 \ast num _class$ ）。这个 $2\ast2$ 的每个像素点，就类似于对应于一个角落裁剪下来的图片通过网络得到的预测分类结果。只不过 AlexNet 把这4个像素点，相加在一起，求取平均值，作为该类别的概率值。而 OverFeat 采用另外的方法。</p>
<p><br></p>
<h1 id="Offset-Max-Pooling"><a href="#Offset-Max-Pooling" class="headerlink" title="Offset Max-Pooling"></a>Offset Max-Pooling</h1><p>先举一个一维的例子，有 20 个神经元，选择 size = 3 的 non-overlapping 池化层，那么结果将会是下面 $\bigtriangleup = 0$ 的情况；我们除了以 1 作为初始位置外，还可以从位置 2 或者 3 开始，它们分别对应 $\bigtriangleup = 1$ 和 $\bigtriangleup = 2$ 的 情况，在一般的 CNN 网络中，我们只用 $\bigtriangleup = 0$ 的结果，而 offset pooling 可以设置 offset，从而得到多个池化的结果。</p>
<img src="/2018/11/27/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94%20OverFeat,%20Integrated%20Recognition,%20Localization%20and%20Detection%20using%20Convolutional%20Networks/1d_offset_pool.png" class="">
<p>从一维升级到二维的情况，我们分别在两个维度进行 offset pooling，那么 $(\bigtriangleup_x, \bigtriangleup_y)$ 就会共有 9 种池化结果。如果我们在做图片分类的时候，在网络的某一个池化层加入了 offset pooling，然后把这 9 种池化结果，分别送入后面的网络层，最后我们的图片分类输出结果就可以得到 9 个预测结果，输出的实际维度为 $ 3  \ast  3 \ast num _class $ （每个类别都可以得到 9 种概率值，然后我们对每个类别的 9 种概率，取其最大值，做为此类别的预测概率值）。 </p>
<p><br></p>
<h1 id="OverFeat-Classification"><a href="#OverFeat-Classification" class="headerlink" title="OverFeat Classification"></a>OverFeat Classification</h1><h2 id="网络架构"><a href="#网络架构" class="headerlink" title="网络架构"></a>网络架构</h2><p>论文提出了两种网络架构，分别是 Fast 模型和 Accurate 模型，网络结构分别如下，两种架构都是在 AlexNet 上进行修改，对比 AlexNet 区别在于</p>
<ul>
<li>训练时输入维度固定，测试时用多尺度输入（multi-scale），即输入的维度不固定。</li>
<li>没有用 local response normalization，因为发现没什么用。</li>
<li>没有采用 overlap 的 max-pooling。</li>
<li>第一层卷积层的参数作了稍微改动，accurate模型第一个卷积核改为 $7 \times 7$，stride为 2。3,4,5 层的卷积核个数进行了修改。</li>
</ul>
<p>其他方便和 AlexNet 一样，训练的参数几乎一致。</p>
<img src="/2018/11/27/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94%20OverFeat,%20Integrated%20Recognition,%20Localization%20and%20Detection%20using%20Convolutional%20Networks/fast.png" class="">
<img src="/2018/11/27/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94%20OverFeat,%20Integrated%20Recognition,%20Localization%20and%20Detection%20using%20Convolutional%20Networks/accurate.png" class="">
<p>OverFeat 最具创新的地方在于它的测试过程，作者认为 AlexNet 测试的做法存在两个问题</p>
<ul>
<li>将原来的图片从 4 个角落里裁剪出图片，会把原图片的很多区域给忽略。</li>
<li>裁剪窗口会存在重叠，因此会有冗余的计算量。</li>
</ul>
<p>而 OverFeat 算法在测试过程，不再是固定维度的一张图片，而是采用 6 张维度不同的图片作为输入，这就是 multi-scale 多尺度，如下表格，输入的维度各不相同，然后当前向传播到第五层的时候（accurate model 的第五层是卷积层过渡到全连接层），进行 offset pooling，从 layer-5 pre-pool 到 layer-5 post-pool，这一步的实现是通过池化大小为 (3, 3) 进行池化， $\bigtriangleup_x =1,2,3$，$\bigtriangleup_y = 1,2,3$，因此对于每一个 feature map，会得到 9 个池化结果图，比如下面 scale-1，$17 \times 17 \rightarrow (5 \times 5) \times(3 \times 3)$ 。</p>
<img src="/2018/11/27/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94%20OverFeat,%20Integrated%20Recognition,%20Localization%20and%20Detection%20using%20Convolutional%20Networks/multi-scale.png" class="">
<p>从 layer-5 post-pool 到 classifier map(pre-reshape)：我们知道在训练的时候，从卷积层到全连接层，输入的大小是 $4096 \ast(5\ast5)$。但是我们现在输入的是各种不同大小的图片，因此就需要采用 FCN 的方法，让网络继续前向传导。我们从 layer-5 post-pool 到第六层的时候，如果把全连接看成是卷积，那么其实这个时候卷积核的大小为 $5 \ast5$，因为训练的时候，layer-5 post-pool 得到的结果是 $5 \ast5$。因此在预测分类的时候，假设layer-5 post-pool 得到的是 $7\ast9$（上面表格中的 scale-3），经过 $5\ast5$ 的卷积核进行卷积后，那么它将得到 $(7-5+1)\ast(9-5+1)=3\ast5$的输出。</p>
<p>注意，因为我们采用了 offset pooling，因此 classifier map (pre-reshape) 的实际维度都会乘以 9 倍，因此对于每一个 scale，由于 FCN 和 offset pooling，我们会得到很多个预测结果，比如 scale-3，我们得到 105 个（FCN导致15个，offset导致9个），然后就取最大的作为该 scale 的预测值，分析不同的 scale 得到最后的预测值。</p>
<p><br><br><em>In Conclusion</em></p>
<p>定位和检测任务在这里不做说明，一来不是我关注的重点且论文没有多说，二来它们只是把后面的全连接网络进行了修改而已。</p>
<p>这篇论文的核心在于 FCN 和 offset pooling，以及迁移学习的重要性，卷积神经网络在挖掘特征上很强势，因此才可以将特征提取层迁移到其它任务上。</p>
<p><br><br><em>reference</em></p>
<ol>
<li><a href="https://www.jianshu.com/p/6d441e208547">blog1</a> </li>
<li><a href="https://www.cnblogs.com/liaohuiqiang/p/9348276.html">blog2</a></li>
</ol>
<p><br></p>
]]></content>
  </entry>
  <entry>
    <title>【论文阅读】—— Aggregated Residual Transformations for Deep Neural Networks</title>
    <url>/2018/12/03/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Aggregated-Residual-Transformations-for-Deep-Neural-Networks/</url>
    <content><![CDATA[<blockquote>
<p>Title：Aggregated Residual Transformations for Deep Neural Networks<br>Authors：Saining Xie，Kaiming He<br>Session：CVPR, 2017<br>Abstract：提高网络性能，除了深度和宽度外，作者提出了 cardinality 的概念，并修改 ResNet 成一个高度模块化的网络 ResNeXt，对比 ResNet 和 Inception，ResNeXt 的超参数很少，不需要精细的网络设计，且网络参数和计算量都比 ResNet 少。</p>
</blockquote>
<span id="more"></span>
<p><br></p>
<p><em>preface</em></p>
<ul>
<li>VGG, ResNet：通过堆叠相同形状的的网络 block，即 depth。</li>
<li>Inception：split-transform-merge 的模式，首先用 $1 \times 1$ 的卷积核进行 split，然后用 $3 \times 3$ 或 $5 \times 5$ 的卷积核进行 transform，最后进行 channel 级别上的 merge。</li>
<li>ResNeXt：采用 VGG/ResNet 的网络的 depth 加深方式，同时利用 split-transform-merge 策略。</li>
</ul>
<p><br></p>
<h1 id="ResNeXt"><a href="#ResNeXt" class="headerlink" title="ResNeXt"></a>ResNeXt</h1><p>作者想把 Inception 的思想融入 ResNet，但是 Inception block 都是需要人工设计的，就好像 Inception-ResNet 中的各个 residual block那样。作者想希望设计一个高度模块化的网络，因此提出了下面的 residual block，左边的图是 ResNet 的 residual block，右边是 ResNeXt 的，可以看出右边的图 residual block 的形式就等同于 Inception block，只是右边的图每一条 path 都是相同的卷积层，或者说相同的拓扑结构，作者把 path 的数量称为 cardinality，而与 Inception block 不同的是，每一条 path 最后的并不是 concatenate，而是 summation，因为每一条 path 的拓扑相同，因此每一条 path 的输出大小相同，因此作者采取相加的方式进行 merge。</p>
<img src="/2018/12/03/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Aggregated-Residual-Transformations-for-Deep-Neural-Networks/1.png" class="">
<p>作者提出的这种 residual block 可以看成是 Inception-ResNet 论文里面的 block，或者看成 grouped convolution（在 AlexNet 论文中提到）。</p>
<p>下图的 c 是 grouped convolution，AlexNet 中用到 grouped convolution 主要是为了并行训练，放到两个GPU中训练网络；而 grouped convolution 首先会将输入按照 channel 分成多个 group，然后每个 group 再进行卷积，卷积后再进行拼接。ResNeXt 可以看成是 grouped convolution 是因为本身在一个 block 里面就有 32 条 path，现在用 grouped convolution 的角度只是先把这 32 条 path 整合成 channel 为 128 的数据，再分 32 组 group 去卷积，每组卷积的输入输出依旧为 4。作者在实现 ResNeXt 的时候就是采取 grouped convolution 的方式，这一思想再后面的论文也有体现，比如 shuffleNet。  </p>
<img src="/2018/12/03/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Aggregated-Residual-Transformations-for-Deep-Neural-Networks/2.png" class="">
<p>作者在做对比实验的时候，是保持 ResNet 和 ResNeXt 的 complexity 一致，complexity 一般是指网络的参数和计算量，具体到每个 block，为了维持一致的 complexity，需要计算 ResNet 的参数量和 ResNeXt 的参数量。假设 cardinality 为 C，bottleneck width 为 d，则</p>
<script type="math/tex; mode=display">256 \times 64 +3 \times 3 \times 64 \times 64+64 \times 256 = C \times(256 \times d + 3 \times3 \times d \times d + 64 \times 256) \approx 70k</script><p>$C = 32,d=4$ 为上图的情况。<strong>而 ResNet 相当于是 $C=1$ 的情况，只有一条 path。</strong></p>
<img src="/2018/12/03/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Aggregated-Residual-Transformations-for-Deep-Neural-Networks/3.png" class="">
<p><br></p>
<h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><p>我们把 50 层 ResNeXt，C = 32，d = 4 的网络标记为 $ResNeXt-50（32 \times 4d）$</p>
<p><strong>Cardinality vs bottleneck width</strong></p>
<p>首先作者需要找出一对最好的 cardinality 和 width，且和 ResNet 保持一致的 complexity。</p>
<ul>
<li>ResNext-50 最好的 error 是 22.2%，比 ResNet-50 低了 1.7%。</li>
<li>当 cardinality 增大，error 持续减小</li>
</ul>
<img src="/2018/12/03/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Aggregated-Residual-Transformations-for-Deep-Neural-Networks/4.png" class="">
<p><strong>Increasing Cardinality vs Going Deeper/Wider</strong></p>
<p>deeper 指网络层数增多，ResNet-101 到 ResNet-200；wider 指 bottleneck width 增宽，即 channel 数增多；</p>
<ul>
<li>将 ResNet 加深和加宽，error 下降得不多，分别下降了 0.3% 和 0.7%。</li>
<li>增大 cardinality，error 下降得比较明显，作者分别对 ResNet 和 ResNeXt 进行 cardinality 增大，ResNet 原来 cardinality 为 1，ResNeXt 原来为 32，均增大一倍。</li>
</ul>
<img src="/2018/12/03/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Aggregated-Residual-Transformations-for-Deep-Neural-Networks/5.png" class="">
<p><strong>对比其它模型</strong></p>
<img src="/2018/12/03/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Aggregated-Residual-Transformations-for-Deep-Neural-Networks/6.png" class="">
<p><br><br><em>In Conclusion</em><br>个人觉得，ResNeXt 最大的贡献在于提供了一种高度模块化的网络，不用刻意地去设计，只需要改变 cardinality 和每个 block 的卷积层就可以设计一个模型了；而 cardinality 这个点也是很有创新性。<br>cardinality 是衡量神经网络在深度和宽度之外的另一个重要因素。作者还指出，与 Inception 相比，这种新的架构更容易适应新的数据集/任务，因为它有一个简单的范式，而且需要微调的超参数只有一个，而 Inception 有许多超参数（如每个路径的卷积层核的大小）需要微调。</p>
<p><br></p>
]]></content>
  </entry>
  <entry>
    <title>【论文阅读】—— Batch Normalization, Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
    <url>/2018/12/01/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift/</url>
    <content><![CDATA[<blockquote>
<p>Title：Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift<br>Authors：Christian Szegedy, Sergey Ioffe<br>Session：ICML, 2015<br>Abstract：Inception 的作者提出 batchNorm，从而解决 internal covariate shift 的问题，使得网络训练更快，并在自己的 Inception 网络上加入 batchNorm，使得在 ImageNet 上的错误率进一步降低，该网络被称为 Inception-BN。</p>
</blockquote>
<span id="more"></span>
<p><br></p>
<p><em>preface</em></p>
<p>为什么深度神经网络<strong>随着网络深度加深，训练起来越困难，收敛越来越慢？</strong>这是个在深度学习领域很接近本质的好问题。很多论文都是解决这个问题的，比如 ReLU 激活函数，再比如 Residual Network，batchNorm 本质上也是解释，并从某个不同的角度来解决这个问题的。 </p>
<p><br></p>
<h1 id="Internal-Covariate-Shift"><a href="#Internal-Covariate-Shift" class="headerlink" title="Internal Covariate Shift"></a>Internal Covariate Shift</h1><p>深度网络难以训练的原因在于，随着前向传递，每一层输入数据的分布都在改变，一个层的输入受前面所有层的参数的影响，细微的改变会被放大，由于每个层的输入数据分布不断在改变，网络需要不断地去学习新的分布。</p>
<p>covariate shift 是用来描述一个学习系统的输入数据的分布发生变化。而其实 covariate shift 也可以用来描述一个子网络，即一个子网络的输入分布发生变化，它同样也会经历 covariate shift，而在神经网络里面，我们把一个层的输入数据分布发生变化叫做 internal covariate shift。</p>
<p>batchNorm 的思想来自于：之前的研究表明如果在图像处理中对输入图像进行白化（Whiten）操作的话——所谓<strong>白化</strong>，<strong>就是对输入数据分布变换到0均值，单位方差的正态分布</strong>——那么神经网络会较快收敛。因此作者推论：图像是深度神经网络的输入层，做白化能加快收敛，那么其实对于深度网络来说，其中某个隐层的神经元是下一层的输入，即深度神经网络的每一个隐层都是输入层，只是相对下一层来说而已，那么能不能对每个隐层都做白化呢？</p>
<p>而 batchNorm 的基本思想就是：把网络中每个隐层节点的<strong>激活函数输入分布</strong>固定下来，就可以避免 internal covariate shift。为什么需要固定激活函数输入分布呢？我们知道激活函数，如 sigmoid 函数，它的两边都是梯度饱和区域，只有 0 附近的区域是有梯度的，因此如果能将 sigmoid 的输入固定到 0 附近，那么就可以解决梯度的问题。</p>
<p><br></p>
<h1 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h1><p>上面提到，batchNorm 就好比 whitening，将激活函数的输入进行标准化，从而使它的均值为 0，方差为 1。需要注意的是，标准化的对象是某一隐层的一个神经元，假设某一隐层有 d 个神经元，则分别对每个神经元进行下面的标准化操作。均值和方差是一个 batch 数据在该隐藏层神经元输出的均值和方差。</p>
<script type="math/tex; mode=display">x^k = Wu^k +b</script><script type="math/tex; mode=display">\hat{x}^k = \frac{x - E(x^k)}{\sqrt{Var(x^k)}}，k \in \{1,2, ..., d \}</script><p>所以经过 batchNorm，每个激活函数的输入数据分布都会是一个标准正态分布，即解决了上面提到的 internal covariate shift 的问题。但是进行这样的标准化可能会影响网络表达的能力，因为数据分布被强行拉到了 0 附近，因此作者将标准化后的数据再进行一个 scale 和 shift 的操作，$\gamma$ 和 $\beta$ 是两个学习的参数，且每个神经元都有对应的 $\gamma$ 和 $\beta$。如果 $\gamma^k = \sqrt{Var(x^k)}$，$\beta^k = E(x^k)$，那么就会将它重新映射到原来的分布。</p>
<p>$y^k = \gamma^k \hat{x}^k + \beta^k$</p>
<p>对于一个普通的前向传播网络，batchNorm 层加到激活函数前，如下图</p>
<img src="/2018/12/01/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift/bn.png" class="">
<p>算法的流程如下</p>
<img src="/2018/12/01/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift/bn1.png" class="">
<p>上面提到的算法是训练时候的算法，训练的时候我们是用 mini-batch 的方式去训练一个网络，但是预测的时候，我们是一个一个 example 的去预测，我们不希望一个预测的样本受到其它样本的干扰（训练的时候，batch 内样本互相影响）。作者的解决方法是：预测的时候固定均值和方差，而这个固定的均值和方差是由训练阶段各个 batch 的均值和方差的一个统计量，简单来说就是，用各个 batch 的均值和方差的平均值作为预测阶段的均值和方差。</p>
<p><strong>CNN 上的 batchNorm</strong></p>
<ul>
<li>在 feature map 上进行标准化，一个 feature map 类比一个隐层的神经元。</li>
<li>前向传递网络中一个 batch 大小为 m，而在 CNN 中做 batchNorm，假如 feature map 的大小为 $p \times q$，那么就相当于 batch size 为 $m \times p \times q$。</li>
<li>简单来说就是，将 batch 里面同一层的 feature map 进行归一化，归一化的时候，不以 feature map 为一个样本，而是以 feature map 的一个 pixel 为一样本，再求 pixel 级别的均值方差。</li>
<li>不同的 channel 相当于隐层不同的神经元，所以每个 channel 的feature map 对应一个 $\gamma$ 和 $\beta$。</li>
</ul>
<p>batchNorm 的好处</p>
<ul>
<li>正则作用，训练时候，batch 里面的样本不再是没有关系的，而是相互影响。</li>
<li>增快训练效率，batchNorm 能够解决梯度的问题，因此学习率可以稍微调大一点，加快训练速度。</li>
</ul>
<p><br></p>
<h1 id="Inception-BN"><a href="#Inception-BN" class="headerlink" title="Inception-BN"></a>Inception-BN</h1><p>这个版本相对于 Inception-v1，除了在每一个激活函数前加入了 batchNorm 之外，还进行了下面的调整</p>
<ul>
<li>跟 VGG 一样，Inception block 里面的 $5 \times 5$ 的卷积核用连续两个 $3 \times 3$ 来代替。</li>
<li>在 Inception block 里面的池化层，不仅仅使用 max-pooling，还会使用 averaged-pooling。</li>
<li>在 Inception-v1，会在 Inception block 之间添加 max-pooling 进行降维。这种结构将会被去掉，而在 concatenation 之前，会用一个 stride 为 2 的卷积层和池化层进行降维。</li>
</ul>
<img src="/2018/12/01/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift/net.png" class="">
<p>由于 batchNorm 能够加快网络的训练，因此作者在训练上还进行了以下的调整</p>
<ul>
<li>增大学习率</li>
<li>去除 dropout</li>
<li>减小 L2 正则项参数</li>
<li>加快学习率 decay，因为网络训练加快了</li>
<li>去除 LRN</li>
</ul>
<p>作者根据这些设置训练了好几个网络</p>
<ul>
<li>Inception：即 Inception-v1，初始学习率为 0.0015.</li>
<li>BN-Baseline：即 Inception-BN</li>
<li>BN-x5：Inception-BN，初始学习率为 5 倍，即 0.0075.</li>
<li>BN-x30：跟 BN-x5 一样，初始学习率为 0.045.</li>
<li>BN-x5-Sigmoid：跟 BN-x5 一样，换成 sigmoid 激活。</li>
</ul>
<p>从下面结果可以看出，加入 batchNorm 之后能够明显加快网络训练，增大学习率使学习更快，BN-x30 比 Inception 快了接近 16 倍，并且准确率更高。注意的是：BN-x5-Sigmoid 也能达到很好的结果，是因为 batchNorm 解决了梯度的问题，如果没有 batchNorm，作者说准确率不及千分之一。</p>
<img src="/2018/12/01/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift/res1.png" class="">
<img src="/2018/12/01/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift/res2.png" class="">
<p><br></p>
]]></content>
  </entry>
  <entry>
    <title>【论文阅读】—— Data-Free Learning of Student Networks</title>
    <url>/2019/10/30/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Data-Free-Learning-of-Student-Networks/</url>
    <content><![CDATA[<blockquote>
<p>Title：Data-Free Learning of Student Networks<br>Authors：Hanting Chen, Yunhe Wang et.al<br>Conference：ICCV 2019<br>Abstract：作者提出了一种无需原始数据的蒸馏方式，通过用对抗生成网络（GAN）去生成数据集，能够达到原数据的蒸馏效果</p>
</blockquote>
<span id="more"></span>
<p><em>preface</em></p>
<p>传统的知识蒸馏框架是基于以下假设：用来训练教师网络的数据集，如 ImageNet、Cifar，同时用来做知识蒸馏时的数据集，但是在现实场景下，用来训练教师网络的数据可能会处于隐私保护的原因，不会公开出来，而公开的仅仅只有教师网络这一个模型，相当于是一个接口，这时候我们没有了训练教师网络的数据集，应该如何进行知识蒸馏来训练学生网络呢？</p>
<p>解决无数据下的知识蒸馏的基本思路是：构造合成数据集，使得合成的数据集能够模拟原数据集的数据分布。Data-Free 下的知识蒸馏的第一篇论文是 NIPS 2017 workshop 的一篇文章，<a href="https://arxiv.org/pdf/1710.07535.pdf"> Data-free knowledge distillation for deep neural networks</a>，这篇文章的主要思路是通过一些 meta-data 来重构数据集，meta-data 为训练教师网络时收集的一些信息，如网络输出层或中间层的激活值的均值或方差等，然后用这些 meta-data 来指导生成图片，使得合成的图片的激活值方差尽可能地接近 meta-data。</p>
<img src="/2019/10/30/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Data-Free-Learning-of-Student-Networks/1.png" class="">
<p>但这种方法还是需要 meta-data 作为支撑，而 ICCV 2019 的这篇文章通过 GAN 直接模拟合成数据集，且合成的数据集蒸馏效果更好。</p>
<p><br></p>
<h2 id="GAN-Loss-Function"><a href="#GAN-Loss-Function" class="headerlink" title="GAN Loss Function"></a>GAN Loss Function</h2><p>作者提出的整体架构如下，作者把教师网络作为判别器去指导生成器的训练，训练好的生成器生成图片，然后分别通过教师和学生网络，然后进行知识蒸馏。</p>
<img src="/2019/10/30/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Data-Free-Learning-of-Student-Networks/2.png" class="">
<p>但是传统 GAN 中的判别器时一个二分类的判别器，用来判断生成的图片的真假，而教师网络是多分类网络，用来判断输入图像属于哪一个类别，为了能够使得生成器生成的数据能够尽可能地像原数据集，作者提出了 3 个损失来约束生成器。</p>
<h3 id="One-hot-Loss"><a href="#One-hot-Loss" class="headerlink" title="One-hot Loss"></a>One-hot Loss</h3><p>先定义一些符号：</p>
<ul>
<li>生成器的输入是一些随机的向量：${z^1,z^2,…,z^n }$</li>
<li>生成器的输出是一些生成的图片：${x^1,x^2,…,x^n }$，$x^i=G(z^i)$</li>
<li>将生成的图片输入到判别器（教师网络）得到输出：${y^1_T,y^2_T,…,y^n_T }$，$y_T^i=D(x^i)$</li>
<li>判别器（教师网络）预测的类别：${t^1,t^2,…,t^n }$，$t^i=argmax_j(y^i_T)_j$</li>
</ul>
<p>要使生成的图片尽可能像原数据集，首先要使得生成图片通过教师网络后的结果 $y_T^i$ 尽可能地像一个 one-hot 向量，即能够预测出某一个类别出来，因此我们把 $t_i$ 看作是一个伪标签来约束生成器。</p>
<script type="math/tex; mode=display">
L_{oh}=\frac{1}{n}\sum_i H_{cross\_entropy}(y_T^i, t^i)</script><h3 id="Information-Entropy-Loss"><a href="#Information-Entropy-Loss" class="headerlink" title="Information Entropy Loss"></a>Information Entropy Loss</h3><p>除了从单张图片去考虑生成器的生成质量，还要从整体上去考虑，要使生成器生成的图片尽可能地像训练集原始的图片，其次还要做到生成的图片每个类别是均衡的。这里作者用了信息熵来表示各种类别的均衡性，当每个类别生成的概率都一样时，信息熵达到最大值，因此通过最大化信息熵来训练网络，$H_{info}(p)=-\frac{1}{k}\sum_i p_ilog(p_i)$</p>
<script type="math/tex; mode=display">
L_{ie}=-H_{info}(\frac{1}{n}\sum_i y^i_T)</script><p>每个类别的概率为教师网络输出层在不同样本下的平均</p>
<h3 id="Activation-Loss"><a href="#Activation-Loss" class="headerlink" title="Activation Loss"></a>Activation Loss</h3><p>第三个损失函数是考虑到图片的真实性，一般真实的图片对应的网络激活值都比较大，所以把网络激活值同时加到损失函数里面进行约束。这里用到的特征层为全连接层前的特征图 $f_T^i$</p>
<script type="math/tex; mode=display">
L_a=-\frac{1}{n}\sum_i||f_T^i||_1</script><p><br></p>
<p>整体的 Loss 为三个 loss 的加权</p>
<script type="math/tex; mode=display">
L_{Total}=L_{oh}+\alpha L_a + \beta L_{ie}</script><p>伪代码如下，训练分为两个阶段，先训练生成器，使得生成器合成的图片尽可能接近真实数据集分布，第二个阶段为用训练好的生成器生成图片来进行知识蒸馏。</p>
<img src="/2019/10/30/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Data-Free-Learning-of-Student-Networks/3.png" class="">
<p><br></p>
<h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><h3 id="Ablation-study"><a href="#Ablation-study" class="headerlink" title="Ablation study"></a>Ablation study</h3><p>先验证三个 loss 对应的作用，实验设置为：教师网络为 LeNet-5，学生网络为 LeNet-5-HALF，数据集为 MINST。下表结果为用不同损失函数训练出来的生成器进行知识蒸馏后学生网络的性能，从结果可以看出，在 MINST 这种简单的数据集，即使不训练生成器，随机的图片都有 88 的精度，而当不同 loss 被考虑的时候，可以看出信息熵损失是最有用的，当没有信息熵损失的时候，结果都很差。另外两个损失作为信息熵损失的补充，进一步提升了生成图片的质量</p>
<img src="/2019/10/30/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Data-Free-Learning-of-Student-Networks/4.png" class="">
<h3 id="Comparison-with-other-methods"><a href="#Comparison-with-other-methods" class="headerlink" title="Comparison with other methods"></a>Comparison with other methods</h3><h4 id="Results-in-MINST"><a href="#Results-in-MINST" class="headerlink" title="Results in MINST"></a>Results in MINST</h4><ul>
<li>Normal distribution：不训练生成器，直接用正态分布的随机数据去进行知识蒸馏训练学生网络，然后再用MINST 测试。</li>
<li>Alternative data：用一个类似 MINST 的数据集 USPS，也是一个手写数字的数据集，用这个数据集去进行知识蒸馏，再用 MINST 测试，可以看出即使是相近的数据集，他们的数据分布也会有一些不一样，所以当用 MINST 测试的时候性能会下降。</li>
<li>Meta-data：上面提到的 NIPS 2017 workshop 上的方法，用 meta-data 生成数据集，再用合成的数据集来进行知识蒸馏，再用 MINST 测试。</li>
<li>DAFL：用 GAN 的方式生成数据集，用合成的数据进行知识蒸馏，用用 MINST 测试。</li>
</ul>
<p>可以看出用 GAN 生成的数据集是最像原数据集的，因为用合成的图片训练的知识蒸馏比用原 MINST 数据知识蒸馏，性能并没有降太多。</p>
<img src="/2019/10/30/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Data-Free-Learning-of-Student-Networks/5.png" class="">
<h4 id="Results-in-Cifar"><a href="#Results-in-Cifar" class="headerlink" title="Results in Cifar"></a>Results in Cifar</h4><img src="/2019/10/30/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Data-Free-Learning-of-Student-Networks/6.png" class="">
<h3 id="Visualization"><a href="#Visualization" class="headerlink" title="Visualization"></a>Visualization</h3><p>从图片可以看出，生成图片的轮廓还是有点像原始图片的。</p>
<img src="/2019/10/30/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Data-Free-Learning-of-Student-Networks/7.png" class="">
<p>用生成图片训练的学生网络的 filter 和用原数据训练的教师网络的 filter 接近。</p>
<img src="/2019/10/30/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Data-Free-Learning-of-Student-Networks/8.png" class="">
<p><br></p>
]]></content>
  </entry>
  <entry>
    <title>【论文阅读】—— Convolutional Networks with Adaptive Computation Graphs</title>
    <url>/2018/12/16/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Convolutional-Networks-with-Adaptive-Computation-Graphs/</url>
    <content><![CDATA[<blockquote>
<p>Title：Convolutional Networks with Adaptive Computation Graphs<br>Authors：Andreas Veit, Serge Belongie<br>Abstract：作者提出了 Adanet，是一种自适应的计算图，在 ResNet 的基础上，加入了门结构来决定是否需要执行某个 block，引入这种门结构并没有带来额外太多的计算量，反而能加快训练过程。</p>
</blockquote>
<span id="more"></span>
<p><br></p>
<p><em>preface</em></p>
<p>深度学习领域在网络深度和网络结构上都进行了很多的研究，尽管这些网络在结构细节上会有不同，但是他们有一点是相同的：他们都是一个固定的模型，网络结构与输入图像没有任何关系。</p>
<p>《Residual networks behave like ensembles of relatively shallow networks》论文中提到去掉  ResNet 的某几层并不会影响性能，这也表明没有任何一个独立的层是对性能起到关键性作用的，网络层存在着冗余，很多计算量是没有必要的。因此作者就提出了网络结构并不需要固定不变，对于不同的输入，可以组装不同的网络去进行预测。</p>
<p>作者提出的 AdaNet 是基于 ResNet 进行的改进，在每一个 residual block 中加入一个 gating function 来决定是否需要执行这个 block 的操作。</p>
<img src="/2018/12/16/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Convolutional-Networks-with-Adaptive-Computation-Graphs/1.png" class="">
<p>作者的这个工作和 regularization with stochastic noise 有关，stochastic path 随机的丢弃 ResNet 的某些层，而 AdaNet 根据输入来决定跳过哪些层。</p>
<p>AdaNet 也可以看成是 attention 机制的一个例子，选择重要的层去执行；而 SENet 是根据卷积输出中 channel 的重要性进行 rescale。</p>
<p><br></p>
<h1 id="AdaNet"><a href="#AdaNet" class="headerlink" title="AdaNet"></a>AdaNet</h1><p>ResNet 可以定义如下，它的提出使得上千层的网络得以诞生，但是研究也表明，尽管这些层是一起训练的，去掉任何一层也不会影响最后的性能。</p>
<script type="math/tex; mode=display">x_l = x_{l-1} + F_l(x_{l-1})</script><h2 id="Adaptive-Computation-Graph"><a href="#Adaptive-Computation-Graph" class="headerlink" title="Adaptive Computation Graph"></a>Adaptive Computation Graph</h2><p>作者受到《Residual networks behave like ensembles of relatively shallow networks》这篇论文结果的启发，思考卷积网络是否需要一个固定的结构，由于固定的结构会带来很多冗余的计算，因此作者想自适应地选择或组装 (assemble) 网络，自适应的网络会根据输入来决定通过哪些必要的层，而跳过不必要的层，因此提出了门结构的 residual block。$z(x_{l-1})$ 是 gating function，根据每一层的输入来决定是否执行。</p>
<script type="math/tex; mode=display">x_l = x_{l-1} + z(x_{l-1}) \cdot F_l(x_{l-1}), z(x_{l-1}) \in \{0,1 \}</script><p>作者把上面的方式和 highway network 进行了对比，highway network 中每一层的输出为：$x<em>l = (1-z(x</em>{l-1})) \cdot x<em>{l-1} + z(x</em>{l-1}) \cdot F<em>l(x</em>{l-1})$，两个式子有点类似，但是 highway network 的式子更像是一种 soft 注意力机制，关注某些层和不关注其他层。而 AdaNet 是直接跳过不执行某一层。</p>
<h2 id="Gating-Unit"><a href="#Gating-Unit" class="headerlink" title="Gating Unit"></a>Gating Unit</h2><p>$z<em>(x</em>{l-1})$ 分为两个阶段</p>
<ol>
<li>根据输入，评估需要执行 residual block 的概率。</li>
<li>根据概率值，抽样出一个离散的样本。</li>
</ol>
<img src="/2018/12/16/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Convolutional-Networks-with-Adaptive-Computation-Graphs/2.png" class="">
<p>考虑到 AdaNet 的主要目的是为了减少不必要的计算量，因此引入的门结构必须比较轻量，不然整体的计算量无法减少。首先作者采用 global avg-pooling 进行 channel 级别的特征提取。</p>
<script type="math/tex; mode=display">z_c = \frac{1}{H\times W}\sum \sum x_{i,j,c}</script><p>通过 global pooling 得到一个 $1 \times 1 \times C$ 的 channel descriptor，然后再用两层全连接网络去捕获 channel 之间的关系。</p>
<script type="math/tex; mode=display">\beta = W_2\sigma(W_1z)</script><p>$W_1 \in R^{\frac{C}{d}\times C}$，$W_2 \in R^{2\times \frac{C}{d}}$，$d$ 为 reduction ratio。输出的 $\beta$ 是一个两个元素的一维向量，表征着执行和跳过该 block 的概率。</p>
<p>假设 $X$ 是一个随机变量，$X = 0$ 表示跳过该 block，$X = 1$ 表示执行该 block，那么 $P(X = 0) = \beta_0$，$P(X = 1) = \beta_1$，我们现在想得到一个服从该离散变量分布的一个值 $x$，即 0 或 1，一个最简单的做法是根据概率去采样，但是采样出来的只有 $x$ 的值，并没有生成 $x$ 的式子，即采样这个操作不可微，本来 $x$ 的值是和 $\beta_0$、$\beta_1$ 有关的，但是采样这个操作，我们得到的 $x$ 无法对 $\beta$ 进行求导，这就使得神经网络里面的梯度无法回传。</p>
<p>我们能否给出一个以 $\beta$ 为输入的函数，输出的结果为随机变量 $X$ 的采样值 $x$ 呢？答案是肯定的，作者采用了 Gumbel-Max trick 进行再参数化，如下，$z$ 是一个 onehot 向量，$g_i$ 是 Gumbel 分布 G 的采样，$G = -log(-log(U))$，$U \sim Uniform(0,1)$</p>
<script type="math/tex; mode=display">z = arg\ max(log\beta_i + g_i)</script><p>即</p>
<script type="math/tex; mode=display">P(x|Pa_x)=\begin{cases}  1, & i=argmax_j(log(\beta_j) + g_j) \\  0, & otherwise  \end{cases}</script><p>因此得到的 $z$ 是一个两个元素的一维向量，元素值为 0 或 1，按照上图的流程，若 $z_1 = 1$，则执行该 block。</p>
<p>上面的 Gumbel-Max trick 实现了从 $\beta$ 向量到 $z$ 向量的转变，但是上面的流程中，argmax 操作是不可导的，作者提出用可导的 softmax 函数去代替 argmax，因此得到的 $z$ 向量为</p>
<script type="math/tex; mode=display">z_i = softmax((log(\beta_i) + g_i) / \tau)</script><p>$\tau$ 是温度系数，当 $\tau \rightarrow 0$ 的时候，类别之间的距离被拉得很远，softmax 的结果就会趋于一个 onehot，softmax 就能逼近 argmax；而 $\tau \rightarrow \infty$，不同类别的概率趋于一致，softmax 的结果趋于一个均匀分布。</p>
<p>因此作者提出，训练中前向传播，严格用 argmax 得到离散值，反向传播用 softmax 来求梯度。</p>
<p><strong>损失函数</strong></p>
<p>除了 multi-class logistics loss $L_{MC}$ 外，作者还想将计算量写入损失函数。定义一个 batch 的样本应该通过某一层的比率为 t (target rate)，$\overline{z}_l$ 代表实际中一个 batch 样本通过的样本比例，定义另外一个 loss 为，$N$ 为 batch size。</p>
<script type="math/tex; mode=display">L_{target} = \sum_1^N (\overline{z}_l - t)^2</script><script type="math/tex; mode=display">L_{Adanet} = L_{MC} + L_{target}</script><p>其中 $t = 0.6$，$\lambda = 2$</p>
<p><br></p>
<h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><p>作者先在 CIFAR-10 上做了实验，AdaNet 有一定的提升，且计算量减少了一点。</p>
<img src="/2018/12/16/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Convolutional-Networks-with-Adaptive-Computation-Graphs/3.png" class="">
<p>下图是不同层的通过率，downsampling layer 都有极高的通过率，Wide AdaNet 在后面的层，不同类别之间的通过率有明显的不同。</p>
<img src="/2018/12/16/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Convolutional-Networks-with-Adaptive-Computation-Graphs/4.png" class="">
<p>下面是 ImageNet 的结果，并没有变得很好，但是计算量少了不少。</p>
<img src="/2018/12/16/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Convolutional-Networks-with-Adaptive-Computation-Graphs/5.png" class="">
<p>下图表示前 30 个 epoch，通过率的变化，通过率初始化为 0.85，可以看出后面的层和 downsampling 层都比较快的达到通过率 100%，而其它层的通过率则不断下跌。</p>
<img src="/2018/12/16/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Convolutional-Networks-with-Adaptive-Computation-Graphs/6.png" class="">
<p>最后，作者还表示 AdaNet 可以防御对抗样本的攻击。</p>
<p><br></p>
]]></content>
  </entry>
  <entry>
    <title>【论文阅读】—— Deep Residual Learning for Image Recognition</title>
    <url>/2018/11/30/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Deep-Residual-Learning-for-Image-Recognition/</url>
    <content><![CDATA[<blockquote>
<p>Title：Deep Residual Learning for Image Recognition<br>Authors：Kaiming He et al<br>Session：CVPR, 2016<br>Abstract：这就是传说中的ResNet，作者提出了残差学习(residual learning)，解决了深层网络的退化问题(degradation)，并且破天荒地将网络层数加深到 152 层，并且揽获 ILSVRC 2015 的所有比赛冠军（分类，检测，定位）。</p>
</blockquote>
<span id="more"></span>
<p><br></p>
<p><em>preface</em></p>
<p>深度神经网络已经在前几年 ILSVRC 中展示了它的威力，像 VGG 和 GoogLeNet 都将网络层数提升到了 20 层左右，但是作者发现随着神经网络的加深，网络会出现退化问题(degradation problem)，即训练误差不降反升，从下图可以看出，56 层的网络训练误差比 20 层的网络还高，这不是由于过拟合所致，2015 年已经有很多防过拟合的方法出现，如 batch normalization，dropout，各种初始化方式；且如果是过拟合的问题，那么训练误差理应更低，但是从下图可以看出，深层的网络训练误差和测试误差都比浅层网络要高，所以退化问题不是过拟合所致。</p>
<img src="/2018/11/30/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Deep-Residual-Learning-for-Image-Recognition/degradation.png" class="">
<p><br></p>
<h1 id="Residual-Learning"><a href="#Residual-Learning" class="headerlink" title="Residual Learning"></a>Residual Learning</h1><p>网络的退化问题表明，不是所有的网络都是容易优化的，起码深层网络比浅层网络难以优化和收敛。</p>
<blockquote>
<p>Identity mapping(function)：恒等映射（函数）是指输出等于输入的映射，即 $y = x$。</p>
</blockquote>
<p>假如我们将上面例子的浅层网络（20层网络）通过在其后面叠加恒等函数（不是真的叠加，而是让后面的层去学习一个恒等函数，从而得到恒等函数的效果），我们就可以得到一个深层网络，理论上这个深层网络和浅层网络是等同效果的（如果能学习到恒等函数的话），深层网络可以看成是浅层网络的一个 counterpart，因此深层网络的错误率不应该比浅层网络要高。但是上面的实验也表明了，深层网络有退化问题，这个浅层网络的 counterpart 的错误率要比浅层网络本身要高。</p>
<p>因此作者提出猜想：<strong>或许非线性层比较难学习出一个恒等函数</strong>，因此连浅层网络的错误率都无法逼近，更不用说降低错误率了。</p>
<p>那么如何才能更加有效地学习恒等函数呢？作者提出了残差学习，只要能解决这个问题，就相当于能够解决网络的退化问题。</p>
<p>我们假设网络某几层非线性层（不一定要整个网络）需要学习的 mapping 为 $H(x)$，即潜在真实的 mapping 就为 $H(x)$。与其让这几层非线性层学习整个 $H(x)$，我们让它学习 $F(x)$，$F(x) = H(x) - x$，$F(x)$ 称为原映射 $H(x)$ 的残差；作者假设残差的学习要比原映射容易，一个极端的例子是：假设我们要学习的真实映射 $H(x)$ 就是一个恒等函数，即 $H(x) = x$，那么我们只需要学习让这几层非线性层输出 $F(x) = 0$ 即可；但是如果不是 residual learning，即上面提到的情况，让这几层非线性层直接学习 $g(x) = x$，这要比让这些非线性层学习 $g(x) = 0$ 要难，$g(x)$ 为这几层线性层学习到的映射。</p>
<p><br></p>
<h1 id="Why-Resnets-work"><a href="#Why-Resnets-work" class="headerlink" title="Why Resnets work?"></a>Why Resnets work?</h1><p>考虑一下下面的这个网络，输出为：</p>
<script type="math/tex; mode=display">a^{[l+2]} = Relu(z^{[l+2]} + a^{[l]}) = Relu(w^{[l+2]}a^{[l+1]} + b^{[l+2]} +a^{[l]})</script><p>假设由于正则或 weight decay 使得 $w^{[l+2]}a^{[l+1]} + b^{[l+2]} = 0$，那么 $a^{[l+2]} = a^{[l]}$，即一个恒等函数。</p>
<p>这表明 residual block 很容易会学到恒等函数，因此在下面的 big NN 后面添加这些 residual block，并不会 hurt 原来 NN 的能力，也就是说添加 residual block 至少不会使得性能下降，反而还会学到一些有用的信息使得性能上升。</p>
<p>而 plain network 当加深的时候，网络甚至连恒等函数都学不到，这也就是为什么网络加深，反而性能下降的原因。而 residual block 解决退化问题的最主要原因是，这些 residual block 能够保证不会 hurt performance，有时候还会 help performance。</p>
<img src="/2018/11/30/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Deep-Residual-Learning-for-Image-Recognition/7.png" class="">
<p>假设我们现在将网络的所有层都改为 residual block，即把上面例子的 Big NN 改成 residual block 的形式，首先这样改动，并不会改变这个 Big NN 的性能，因为假设几层非线性层能够逼近任意函数 $H(x)$，那么它肯定能够逼近 $H(x) - x$。因此基于 residual block 的 big NN 会学到和没有 residual block 的 big NN 一样的函数。在这基础上，根据上面的证明，在它后面加很多个 residual block，也不会 hurt performance，所以就可以解决退化问题了。</p>
<p><br></p>
<h1 id="Identity-Mapping-by-Shortcuts"><a href="#Identity-Mapping-by-Shortcuts" class="headerlink" title="Identity Mapping by Shortcuts"></a>Identity Mapping by Shortcuts</h1><p>作者基于上面的理论和猜想，提出了 residual block，如下图</p>
<img src="/2018/11/30/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Deep-Residual-Learning-for-Image-Recognition/residual_block.png" class="">
<p>作者称这种首尾相连的线为 shortcut 或者 skip-connection，它们是用来实现 Identity Mapping 的，一个 block 的方程如下，非线性层只需学习残差即可</p>
<script type="math/tex; mode=display">(1)：y = F(x, \{ W_i\}) +x</script><p>这种 shortcut 连接并没有引入任何的参数和很多的计算量（除了 element-wise 加法），但是却能解决退化问题。</p>
<p>当 $F(x, {  W_i})$ 的输出与 $x$ 的维度不匹配的时候，我们还需要将输入 $x$ 进行 projection。</p>
<script type="math/tex; mode=display">(2)：y = F(x, \{  W_i\}) +W_s x</script><p>公式 (1) 称为 Identity Mapping，而公式 (2) 称为 projection shortcut，一般上除非维度不匹配，否则都用 Identity Mapping 即可，没有太多必要引入额外的参数。</p>
<p><br></p>
<h1 id="ResNet结构"><a href="#ResNet结构" class="headerlink" title="ResNet结构"></a>ResNet结构</h1><p>下图是 VGG-19、plain network 以及 residual network 的网络图。plain network 和 residual network 都去掉了全连接层，而改用了 global pooling，residual network 中的实线连接为 Identity mapping，而虚线连接为 projection shortcut。</p>
<p>在具体实现上，projection shortcut 有两种选择</p>
<ol>
<li>直接通过 zero padding，使得输入和输出的维度相同。</li>
<li>通过上面的公式 (2) 用 $1\times 1$ 的卷积核将输入映射到输出。</li>
</ol>
<img src="/2018/11/30/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Deep-Residual-Learning-for-Image-Recognition/1.png" class="">
<p>作者共实现了层数分别为18、34、50、101 和 152 的 resnet，网络结构如下图，通过不断地堆叠 residual block，注意一点，卷积操作都是采取 SAME 卷积，feature map 维度减小是在每一个 conv i_x 的开头通过设置 stride = 2 将其缩小一半，具体看上图的虚线连接的 block。</p>
<p>从下表可以看出随着层数的增多，resnet 的计算量并没有提升很多，而参数也是要比 VGG 等网络要少。</p>
<img src="/2018/11/30/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Deep-Residual-Learning-for-Image-Recognition/2.png" class="">
<p>作者在构建 50 层以上的 resnet 的时候，采用了 GoogLeNet 一样的策略，用一个 $1 \times 1$ 的卷积核作为 bottleneck，来减少参数和计算量。</p>
<img src="/2018/11/30/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Deep-Residual-Learning-for-Image-Recognition/bottleneck.png" class="">
<p><br></p>
<h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><p><strong>实验设置：</strong></p>
<ul>
<li>训练阶段：预处理时像 VGG 一样随机采样 [256, 480] 的最短边，然后随机 crop 出 $224 \times 224$ 的图像以及水平翻转，然后做mean substracted。 </li>
<li>测试阶段：预测时候使用 AlexNet 的10-crop测试法，最好的结果是跟从 VGG 中的全卷积后的 multi-scale 评估，scale为 {224, 256, 384, 480, 640}。</li>
</ul>
<h2 id="Plain-Network-vs-Residual-Network"><a href="#Plain-Network-vs-Residual-Network" class="headerlink" title="Plain Network vs Residual Network"></a>Plain Network vs Residual Network</h2><ul>
<li>深层的 plain 网络的误差要比浅层的 plain 网络误差要高，表明出现退化问题。</li>
<li>作者分析了深层 plain 网络的前向激活和反向梯度值，都没有发生 vanish 的现象，证明不是梯度消失所导致。</li>
<li>residual network 为了和 plain network作对比，用的是 Identity mapping 和 zero padding for dismatch dimension，这是为了不引入额外的参数。</li>
<li>对比 plain network，resnet-34 的错误率要比 resnet-18 要低，意味着退化问题可以用 residual learning 解决。</li>
<li>当网络不是很深的时候，如 18 层，resnet 和 plain network 差不多，但是 resnet 收敛要更快。</li>
</ul>
<img src="/2018/11/30/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Deep-Residual-Learning-for-Image-Recognition/3.png" class="">
<img src="/2018/11/30/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Deep-Residual-Learning-for-Image-Recognition/4.png" class="">
<h2 id="Identity-Mapping-vs-Projection-Shortcuts"><a href="#Identity-Mapping-vs-Projection-Shortcuts" class="headerlink" title="Identity Mapping vs Projection Shortcuts"></a>Identity Mapping vs Projection Shortcuts</h2><p>skip-connection 的三种设置</p>
<ul>
<li>A. zero padding for increased dimension；parameter-free Identity mapping for other shortcuts.</li>
<li>B. projection shortcuts for increased dimension；parameter-free Identity mapping for other shortcuts.</li>
<li>C. all shortcuts are projection. （输入输出维度相同的也用一个 $1 \times 1$ 的卷积核去处理）</li>
</ul>
<p>结果表明 B 要比 A 稍微好一点，C 比 B 稍微好一点，但整体差不多，说明解决退化问题，projection shortcut 并不是必须。</p>
<img src="/2018/11/30/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Deep-Residual-Learning-for-Image-Recognition/5.png" class="">
<h2 id="Comparision-with-other-models"><a href="#Comparision-with-other-models" class="headerlink" title="Comparision with other models"></a>Comparision with other models</h2><ul>
<li>深度越深的 resnet 准确率越高，且没有出现退化问题。</li>
<li>实验表明，resnet 要比其他模型要好，且参数和计算量都要少。</li>
</ul>
<img src="/2018/11/30/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Deep-Residual-Learning-for-Image-Recognition/6.png" class="">
<p>作者还把 resnet 应用到了 CIFAR-10 分类上，且使用了 1000 层的resnet，错误率降低到 1% 以下，这表明 resnet 能够解决深层网络的退化问题。</p>
<p><br></p>
]]></content>
  </entry>
  <entry>
    <title>【论文阅读】—— Densely Connected Convolutional Networks</title>
    <url>/2018/12/04/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Densely-Connected-Convolutional-Networks/</url>
    <content><![CDATA[<blockquote>
<p>Title：Densely Connected Convolutional Networks<br>Author：Gao Huang, Zhuang Liu, et al.<br>Session：CVPR, 2017<br>Abstract：作者发现 ResNet 的结构有些许冗余，因此提出要将网络中的每一层都相互相连，而不是像 ResNet 那样只是 block 之间连接。作者提出的这种 DenseNet 结构参数量比 ResNet 要少，网络层数也比 ResNet 要少，但是效果却比 ResNet 要好。</p>
</blockquote>
<span id="more"></span>
<p><br></p>
<p><em>preface</em></p>
<p>作者黄高在 “Deep networks with stochastic depth” 论文中提出，ResNet 层数太多，大部分的残差块只能提高少量的信息，所以在 ResNet 基础上随机丢弃一些层，发现可以提高 ResNet 的泛化能力。 因此作者觉得网络中某一层的输入不仅仅只依赖于它上一层的输出，而依赖于更加上层的输出。由于随机丢弃某些层反而导致泛化能力变强，这意味着 ResNet 有明显的冗余，并不是每一层都需要，网络中的每一层只能提取很少量的特征。</p>
<p>基于上面的推断，作者提出<strong>让网络中的每一层和前面的所有层相连</strong>，同时把每一层设计的比较窄（即 feature map比较小），从而使每一层学到的特征变少来降低冗余。虽然网络每一层都相互连接，但是整体上网络的参数并没有比 ResNet 要多，因为每一层网络变简单了。除了参数变少外，DenseNet 的结构使得信号的前向传播和梯度的反向传递更加容易。</p>
<p><br></p>
<h1 id="DenseNet"><a href="#DenseNet" class="headerlink" title="DenseNet"></a>DenseNet</h1><p>ResNet 中每一个 residual block 可以看成以下的式子，$H_l()$ 表示第 $l$ 个 block 的 residual function，按照 ResNet-v2，应该是 BN-ReLu-Conv。</p>
<script type="math/tex; mode=display">x_l = H_l(x_{l-1}) + x_{l-1}</script><p>作者认为相加可能会阻碍信息的流动，因此作者提出的 DenseNet 不仅将前面层的输出作为后面层的输入，merge 方式也采取 concatenation 而不是 summation，因此 DenseNet 中某一层可以看成下面该式子</p>
<script type="math/tex; mode=display">x_l = H_l([x_0, x_1, ..., x_{l-1}])</script><p>上面的式子成立的前提是：$x<em>0, x_1,…,x</em>{l-1}$ feature map 的大小相同，然后在 channel 维度上拼接，有点像 Inception 中的 split-transform-merge 的思想。为了能够 concatenation，作者将网络分为两个 component：transition layer 和 dense block，如下图。</p>
<ul>
<li>transition layer：由一个 $1\times 1$ 的卷积层和一个 $2\times2$ 的 avg-pooling 层组成，pooling 层的 stride 为 2。transition layer 的作用在于降维，每次将 feature map 的大小减半。</li>
<li>dense block：由一系列的 BN-ReLu-Conv 层组成，每一个 Conv 的输出都是相同大小，前面提到 transition layer 的作用在于降维，因此在 dense block 中 feature map 的大小会维持不变，Conv 采用的是 SAME 卷积。</li>
</ul>
<img src="/2018/12/04/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Densely-Connected-Convolutional-Networks/2.png" class="">
<p><strong>Growth rate</strong>：dense block 中每一层的卷积层输出不仅 feature map 大小一样，channel 大小也是一样，作者把 channel 大小称为 growth rate $k$，因此第 $l$ 层的输入的 channel 大小为 $k_0 + k \times(l-1)$，而输出的 channel 大小依旧为 k，下图表示的是一个 dense block，有五层，k = 4</p>
<img src="/2018/12/04/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Densely-Connected-Convolutional-Networks/1.png" class="">
<p>作者的实验表明，k 不用很大效果就很好了。因为每一层都可以访问前面所有层的 feature map，整个网络相当于有一个 global state，每个层都添加 k 个 feature map 到这个状态里面；而 k 约束了添加信息的多少；这个 global state可以从网络的任何地方访问得到，而不像传统的网络一样，每一层都需要 replicate 一下。</p>
<p>作者实现了几个版本的 DenseNet</p>
<ul>
<li>DenseNet-A：dense block 的结构为 BN-ReLu-Conv(3 $\times$ 3)</li>
<li>DenseNet-B：dense block 的第 m 层的结构变为 BN-ReLu-Conv((m-1)k, 1 $\times$ 1, 4k)-BN-ReLu-Conv(4k, 3 $\times$ 3, k)，添加了一个而 bottleneck layer，bottleneck 的输出大小为 4k</li>
<li>DenseNet-C：transition layer 不仅可以将 feature map 的大小减半，还可以减少 channel 数量，作者把 $\theta$ 作为 compression factor，通过 transition layer 后，channel 的数量降为原来的 $\theta$，也就是说不断地减小 growth rate k，这种版本的 DenseNet 为 DenseNet-C。实验中 $\theta$ 取 0.5</li>
<li>DenseNet-BC：结合 bottleneck layer 和 compression factor</li>
</ul>
<p>网络结构如下：</p>
<img src="/2018/12/04/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Densely-Connected-Convolutional-Networks/3.png" class="">
<p><br></p>
<h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><p>作者分别在 CIFAR-10，CIFAR-100，SVHN 和 ImageNet 上进行实验；前三个数据集分别做了有数据增强和无数据增强的对比，无数据增强的作者用 dropout 来防止过拟合。结果如下，+ 号表示有数据增强，* 号表示自己跑的实验，粗体表示超过所有其它算法，蓝色表示最好的。</p>
<img src="/2018/12/04/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Densely-Connected-Convolutional-Networks/4.png" class="">
<p>主要对比的算法均为 ResNet 和它的变体，如 WRN 和黄高的那篇 “Deep networks with stochastic depth” 论文中的方法。</p>
<ul>
<li>DenseNet 的结果要比其它算法普遍要好</li>
<li>达到同等好的结果，参数少一点</li>
</ul>
<img src="/2018/12/04/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Densely-Connected-Convolutional-Networks/5.png" class="">
<p>下面对比的是参数里量和错误率的关系，可以看出 DenseNet-BC 最为 parameter-efficient，而对比 DenseNet 和 ResNet，DenseNet 只需要三分之一的参数量即可达到 ResNet 的效果，尽管 ResNet 对比 VGG 和 AlexNet 已经有很大的提升。</p>
<img src="/2018/12/04/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Densely-Connected-Convolutional-Networks/6.png" class="">
<p><br></p>
<p><em>In Conclusion</em></p>
<p>DenseNet 在某种程度上和 stochastic depth 相似，因为 stochastic depth 是随机的丢弃某些 residual block 使得某一层能和上几层或者后几层相连，这是随机的；而 DenseNet 将这个随机变为确定，让所有的层互相连接。</p>
<p><br></p>
]]></content>
  </entry>
  <entry>
    <title>【论文阅读】—— Distribution-Aware Coordinate Representation for Human Pose Estimation</title>
    <url>/2020/09/15/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Distribution-Aware-Coordinate-Representation-for-Human-Pose-Estimation/</url>
    <content><![CDATA[<blockquote>
<p>Title: Distribution-Aware Coordinate Representation for Human Pose Estimation<br>Authors: Feng Zhang, Xiatian Zhu, et al<br>Conferences: CVPR 2020<br>Abstract: 作者发现姿态检测任务中，除了网络结构的设计之外，label representation 也很重要，即如何在 keypoint coordinate 和 heatmap 之间相互转换，作者提出了一种新的转换方式，能无缝地与各种姿态检测网络相结合，并恒定地提升网络的性能。</p>
</blockquote>
<span id="more"></span>
<p><em>prefix</em></p>
<p>姿态检测任务不是一个简单的任务，它的难点在于关节点的遮挡、复杂的背景等，过去的很多工作都在设计更深更强的网络结构，而忽略了最本质的 label representation 的部分，姿态检测的 label 分为两类，一种是直接回归坐标，第二种是热力图回归，即先将坐标点转化成热力图上响应最大的点，然后网络输出一个热力图。第二种方法成为了现今姿态检测的主流方法，但回归热力图带来的问题就是计算开销大，所以常规的做法会先有 resolution reduction 后 resolution recovery，如下图，首先会将 bbox 下采样到一个 predefined 的分辨率，热力图也设置成一个预先定义好的分辨率，网络输出的热力图最后会上采样会原来的 image space。</p>
<p>作者把 coordinate 到 heatmap 的过程定义为 coordinate encoding，heatmap 到coordinate 到过程定义为 coordinate decoding，值得注意的是，encoding 的过程有 resolution reduction，所以会引入量化误差，为了解决这一问题，现在一般的做法是在 decoding 的时候，会有一个 0.25 像素的偏移来弥补这个误差，这个做法能够明显提升最后的结果。</p>
<img src="/2020/09/15/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Distribution-Aware-Coordinate-Representation-for-Human-Pose-Estimation/1.png" class="">
<p><br></p>
<h2 id="DARK"><a href="#DARK" class="headerlink" title="DARK"></a>DARK</h2><p>作者提出的 DARK 分为 encoding 和 decoding 部分</p>
<h3 id="Coordinate-Decoding"><a href="#Coordinate-Decoding" class="headerlink" title="Coordinate Decoding"></a>Coordinate Decoding</h3><p>标准的 coordinate decoding 做法是从 heatmap 中挑出响应值最大和次大的两个点 $m$ 和 $s$，最后的点会是最大点向次大点偏移 0.25 个像素，最后再恢复成原图像 space。这种 sub-pixel shifting 是用来补偿 resolution reduction 带来的量化误差，热力图上响应值最大的点未必对应原空间上最大的点，可能只是一个粗略的坐标。</p>
<script type="math/tex; mode=display">
p = m + 0.25 \frac{s-m}{||s-m||_2}</script><script type="math/tex; mode=display">
\hat{p} = \lambda p</script><p>这种做法是 hand-crafted 的，并且没有太多的理论依据，作者提出一种基于分布的 decoding 方法，来获得更加准确的 sub-pixel，首先假设输出的 heatmap 同样是服从二维高斯分布</p>
<script type="math/tex; mode=display">
G(x;\mu,\Sigma)=\frac{1}{2\pi|\Sigma|^{1/2}}exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))</script><p>取 log 后得</p>
<script type="math/tex; mode=display">
P(x;\mu,\Sigma)=ln(G)=-ln(2\pi)-\frac{1}{2}ln(|\Sigma|)-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)</script><p>二元高斯分布的 $\mu$ 是中心，同样也是响应值最高的地方，因此对其求一阶导数会等于 0。</p>
<script type="math/tex; mode=display">
D^\prime(x)|_{x=\mu}=-\Sigma^{-1}(x-\mu)|_{x=\mu}=0</script><p>但由于实际上得到的 heatmap 是离散值，假设最大响应点为 $m$，我们用泰勒展开，用 $m$ 点去估计真正最大的点 $\mu$</p>
<script type="math/tex; mode=display">
P(\mu)=P(m)+D^\prime(m)(\mu-m)+\frac{1}{2}(\mu-m)^TD^{\prime \prime}(m)(\mu-m)</script><p>其中二阶导数（Hessian）为</p>
<script type="math/tex; mode=display">
D^{\prime \prime}(m)=D^{\prime \prime}(x)|_{x=m}=-\Sigma^{-1}</script><p>联合几条式子最后得到最大响应值 $\mu$ 为</p>
<script type="math/tex; mode=display">
\mu = m - (D^{\prime \prime}(m))^{-1}D^{\prime}(m)</script><p>对比现在的标准做法人为像第二大的点偏移 0.25 个像素，作者提出的做法充分利用了热力图的分布信息，但该方法是基于网络输出的分布是高斯分布的前提，但实际上输出的热力图不是完全的高斯分布，如下图，输出的热力图会出现在关节点附近好几个峰值。</p>
<img src="/2020/09/15/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Distribution-Aware-Coordinate-Representation-for-Human-Pose-Estimation/2.png" class="">
<p>因此作者在这里对输出的热力图做了一个平滑（modulation）操作来 smooth out 其他的峰值，具体的操作是用一个高斯核 K 来进行卷积，然后再调整热力图的峰值为原热力图的峰值</p>
<script type="math/tex; mode=display">
h^\prime = K \otimes h</script><script type="math/tex; mode=display">
h^\prime = \frac{h^\prime - min(h^\prime)}{max(h^\prime) - min(h^\prime)} \ast max(h)</script><p>总结一下，作者提出的 decoding 分为三步</p>
<ol>
<li>heatmap distribution modulation</li>
<li>Distribution-aware joint localisation by Taylor expansion</li>
<li>resolution recovery to the original coordinate space</li>
</ol>
<img src="/2020/09/15/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Distribution-Aware-Coordinate-Representation-for-Human-Pose-Estimation/3.png" class="">
<h3 id="Coordinate-Encoding"><a href="#Coordinate-Encoding" class="headerlink" title="Coordinate Encoding"></a>Coordinate Encoding</h3><p>假设真实的坐标点为 $g=(u,v)$，resolution reduction 后的坐标点为</p>
<script type="math/tex; mode=display">
g^\prime=(u^\prime, v^\prime) = \frac{g}{\lambda}=(\frac{u}{\lambda}, \frac{v}{\lambda})</script><p>然后会将 $g^\prime$ 进行量化，比如 floor、ceil、round 等操作得到 $g^{\prime\prime}$，现在 encoding 的方法就是在 $g^{\prime\prime}$ 上进行 GT heatmap 的生成，但这肯定会引入一个量化误差在里面，作者提出用 $g^\prime$ 去生成 heatmap 能够更加准确一点。</p>
<p><br></p>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="Evaluating-Coordinate-Representation"><a href="#Evaluating-Coordinate-Representation" class="headerlink" title="Evaluating Coordinate Representation"></a>Evaluating Coordinate Representation</h3><h4 id="Coordinate-decoding"><a href="#Coordinate-decoding" class="headerlink" title="Coordinate decoding"></a>Coordinate decoding</h4><p>作者对比了 0.25 hand-crafted shift 和 DARK，用的网络是 HRNet-w32，输入维度是 128x96，结果表明以前的那种 0.25 shift 的方法能够极大提升性能，但竟然没有被研究</p>
<img src="/2020/09/15/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Distribution-Aware-Coordinate-Representation-for-Human-Pose-Estimation/4.png" class="">
<img src="/2020/09/15/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Distribution-Aware-Coordinate-Representation-for-Human-Pose-Estimation/5.png" class="">
<h4 id="Coordinate-encoding"><a href="#Coordinate-encoding" class="headerlink" title="Coordinate encoding"></a>Coordinate encoding</h4><p>Unbiased encoding 能恒定提升性能，无论 decoding 方式如何</p>
<img src="/2020/09/15/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Distribution-Aware-Coordinate-Representation-for-Human-Pose-Estimation/6.png" class="">
<h4 id="input-resolution"><a href="#input-resolution" class="headerlink" title="input resolution"></a>input resolution</h4><p>降低分辨率性能掉点严重，但 DARK 在分辨率低的情况下提点比较多</p>
<img src="/2020/09/15/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Distribution-Aware-Coordinate-Representation-for-Human-Pose-Estimation/7.png" class="">
<h4 id="generality"><a href="#generality" class="headerlink" title="generality"></a>generality</h4><p>测试了不同的网络，都能有恒定的提点</p>
<img src="/2020/09/15/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Distribution-Aware-Coordinate-Representation-for-Human-Pose-Estimation/8.png" class="">
<img src="/2020/09/15/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Distribution-Aware-Coordinate-Representation-for-Human-Pose-Estimation/9.png" class="">
<img src="/2020/09/15/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Distribution-Aware-Coordinate-Representation-for-Human-Pose-Estimation/10.png" class="">
<p><br></p>
]]></content>
  </entry>
  <entry>
    <title>【论文阅读】—— Dual Path Network</title>
    <url>/2018/12/11/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Dual-Path-Network/</url>
    <content><![CDATA[<blockquote>
<p>Title：Dual Path Network<br>Authors：颜水成团队<br>Session：NIPS, 2017<br>Abstract：近几年，ResNet 和 DenseNet 都在视觉任务上取得了比较好的成绩，作者结合两者的优点，提出一种双通道的网络 (dual path network, DPN)，并取得了 ILSVRC-2017  目标定位的冠军。</p>
</blockquote>
<span id="more"></span>
<p><br></p>
<p><em>preface</em></p>
<p>ResNe t和 DenseNet是近几年两种比较热门的网络结构，ResNet 把输入直接加到（element-wise adding）卷积的输出上，DenseNet则把每一层的输出都拼接（concatenate）到其后每一层的输入上。在这篇论文中作者用High Order RNN 结构（HORNN）把 DenseNet 和 ResNet 联系到了一起，证明了 DenseNet 能从靠前的层级中提取到新的特征，而 ResNet 本质上是对之前层级中已提取特征的复用。通过把这两种结构的优点结合到一起，提出了 Dual Path Networks（DPN）。</p>
<p><br></p>
<h1 id="Revisiting-ResNet-DenseNet-and-Higher-Order-RNN"><a href="#Revisiting-ResNet-DenseNet-and-Higher-Order-RNN" class="headerlink" title="Revisiting ResNet, DenseNet and Higher Order RNN"></a>Revisiting ResNet, DenseNet and Higher Order RNN</h1><h2 id="Higher-Order-RNN-HORNN"><a href="#Higher-Order-RNN-HORNN" class="headerlink" title="Higher Order RNN(HORNN)"></a>Higher Order RNN(HORNN)</h2><p>在普通的 RNN 结构中，一个时刻的输入会与上一时刻的状态共同决定这一时刻的状态，而高阶 RNN 中，一个时刻的状态不仅与上一个时刻的状态有关，还和前面所有时刻的状态都有关，每个时刻的计算公式如下，$h^k$ 表示第 $k$ 个时刻的状态，也是当前状态，$f^k<em>t(h^t)$ 表示计算从 $h^t$ 传到 $h^k$ 的信息，$\sum</em>{t=0}^{k-1}f_t^k(h^t)$ 表示前 $k-1$ 个状态传给第 $k$ 个状态的信息和，$g^k(\cdot)$ 将汇聚起来的信息进行处理得到第 $k$ 个时刻的状态。</p>
<script type="math/tex; mode=display">h^k = g^k[\sum_{t=0}^{k-1}f_t^k(h^t)]</script><p>HORNN 一个重要的特点是，对于任意的 $k$ 和 $t$，都有：</p>
<ul>
<li>$g^k(\cdot) = g(\cdot)$，即每个时刻的状态计算函数是相同的，就像普通的 RNN 一样，不同时刻用同一个隐层。</li>
<li>$f^k_t(\cdot) = f_t(\cdot)$，即每个状态对后面时刻状态传递的信息是一样的，不会随着不同时刻而改变。</li>
</ul>
<h2 id="用-HORNN-去解释-DenseNet"><a href="#用-HORNN-去解释-DenseNet" class="headerlink" title="用 HORNN 去解释 DenseNet"></a>用 HORNN 去解释 DenseNet</h2><p>DenseNet 每一层的输入是前面所有层输出 concate 的结果，concate 之后进入一个 bottleneck layer，即 $1 \times 1$ 的卷积层，然后经过一个 $3 \times 3$ 的卷积层，得到该层的输出。</p>
<p>假设 DenseNet 的 growth rate 为 c，每一层输出的 feature map 大小为 $h \times w$，那么第 $k$ 层的输入大小为 $h \times w \times (k-1)c$，$f^k(\cdot)$ 为 $1 \times 1 \times (k-1)c$ 的卷积层，由于是一个 $1 \times 1$ 的卷积，就相当于 feature map 的每个像素点 $x$ (一个长度为 $(k-1)c$ 的向量) 和卷积核 $w$ 进行向量內积，$w$ 和 $x$ 都是一个 $(k-1)c$ 维的向量，我们不妨将这两个向量分成 $k-1$ 份，$x = [x<em>1, x_2, …, x</em>{k-1}]$，$w = [w<em>1, w_2, …, w</em>{k-1}]$，每一份单独做內积再相加等同于整体內积，即 $w^Tx = \sum w_i^T x_i$。</p>
<p>由上面的推论，我们可以得知，concat 后作 $1 \times 1$ 的卷积，等同于前面 $k-1$ 个状态单独做一个 $1 \times 1$ 的卷积后相加，而 $f_t^k(\cdot)$ 等于 $f^k$ 对应的 $1 \times 1$ 的卷积核在 channel 上的第 $t$ 组，如 $f_1^k(\cdot)$ 就等于卷积核的 $[1,1, 0 : c]$ 这部分。</p>
<img src="/2018/12/11/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Dual-Path-Network/1.png" class="">
<p>从这可以看出，DenseNet 的形式完全符合 HORNN 的形式，$f_t^k(\cdot)$ 表示第 $k$ 层第 $t$ 组 channel，$g^k(\cdot)$ 表示 $3 \times 3$ 的卷积层。需要注意的是，DenseNet 不满足 HORNN 的一个条件，即  $f^k_t(\cdot) = f_t(\cdot)$ 和 $g^k(\cdot) = g(\cdot)$，因为 DenseNet 每一层的 $1 \times 1$ 卷积层和 $3 \times 3$ 卷积层都不同。</p>
<img src="/2018/12/11/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Dual-Path-Network/2.png" class="">
<p>下图从通道的角度去理解 DenseNet，左边表示 channel，即不同层 concat 的结果，或者不同时刻状态 concat 的结果，一个灰色的圆圈代表一个时刻的状态或一层的输出（虽然第一个圈和第二个圈对应的 channel 的宽度不成比例，但是我们将其理解为一个状态），我们把目光放到第二个时刻，这时候 channel 中有两层的输出，我们将其单独提取出来，每一个状态（每一层的输出）经过一个 $1 \times 1$ 的卷积层后进行相加，这里的 $1 \times 1$ 的卷积层就是 $f^k_t(\cdot)$，而一个灰圈代表 $h^t$。按照上面的图，相加后的结果应该直接经过 $g^k(\cdot)$ 函数，即 $3 \times 3$ 的卷积层，但是这里作者另外加了一个 $1 \times 1$ 的卷积层（带下划线），目的是为了和 ResNet 匹配过来，没有其他用途，$3 \times 3$ 卷积核的输出作为这一时刻的状态，concat 到左边的 channel 中，下一层从左边取出来的就是 3 个时刻的状态，即 3 个灰圈。</p>
<img src="/2018/12/11/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Dual-Path-Network/4.png" class="">
<h2 id="用-DenseNet-去解释-ResNet"><a href="#用-DenseNet-去解释-ResNet" class="headerlink" title="用 DenseNet 去解释 ResNet"></a>用 DenseNet 去解释 ResNet</h2><p>我们知道，DenseNet 可以写成 HORNN 的形式。</p>
<script type="math/tex; mode=display">h^k = g^k[\sum_{t=0}^{k-1}f_t^k(h^t)]</script><p>将上式子进行一些变量替换，并且假设 $f^k_t(\cdot) = f_t(\cdot)$，令</p>
<script type="math/tex; mode=display">r^k = \sum_{t=0}^{k-1}f_t^k(h^t) = \sum_{t=0}^{k-1}f^k(h^t) = r^{k-1} + f_{k-1}(h^{k-1})</script><p>而 $h^k = g^k(r^k)$，所以</p>
<script type="math/tex; mode=display">r^k = r^{k-1} + f_{k-1}(g^{k-1}(h^{k-1})) = r^{k-1} + \phi^{k-1}(r^{k-1})</script><p>上式就是 ResNet 的结构，$\phi^{k-1}(\cdot) = f_{k-1}(g^{k-1}(\cdot))$ 为 residual function。由上面的推导可以得出，ResNet 是 DenseNet 在 $f^k_t(\cdot) = f_t(\cdot)$ 情况下的特殊表达式，下图上半部分是我们熟知的 ResNet 结构，一个 block 由一个 $3 \times 3$ 的卷积层和 $ 1\times 1$ 的卷积层组成，每个 block 的状态为 $r^k$，状态转移是 ResNet 的方式；下半部分是其等效图，一个 block 的组成成分变为 $1 \times 1$ 和 $3 \times 3$，每个 block 的状态为 $h^k$，状态的转移需要先将前面 $k-1$ 个状态经过一个 $f_t(\cdot)$ 函数然后相加后，经过 $g^k(\cdot)$ 得到该时刻的状态 $h^k$。</p>
<img src="/2018/12/11/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Dual-Path-Network/3.png" class="">
<p>上下两幅图对于 $g^k(\cdot)$ 有一点出入，下图是按照论文来的</p>
<img src="/2018/12/11/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Dual-Path-Network/7.png" class="">
<p>下图是 ResNet 的图，左边的矩形代表 channel，灰圈代表 $r^k$，$r^k$ 经过一系列卷积后和 $r^k$ 相加得到 $r^{k+1}$，这幅图并没有从 HORNN 的角度去解释 ResNet（即上图的下半部分）。</p>
<img src="/2018/12/11/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Dual-Path-Network/5.png" class="">
<p>上面我们证明了 ResNet 是 DenseNet 在 $f^k<em>t(\cdot) = f_t(\cdot)$ 下的特殊表达式，下图证明了这一点，(b) 中每一时刻的 $1 \times 1$ 卷积都不同，即绿色和橙色的箭头在每个时刻都不同。而 (c) 将 (b) 的形式 reformulate 了一下，将加法写成了一个独立的主分支，由于不同时刻的 $f_t(\cdot)$ 都一样。那么对于 k 时刻，$f_1(h^1) + … + f</em>{k-1}(h^{k-1})$，而对于 k+1 时刻，$f<em>1(h^1) + … + f</em>{k-1}(h^{k-1}) + f_k(h^k)$，因此只需要每次往右边的主干上加入 $f_k(h^k)$ 即可，这也就是下面的 (c) 图，$\underline{1 \times 1}$ 和 $3 \times 3$ 表示 $g^k(\cdot)$，经过 $g^k(\cdot)$ 得到 $h^k$，然后经过一个 $1\times1$ 的卷积得到 $f_k(h^k)$（橙色箭头）加入到右边。绿色和橙色的箭头只会出现在residual block中一次，也就意味着被加到了主分支之后，再也不会改变了。这也就是 with shared connections 的含义。  </p>
<img src="/2018/12/11/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Dual-Path-Network/6.png" class="">
<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>从上面的分析可以看出，由于 ResNet 中 $f^k_t = f_t$，所以后面的层在复用前面层已经提取过的特征，除去这些直连的复用特征外，真正由卷积提取出来的特征“纯度”就比较高了，基本都是之前没有提取到过的全新特征，所以作者说 ResNet 提取的特征中冗余度比较低，且鼓励 feature reuse。而 DenseNet 每一层的 $f^k_t$ 都是不相同的，前面层提取出的特征不再是被后面层简单的复用，而是创造了全新的特征，这种结构后面层用卷积提取到的特征很有可能是前面层已提取过的，所以作者说 DenseNet 提取的特征冗余度高，但是鼓励探索新特征。</p>
<p>ResNet 有高的特征复用率，但冗余度低，DenseNet 能创造新特征，但是冗余度高，作者提出把两种结构结合起来，提出 dual path network。</p>
<p><br></p>
<h1 id="Dual-Path-Network"><a href="#Dual-Path-Network" class="headerlink" title="Dual Path Network"></a>Dual Path Network</h1><p>DPN 顾名思义就是有两条通道的网络，而这两条通道分别是 DenSeNet 和 ResNet</p>
<script type="math/tex; mode=display">x^k = \sum_{t=1}^{k-1}f_t^k(h^t)</script><script type="math/tex; mode=display">y^k = \sum_{t=1}^{k-1}v_t(h^t) = y^{k-1} + \phi^{k-1}(y^{k-1})</script><script type="math/tex; mode=display">r^k = x^k + y^k</script><script type="math/tex; mode=display">h^k = g^k(r^k)</script><p>下图是 DPN 的结构，(e) 图是将 (d) 图的两个主干合并的结果，卷积后的结果被切分成两部分，一部分加到 ResNet 的通道里面，一部分 concat 到 DenseNet 中。(d) 图和上面的公式有点不符，ResNet 通道出来的 $1 \times 1$ 卷积不知道怎样解释。</p>
<img src="/2018/12/11/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Dual-Path-Network/8.png" class="">
<p>整体的网络结构如下，DPN 和 DenseNet 和 ResNeXt 结构不同之处在于每个 block 的设计</p>
<ul>
<li>$3 \times 3$ 的卷积层采用的是 group convolution</li>
<li>$1 \times 1 \times 256(+16)$ 中的 256 代表的是 ResNet 的通道数，16 代表的是 DenseNet 一层的输出通道数，将结果分成 256 和 16 两部分，256 的 element-wise 的加到 ResNet 通道，16 的 concat 到 DenseNet 通道，然后继续下一个 block，同样输出 256 + 16 个通道，重复操作。</li>
<li>在 conv 之间过渡的时候，如 conv2 输出到 conv3 输入，需要经过一个下采样，即 feature map 大小减半，通道数翻倍，具体实现上是将上一个 conv 的输出经过一个 stride 为 2 的 $1 \times 1 $ 的卷积层，将通道数翻倍。而在 conv2 中 max-pooling 层过渡到 DPN block 的时候，也是用这种方式，经过一个 $1 \times 1$ 的卷积层，将通道数调整为 256 + 16$\times$2，然后将通道分成两份 256 和 16$\times$2，至于为什么 DenseNet 的通道数需要初始化为 2$\times$+k，作者没有说明。</li>
<li>一点细节：每个 conv 中的 $3 \times 3$ 卷积层的输出 channel 数为 ResNet 在这一层的 channel 数，+k 则为该层 DenseNet 的 channel 数。</li>
</ul>
<p>具体实现的细节，可以参考：<a href="https://github.com/cypw/DPNs/tree/master/settings">https://github.com/cypw/DPNs/tree/master/settings</a>。 </p>
<img src="/2018/12/11/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Dual-Path-Network/9.png" class="">
<p><br></p>
<h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><p>作者在 classification、object detection 和 semantic segmentation 三个任务中验证了 DPN 的有效性，这里只讲基于 ImageNet 的分类任务</p>
<ul>
<li>先看第一个 block，DPN-92 在 top-1 error 中最好，比 ResNeXt 要低 0.5%，且计算量要低。</li>
<li>再看第二个 block，深一点的 DPN-98 还是要比 ResNeXt 要好，计算量也少了 25%。</li>
<li>第三个 block，继续加深 DPN 到 131 层，效果依旧明显。</li>
</ul>
<img src="/2018/12/11/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Dual-Path-Network/10.png" class="">
<p><br></p>
<p><em>In Conclusion</em></p>
<p>作者从 HORNN 的角度去理解 ResNet 和 DenseNet，并发现 ResNet 是 feature reuse，而 DenseNet 是探索新特征，基于这两个 motivation，提出 DPN，实验结果也证明了其在多项任务上的有效性。</p>
<p><br></p>
<p><em>References</em></p>
<ol>
<li><a href="https://zhuanlan.zhihu.com/p/42906045">https://zhuanlan.zhihu.com/p/42906045</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/34993147">https://zhuanlan.zhihu.com/p/34993147</a></li>
<li><a href="https://github.com/cypw/DPNs/tree/master/settings">https://github.com/cypw/DPNs/tree/master/settings</a></li>
<li><a href="https://www.jianshu.com/p/c13a9900d643">https://www.jianshu.com/p/c13a9900d643</a></li>
</ol>
<p><br></p>
]]></content>
  </entry>
  <entry>
    <title>【论文阅读】—— Explaining and Harnessing Adversarial Examples</title>
    <url>/2018/11/26/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Explaining-and-Harnessing-Adversarial-Examples/</url>
    <content><![CDATA[<blockquote>
<p>Title：Explaining and Harnessing Adversarial Examples<br>link：<a href="https://arxiv.org/abs/1412.6572">https://arxiv.org/abs/1412.6572</a><br>Abstract：该文章简单介绍对抗样本、对抗样本存在的原因，以及如何生成对抗样本进行对抗攻击——FGSM算法。</p>
</blockquote>
<span id="more"></span>
<p><br></p>
<h1 id="What-is-adversarial-example"><a href="#What-is-adversarial-example" class="headerlink" title="What is adversarial example?"></a>What is adversarial example?</h1><p>深度学习的发展使得机器在很多任务上都达到了接近人类的准确率，但是 Szegedy 等人在 “Intriguing properties of neural network” 中首次发现了这些深度网络的flaw并提出了对抗样本，对抗样本指的是将一些特定的扰动（perturbation）加到clean example上，使得模型对这些样本的分类结果发生错误，需要注意的是，这些扰动通常来说非常地小，即人类无法察觉，从下面的例子中就可以看出对抗样本是如何得到的。将一些扰动加到原图像上，分类器就会马上分类错误。</p>
<img src="/2018/11/26/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Explaining-and-Harnessing-Adversarial-Examples/1.png" class="">
<p><br></p>
<h1 id="Why-do-adversarial-examples-exist"><a href="#Why-do-adversarial-examples-exist" class="headerlink" title="Why do adversarial examples exist?"></a>Why do adversarial examples exist?</h1><ul>
<li><p>过拟合导致？</p>
<ul>
<li>对抗样本刚被发现的时候，研究者都认为是模型参数过多，模型过度拟合训练集所致。</li>
<li>如果对抗样本真的是过拟合导致，那么对抗样本的出现就不会是unique的，当我们重新训练模型，应该会随机出现其它的分类错误的“对抗样本”。但是实际上发现不同的模型会对同一个对抗样本错误分类（这种叫做对抗样本的transferability），并且如果把对抗样本和原来的样本作差，然后将差加到任何的一个clean example上，这个样本马上会变成对抗样本。</li>
<li>所以，这些发现也表明了对抗样本不可能是过拟合所致。</li>
</ul>
</li>
<li><p>模型的线性性导致！</p>
<ul>
<li><p>Ian Goodfellow 在2014年发表了 “explaining and harnessing adversarial example” 这篇文章，并说明对抗样本的存在是由于模型过于线性所致。</p>
</li>
<li><p>由于样本的输入特征的精度有限，一般的图像的每个像素都是8bits，所以样本中低于1/255的信息都会被丢弃，当样本 $x$ 添加一定的扰动值 $\eta$，$\eta$ 满足 $||\eta||_{\infty} &lt; \epsilon$，即最大的扰动量小于 $\epsilon$。考虑一个线性模型，线性模型的特点在于 dot product，即 $score = w^T x$。由于 $\tilde{x} = x + \eta$，则 $score = w^Tx + w^T\eta$，可以看出当样本改变 $\eta$ 时，activation值就会改变 $w^T\eta$。</p>
</li>
<li><p>Ian Goodfellow 认为令 $\eta = sign(w)$，可以使得 $w^T\eta$ 最大化，假设 $w$ 有 $n$ 个维度，且平均值为 $m$，那么activation的增量最多为 $\epsilon mn$。由于 $||\eta||_{\infty}$ 不会随着维度 $n$ 的变化而变化，但是增量却会随着 $n$ 增大而线性增长，因此对于一个高维度的问题，一个样本中即使是无限小的扰动，也会叠加对最后的activation产生很大的影响</p>
</li>
<li><p>神经网络似乎自诞生以来就和非线性挂钩，激活函数非线性，那么为什么深度网络还是会有对抗样本呢？Ian Goodfellow 认为现在绝大多数的激活函数都是 piecewise linear，Relu函数是分段线性的，sigmoid 和 tanh在我们care的部分也近似线性。</p>
</li>
<li><p>下图表明线性模型导致对抗样本的出现，在clean example的领域内找到某一个点，它跨过了决策面被分类错误，但如果是非线性的决策面，这种情况就不会发生。</p>
<img src="/2018/11/26/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Explaining-and-Harnessing-Adversarial-Examples/2.png" class="">
</li>
<li><p>而也有猜想说，对抗样本的存在是因为决策面距离样本点太近所致。</p>
</li>
</ul>
</li>
</ul>
<p><br></p>
<h1 id="Fast-Gradient-Sign-Method-FGSM"><a href="#Fast-Gradient-Sign-Method-FGSM" class="headerlink" title="Fast Gradient Sign Method(FGSM)"></a>Fast Gradient Sign Method(FGSM)</h1><p>Ian Goodfellow 根据模型的线性性，提出一种产生对抗样本的方法，通过计算模型对输入的梯度的符号作为输入的扰动值。</p>
<script type="math/tex; mode=display">\eta = \epsilon sign(\bigtriangledown _xJ(\theta, x, y))</script><script type="math/tex; mode=display">x = x + \eta</script><p>我们平常用的梯度下降方法是通过求损失函数对于模型参数的梯度，进而更新参数使得模型的损失降低。而 FGSM 方法求模型损失函数对于输入的梯度，然后用<strong>梯度上升</strong>的方法，使得模型的损失增大。模型的损失增大意味着模型对于该输入样本的分类置信度降低，或者直接分类错误，从而达到对抗样本的目的。</p>
<p>Note：我们可以随意地将输入样本往任一错误类别上改变。比如输入图片是猫，我们可以求损失函数对于狗的梯度，然后将它作为 reference direction来更新猫的图片，就可以将猫往狗的方向改变。</p>
<p><br></p>
<h1 id="Some-Intuitions"><a href="#Some-Intuitions" class="headerlink" title="Some Intuitions"></a>Some Intuitions</h1><p><strong>Are adversarial examples rare?</strong></p>
<p>Ian Goodfellow认为对抗样本的存在并非偶然，并非像有理数分布到实数上的这种程度，而是对抗样本更像是存在于某一个对抗子空间</p>
<p><strong>Are adversarial examples off the manifold</strong></p>
<p>manifold 简单解释即数据存在的区域，通常我们说 manifold是高维空间里面的一个低维空间，比如公路是三维空间里面的一维空间；</p>
<img src="/2018/11/26/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Explaining-and-Harnessing-Adversarial-Examples/3.png" class="" width="500" height="500">
<p>而对抗样本不属于数据的manifold，可以理解为非自然点，即不在正常数据的分布内。有人理解对抗样本的存在是因为训练数据不够大没有覆盖到对抗样本，才导致模型欠拟合出现分类错误，但实际上对抗样本不属于正常数据，即使我们的训练数据有无穷多，也不可能包含对抗样本，因为对抗样本不在正常数据的分布内。</p>
<p><br></p>
<h1 id="对抗训练"><a href="#对抗训练" class="headerlink" title="对抗训练"></a>对抗训练</h1><p>这是基于 Ian Goodfellow 论文中提到的方法</p>
<h2 id="线性模型的对抗训练"><a href="#线性模型的对抗训练" class="headerlink" title="线性模型的对抗训练"></a>线性模型的对抗训练</h2><p>考虑最简单的逻辑回归模型，假设训练一个逻辑回归模型来进行分类，分类的标签 $y \in { -1, +1}$，预测函数 $P(y = 1) = \sigma(w^T x + b)$，损失函数为 $J(x,y) = \zeta(-y(w^Tx + b))$，其中 $\zeta(x) = log(1 + exp(z))$ 为softplus函数。</p>
<p>其实这个损失函数就是我们一般的cross-entropy loss，经过一系列变形成上面的形式。</p>
<script type="math/tex; mode=display">Likelihood(x,y) = \sigma(\hat{y})^{\frac{y+1}{2}}(1-\sigma(\hat{y}))^{1-\frac{y+1}{2}}</script><p>其中 $\hat{y} = w^Tx + b$  </p>
<script type="math/tex; mode=display">Loss(x,y) = -log\ Likelihood(x,y) = \begin{cases} log(1 + exp(-\hat{y})) & y = 1 \\ log(1 + exp(\hat{y})) & y = -1 \end{cases}</script><p><br></p>
<script type="math/tex; mode=display">\zeta(-y\hat{y}) = \begin{cases} log(1 + exp(-\hat{y})) & y = 1 \\ log(1 + exp(\hat{y})) & y = -1 \end{cases}</script><p><br><br>所以，$Loss(x,y) = \zeta(-y(w^Tx + b))$ </p>
<p>对该模型使用FGSM方法，求得扰动量为</p>
<script type="math/tex; mode=display">\eta = \epsilon sign(\bigtriangledown _x \zeta(y(w^Tx + b))) = \epsilon sign(-w) = -\epsilon sign(w)</script><p>因此对抗样本的Loss为</p>
<script type="math/tex; mode=display">\zeta(-y(w^T(x + \eta) + b)) = \zeta(y(\epsilon ||w||_1 - w^Tx -b))</script><p>其中 $w^T \eta = -\epsilon ||w||_1$</p>
<p>对于对抗样本的Loss，加入了模型的权重，就好比L1正则，区别在于L1正则权重是加到整体损失函数之后，而上面的式子权重是加到activation。</p>
<p>当模型能够做出置信度很高的预测的时候，即 $w^Tx + b$ 很大的时候，上式的正则效果会消失；而当预测的置信度很小的时候，会使得欠拟合更加严重，因为上式的主导项变成了权重项。</p>
<h2 id="深度网络的对抗训练"><a href="#深度网络的对抗训练" class="headerlink" title="深度网络的对抗训练"></a>深度网络的对抗训练</h2><p>Ian Goodfellow同时提出了一种基于深度网络的对抗训练方法，同样是基于FGSM算法</p>
<script type="math/tex; mode=display">\tilde{J}(\theta, x, y) = \alpha J(\theta, x, y) + (1-\alpha)J(\theta, x + \epsilon sign(\bigtriangledown_xJ(\theta,x,y)))</script><p>这种对抗训练的方法意味着在训练过程中不断更新对抗样本，从而使得当前模型可以抵御对抗样本。 </p>
<p>文章表明，在不进行对抗训练的情况下，模型识别FGSM攻击方法生成样本的错误率是89.4%，但是通过对抗训练，同样的模型识别对抗样本的错误率下降到17.9%。 </p>
<p><br></p>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul>
<li><a href="https://towardsdatascience.com/know-your-adversary-understanding-adversarial-examples-part-1-2-63af4c2f5830">Know Your Adversary: Understanding Adversarial Examples</a></li>
<li><a href="https://towardsdatascience.com/the-modeler-strikes-back-defense-strategies-against-adversarial-attacks-9aae07b93d00">The Modeler Strikes Back: Defense Strategies Against Adversarial Attacks </a></li>
<li>非常好的一片文章：<a href="http://karpathy.github.io/2015/03/30/breaking-convnets/">Breaking Linear Classifiers on ImageNet</a></li>
<li><a href="https://www.youtube.com/watch?v=CIfsB_EYsVI">CS231, Lecture 16 | Adversarial Examples and Adversarial Training</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/32784766">某人的论文笔记</a></li>
<li><a href="https://blog.csdn.net/cdpac/article/details/53170940">某博文</a></li>
</ul>
<p><br></p>
]]></content>
  </entry>
  <entry>
    <title>【论文阅读】—— FractalNet, Ultra-deep Neural Networks without Residuals</title>
    <url>/2018/12/11/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-FractalNet-Ultra-deep-Neural-Networks-without-Residuals/</url>
    <content><![CDATA[<blockquote>
<p>Title：FRACTALNET: ULTRA-DEEP NEURAL NETWORKS WITHOUT RESIDUALS<br>Authors：Gustav Larsson, Michael Maire, Gregory Shakhnarovich<br>Abstract：作者提出了一个分形网络 (fractalNet)，并且认为 ResNet 中的残差计算并不是深层网络必须的，可以通过一种分形的结构，也可以达到很好的效果。</p>
</blockquote>
<span id="more"></span>
<p><br></p>
<p><em>preface</em></p>
<p>自从 ResNet 提出以来，倍受人们的追捧，在 ResNet 基础上延伸出来的模型结构越来越多，如：Wide ResNet、stochastic Depth ResNet 等等，都取得了非常好的效果，而 Residual 的结构也被认为对于构建极深网络非常重要的结构。而本文，提出了一种新的分形网络结构，它的提出就是为了证明 Residual 对于极深网络并不是必须的，它通过一种分形的结构，达到了类似于教师-学生机制、深度监督的效果，同样取得了非常好的效果。虽然这种方法在目前来说用的比较少，但是我觉得它的这种思路是非常值得学习的。</p>
<h1 id="FractalNet"><a href="#FractalNet" class="headerlink" title="FractalNet"></a>FractalNet</h1><p>整个分形网络的架构图如下，先从最右边看起，网络中包含 5 个 block，每个 block 之间用池化层相连，最后一个全连接层进行预测，然后我们细看每个 block 的结构，初看有一种递归或迭代生成的结构，左上角给出了 fractal 展开的法则，$f_1(\cdot)$ 表示一个最基本的层，可以是卷积层，也可以是自己定义的一些结构，从 $f_1(\cdot)$ 拓展到 $f_2(\cdot)$ 需要将输入分成两条路径，一条路径上为卷积层，另一条路径上为两个依次连接的 $f_1(\cdot)$，然后将两条路径上的信号 join 起来。依此类推 $f_3(\cdot)$ 和 $f_4(\cdot)$，下图中的 $f_4(\cdot)$ 做了一些小小的改动，从 fractal expansion rule 看，join 操作的输入只能是两条路径，比如 A 和 B join 的结果为 C，C 和 D 再做 join，作者修改其为 A、B 和 C 一起 join，因此 join 操作的输入不限定为 2 个。</p>
<p>从 fractal expansion rule 看，每拓展一个 level，网络的层数就 double，所以对于 $f_C(\cdot)$，其网络结构有 $2^{C-1}$ 层。在 FractalNet 中 join 操作，不像之前的工作那样采取求和或拼接，因为 join 的输入路径数量不固定，因此作者将输入信号进行取均值处理，从这可以看出，一个 block 中每一个卷积操作应该都是有相同的卷积核大小和卷积核个数的，并且在 block 中不改变 feature map 的大小，不然无法进行均值化。</p>
<p>分形网络不像 ResNet 那样连一条捷径，而是通过不同长度的子路径组合，其结构也有点像 Inception，将输入分成不同路径，但是 FractalNet 每条路径长度又不完全相同。在思路上，有点像 DenseNet 和 Stochastic Depth ResNet，跨越不同层将信号相连，如果把下面的 $f_4(\cdot)$ 看成是 8 层的网络，那么最左边的路径就是跨越 8 层的连接。</p>
<img src="/2018/12/11/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-FractalNet-Ultra-deep-Neural-Networks-without-Residuals/1.png" class="">
<p>同时，作者受到 Dropout 的启发，提出了 drop path，即训练过程中随机丢弃一些路径，drop path 是 dropout（防止co-adaption）的天然扩展，是一种正则方法，可以防止过拟合，提升模型表现。drop path 分为两种类型：local 和 global。local 指 join 操作会随机丢弃一些输入，且保证至少会有一个输入；global 指从整个网络中随机选择一条路径来进行训练，这样做会使得网络中任意的一条路径都能作为一个分类器使用。drop path 的好处有几点</p>
<ol>
<li>减少过拟合</li>
<li>强化每条路径的输出</li>
<li>是作者重点提出，也是我认为非常值得学习的一点，那就是不同路径的联合，在 drop path 的机制下，起到了一种简单的教师—学生的学习方式。也就是说，如果某一条路径学到了对最终分类起到非常重要的特征时，假如在某一次迭代中，该路径被关闭掉了，那么通过 loss 的反向传播中，可能就会指导和该路径进行联合的另一条路径也学到这种重要的特征。那么通过模型内部的这种不断的教师—学生的学习方式，不仅可以提高整个模型的效果，并且当提取出其中的任意一条路径出来单独使用时，也能够达到非常好的效果。作者在后面也证明了，对于 plain network，在深度达到 40 层以上时会出现退化的问题，但是在 FractalNet 中完全不会，从整个模型中提出其中最深的路径来单独使用时可以达到和整个 FractalNet 接近的效果。</li>
</ol>
<img src="/2018/12/11/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-FractalNet-Ultra-deep-Neural-Networks-without-Residuals/2.png" class="">
<p><br></p>
<h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><p>实验使用的数据集包括：CIFAR-10，CIFAR-100，SVHN，ImageNet。对于前三个数据集，输入图片大小为 $32 \times 32$，FractalNet 有 5 个 block，每个 block 内每个卷积核的输出 channel 数分别为 (64, 128, 256, 512, 512)，block 之间采取 non-overlapping 的池化层，遵循 feature map 减半的同时，channel 数量翻倍。对于 ImageNet 数据集，对比 ResNet-34，作者保留 ResNet-34 的头尾两层，将中间层改为 4 个 fractalnet block，每个 block 内卷积核的输出 channel 数分别为 (128, 256, 512, 1024)。</p>
<p>实验主要对比 ResNet，结果如下</p>
<ul>
<li><p>+表示使用了水平镜像翻转和平移，++表示使用了更多的数据增强</p>
</li>
<li><p>20 层的 FractalNet 超过了最原始的 ResNet</p>
</li>
<li>有数据增强的话，CIFAR-100 上 FractalNet 跟最好的 ResNet 变种接近</li>
<li>没有数据增强的话，FractalNet 比 ResNet 和 Stochastic Depth 要好，证明 FractalNet 的训练模式更不容易过拟合。</li>
<li>drop path 是有用的，能够防止过拟合，在 CIFAR-100 上，加了 drop path，错误率从 35% 降到 28%</li>
<li>当取出最深的一条路径时，和整个 FractalNet 的表现接近</li>
</ul>
<img src="/2018/12/11/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-FractalNet-Ultra-deep-Neural-Networks-without-Residuals/4.png" class="">
<p>比较 FractalNet 和 Plain Network</p>
<ul>
<li>fractalNet 到了160层开始出现退化</li>
<li>plain network 到了40层就出现了退化，到了160层不能收敛 </li>
<li>使用了drop-path的 fractalNet 提取的 path 比平常的网络取得了更优的表现，而且克服了退化问题。<img src="/2018/12/11/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-FractalNet-Ultra-deep-Neural-Networks-without-Residuals/4.png" class="">
<img src="/2018/12/11/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-FractalNet-Ultra-deep-Neural-Networks-without-Residuals/5.png" class="">
</li>
</ul>
<p><br></p>
<p><em>In Conclusion</em></p>
<ol>
<li>论文的实验说明了路径长度才是训练深度网络的需要的基本组件，而不单单是残差块</li>
<li>分形网络和残差网络都有很大的网络深度，但是在训练的时候都具有更短的有效的梯度传播路径</li>
<li>分形网络简化了对这种需求（更短的有效的梯度传播路径）的满足，可以防止网络过深</li>
<li>多余的深度可能会减慢训练速度，但不会损害准确性</li>
</ol>
<p><br></p>
]]></content>
  </entry>
  <entry>
    <title>【论文阅读】—— GaterNet, Dynamic Filter Selection in Convolutional Neural Network via a Dedicated Global Gating Network</title>
    <url>/2018/12/13/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-GaterNet-Dynamic-Filter-Selection-in-Convolutional-Neural-Network-via-a-Dedicated-Global-Gating-Network/</url>
    <content><![CDATA[<blockquote>
<p>Title：GaterNet: Dynamic Filter Selection in Convolutional Neural Network via a Dedicated Global Gating Network<br>Authors：Zhourong Chen, Yang Li, Samy Bengio, Si Si<br>Abstract：conditional computation 是指用网络的一部分层去预测结果，而不是整个网络。这样可以使网络中不同的卷积核能够从不同的样本中学习到特征，一定程度上提高了模型的泛化性能和可解释性。作者提出了一个根据输入样本动态选择卷积核的算法，模型包含两个网络：gater network 和 backbone network，gater network 决定样本通过 backbone network 时需要经过哪些卷积核。</p>
</blockquote>
<span id="more"></span>
<p><br></p>
<p><em>preface</em></p>
<p>conditional computation 是指：对于每一个样本，只用模型的一小部分去进行预测，那么在反向传播的时候，也是只有一小部分的参数需要被更新，这对于训练一个庞大的网络是非常有好处的。</p>
<p>conditional computation 相关的研究主要分为两个方面：Mixture of Experts (MoE) 和 dynamic network configuraion。MoE 通过集成多个子网络来进行预测，子网络的权重由一个 gating module 决定，最近有些研究提出只集成一小部分的子网络，这些子网络是根据不同的样本动态选择的；dynamic network configuration 只有一个主模型，但是会动态选择模型的单元、层数或者其他的 component，通常一些小的 module 会加到每一个网络的 component 中来决定该部分是否需要被选择，这种方法作者称为 local configuration，因为这些小 module 是根据这个 component 当前的信号值来决定是否被选择的，典型的代表是最近 EECV 的一篇论文 《Convolutional networks<br>with adaptive inference graphs》。</p>
<p>作者提出了一种 global configuration 的方式，通过一个 gater network 对输入进行全局探索，最后得出哪些卷积核需要被使用。</p>
<p><br></p>
<h1 id="GaterNet"><a href="#GaterNet" class="headerlink" title="GaterNet"></a>GaterNet</h1><p>GaterNet 分为两个部分：gater network 和 backbone network，给定一个输入样本，gater network 决定 backbone network 使用哪些卷积核进行预测，两个网络用端到端的方式通过反向传播进行训练。</p>
<img src="/2018/12/13/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-GaterNet-Dynamic-Filter-Selection-in-Convolutional-Neural-Network-via-a-Dedicated-Global-Gating-Network/1.png" class="">
<h2 id="Backbone-network"><a href="#Backbone-network" class="headerlink" title="Backbone network"></a>Backbone network</h2><p>backbone network 是模型主要的部分，用来预测最终的结果，任何的 CNN 结构都可以作为 backbone network，如 ResNet、Inception、DenseNet 等等。给定一个输入 $x$，第 $l$ 层的输出为 $O^l(x)$ ：</p>
<script type="math/tex; mode=display">O^l_i(x) = \phi(F^l_i \ast I^l(x))</script><p>$O^l_i(x)$ 是输出 $O^l(x)$ 的第 $i$ 个 channel，$F^l_i(x)$ 是第 $i$ 个卷积核，$I^l(x)$ 是第 $l$ 层的输入，$\ast$ 代表卷积，$\phi(\cdot)$ 代表非线性激活函数。一般情况下，如果没有 gater network，则所有的卷积核 $F_i^l$ 都会被使用，然后得到一个 dense 的输出 $O^l(x)$，有了 gater network，模型会选择一部分的卷积核来进行卷积，而不是全部卷积核。</p>
<h2 id="Gater-network"><a href="#Gater-network" class="headerlink" title="Gater network"></a>Gater network</h2><p>对比于 backbone network，gater network 是辅助性作用，并不会直接学习一些特征用于预测。而是通过处理输入图像，然后生成一个二进制的向量 (gating mask)，这个向量用来动态选择一部分的 backbone network 的卷积核用来预测，gater network 的方程如下：</p>
<script type="math/tex; mode=display">G(x) = D(E(x))</script><p>$E(\cdot)$ 是一个图像特征提取器，$h^{\prime},w^{\prime},c^{\prime}$ 表示输入图像的宽高和通道数，$h$ 表示提取特征的维数。</p>
<script type="math/tex; mode=display">E: x \rightarrow f, x \in R^{h^{\prime} \times w^{\prime} \times c^{\prime}}, f \in R^h</script><p>$D(\cdot)$ 将特征变换到二进制向量，$c$ 表示 backbone network 中卷积核的总数。</p>
<script type="math/tex; mode=display">D: f \rightarrow g, f \in R^h, g \in \{0,1\}^c</script><p>综上，gater network 将一个输入图像 $x$ 映射到一个二进制向量 $g$，有了 gating network，backbone network 中第 $l$ 层的输出重新定义为：</p>
<script type="math/tex; mode=display">O^l_i(x)=\left\{\begin{array}{cc}  zero \ matrix, & g^l_i=0\\  \phi(F^l_i \ast I^l(x)) & g^l_i = 1  \end{array}\right.</script><p>$g^l_i$ 是 $g$ 向量中对应第 $l$ 层第 $i$ 个卷积核的元素，可以看出如果 $g^l_i$ 为 0，第 $l$ 层第 $i$ 个卷积核被跳过，用 0 去代替卷积的输出。当第 $l$ 层有很多的卷积核被跳过时，输出就是一个稀疏的图像。具体实现上，将上式合并成以下的形式：</p>
<script type="math/tex; mode=display">O^l_i(x) = \phi(F^l_i \ast I^l(x)) \cdot g^l_i</script><p>接下来将讨论何如选择函数 $E(\cdot)$ 和 $D(\cdot)$</p>
<h3 id="Feature-Extractor"><a href="#Feature-Extractor" class="headerlink" title="Feature Extractor"></a>Feature Extractor</h3><p>$E(\cdot)$ 是一个特征提取器，任意的 CNN 都可以用来作为 $E(\cdot)$，对比 backbone network，$E(\cdot)$ 有两点不同：</p>
<ol>
<li>输出层被去除，使得 $E(\cdot)$ 输出的是特征，然后被 $D(\cdot)$ 使用</li>
<li>$E(\cdot)$ 不需要像 backbone network 那样复杂的 CNN，$E(\cdot)$ 只需要输出输入图像的一个简单的概括即可，太复杂的 $E(\cdot)$ 会带来过多的参数和计算量，且作者想避免 $E(\cdot)$ 喧宾夺主，毕竟 backbone network 才是真正用来预测的网络。</li>
</ol>
<h3 id="Features-to-Binary-Gates"><a href="#Features-to-Binary-Gates" class="headerlink" title="Features to Binary Gates"></a>Features to Binary Gates</h3><p>$D(\cdot)$ 函数将一个 $h$ 维的向量 $f$ 映射到一个 $c$ 维的二进制向量 $g$，首先考虑用一层全连接网络进行映射，那么这一层网络的参数就有 $h \times c$（忽略 bias），由于 $c$ 代表 backbone network 中卷积核的总数，往往是几千个，而 $h$ 表示特征的个数，通常也不少，因此这一层的参数量和计算量都不少。为了减少计算量，作者用两层全连接层，第一层使用一个 bottleneck 进行降维到 $b$，因此参数量改为 $(h+c)\times b$。</p>
<script type="math/tex; mode=display">f^{\prime} = FC_1(f)</script><script type="math/tex; mode=display">g^{\prime} = FC_2(ReLU(BatchNorm(f^{\prime})))</script><p>$g^{\prime}$ 向量是一个实向量，作者通过一个离散化的方法将其转成一个二进制向量，该方法为 Improved SemHash (改进的语意哈希)。</p>
<p>在训练过程中，将 0 均值和单位标准差的 $c$ 维高斯噪声 $\epsilon$ 加到 $g^{\prime}$，即 $g^{\prime}_{\epsilon} = g^{\prime} + \epsilon$，计算两个向量</p>
<script type="math/tex; mode=display">g_{\alpha} = \sigma^{\prime}(g^{\prime}_{\epsilon})</script><script type="math/tex; mode=display">g_{\beta} = 1(g^{\prime}_{\epsilon} > 0)</script><p>$\sigma^{\prime}(\cdot)$ 是饱和 sigmoid 函数</p>
<script type="math/tex; mode=display">\sigma^{\prime}(x) = max(0, min(1, 1.2\sigma(x) - 0.1))</script><p>$\sigma(x)$ 是 sigmoid 函数，$g<em>\alpha$ 是一个实向量，范围在 0 - 1，可微分；而 $g</em>\beta$ 是一个二进制向量，不可微分。因此作者在前向传播的过程中，随机抽取一半的数据用 $g = g<em>\alpha$，另一半的数据用 $g = g</em>\beta$，当用 $g<em>\beta$ 的时候，用 $g</em>\alpha$ 的梯度去代替 $g_\beta$ 的梯度。</p>
<img src="/2018/12/13/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-GaterNet-Dynamic-Filter-Selection-in-Convolutional-Neural-Network-via-a-Dedicated-Global-Gating-Network/2.png" class="">
<p>在预测阶段，不需要引入高斯噪声，且全部适用 $g = g_\beta$。 </p>
<p>最后，作者希望 gater network 得到的 gates (二进制向量) 越稀疏越好，因此将 gater network 作为正则项</p>
<script type="math/tex; mode=display">L = -logP(y | x, \theta) + \lambda \frac{||G(x)||_1}{c}</script><p><br></p>
<h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><p>作者先在 ResNet 上验证 GaterNet 的有效性，gater network 选用 ResNet-20，backbone network 选用 ResNet-20, ResNet-56, ResNet-164。结果如下，ResNet-Wider 指添加一些卷积核使得参数量和 GaterNet 接近的 ResNet，Gated Filter 指 backbone network 中卷积核的个数。</p>
<ul>
<li>GaterNet 比所有的原始 ResNet 要好，在  CIFAR-100 上，GaterNet 比 ResNet-164 的错误率要低了 1.83%。</li>
<li>GaterNet 同样要比 ResNet-SE 要好，GaterNet 产生二进制的 gate 去选择卷积核，而 ResNet-SE 是 re-scale 通道值，尽管 GaterNet 在前向传递时会有信息损失，因为只用了部分的卷积核，但是模型却有更好的泛化性能，也证明了处理一个样本只需要部分的卷积核，不需要全部。</li>
<li>ResNet-Wider 要比 ResNet 要好，这是显然的，ResNet-20-Wider 甚至要比 GaterNet 要好，这也比较容易理解，因为 gater network 是用 ResNet-20，GaterNet 有一半的参数都在做 gating strategy，而不是直接参与到预测中。但是 GaterNet 依旧和 ResNet-20-Wider 接近。</li>
<li>而对于 ResNet-50 和 ResNet-164，增加一些卷积核变成 ResNet-Wider 并不会增加多少准确率，而 GaterNet 却能明显提高准确率，这也说明了 GaterNet 的有效性，不止在参数量的增多，更在于 gating strategy。</li>
<li>gater network 不需要很复杂，ResNet-20 就已经足够了，当 backbone network 越复杂的时候，gater network 所带来的参数量就越微不足道。</li>
</ul>
<img src="/2018/12/13/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-GaterNet-Dynamic-Filter-Selection-in-Convolutional-Neural-Network-via-a-Dedicated-Global-Gating-Network/3.png" class="">
<p><br></p>
<p><strong>Gate Distribution</strong></p>
<p>这里的 gate 指的是一个卷积核是否被卷积，作者想探究一下 gater network 是否有用，想知道对于每个样本，学习到的二进制向量会是怎样的。作者用  ResNet-164-Gated 这个网络，把 CIFAR-10 的测试集通过 gater network 得到每个测试样本的二进制向量。一个 gate 代表一个卷积核，自然而然 gate 会有三种类型：1）Always On：对于所有样本都是 ON，2）Always Off：对于所有样本都是 OFF，3）Input-dependent gate：或 ON 或 OFF。ResNet-164 共有 54 个 residual unit，每个 unit 中会有一定数量的门，下图表示每个 unit 中三种类型的门的分布情况。</p>
<ul>
<li>在浅层的网络，很大一部分的卷积核都是 OFF 的；</li>
<li>而随着网络加深，Always On 和 Input-dependent 增多。</li>
<li>在最后的两个 unit，Input-dependent 占最大的比例。</li>
</ul>
<p>这和我们的一般认识相符合，浅层网络提取 low-level 的特征，深层网络提取 high-level 特征。</p>
<img src="/2018/12/13/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-GaterNet-Dynamic-Filter-Selection-in-Convolutional-Neural-Network-via-a-Dedicated-Global-Gating-Network/4.png" class="">
<p>作者认为 Input-dependent 这个定义不够精细，对于 CIFAR-10 中 10000 个测试样例，如果对于某一个卷积核，它只卷积了 1 个样本，skip 了 9999 个样本，它也是属于 Input-dependent 类型的。作者想知道每个 Input-dependent 类型的门，有多少个样本通过了这个门。ResNet-164 共有 7200 个卷积核，即 7200 个门，其中有 1567 个 Input-dependent 的门，在这 1567 个门中，有 443 个门几乎是全通过或全拒绝，即上面说到的情况。剩下 1124 个门的通过样本数量在 100 - 9900 之间。</p>
<p>下图的 x 轴表示一个 input-dependent 的门通过样本的个数，y 轴表示 input-dependent 门的个数，整体表示的意识是：通过样本数为 x 的门有 y 个。下图的两边表示的就是那 443 个门，几乎全部通过或几乎全部拒绝。如果将下图进行求和 (积分)，结果必定为 1567。</p>
<img src="/2018/12/13/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-GaterNet-Dynamic-Filter-Selection-in-Convolutional-Neural-Network-via-a-Dedicated-Global-Gating-Network/5.png" class="">
<p>上面的实验都是基于一个门来进行的，探索了每个门通过的样本数，下面坐着探索每个样本通过的门数，经过实验得到，通过的门数最小值和最大值分别是 5380 和 5506，平均通过门数为 5453，通过门数似乎服从正态分布。</p>
<img src="/2018/12/13/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-GaterNet-Dynamic-Filter-Selection-in-Convolutional-Neural-Network-via-a-Dedicated-Global-Gating-Network/6.png" class="">
<p>最后，作者想探究 gater network 究竟学到了什么 gating strategy，因此它用每个样本通过 gater network 得到的二进制向量 (7200维) 去代表每个样本，然后用 PCA 将其降维到 400，再用 t-SNE 映射到二维空间，如下图，可以看出，相同 label 的二进制向量都比较靠近，因此 gater network 学习到：对于相同 label 的样本，它会试着让它通过相似的门，也就是说用 backbone network 的相同部分去处理这些 label 相同的样本。另外，下图的簇并不是区分地很好，这也说明了 gater network 并没有占据 backbone network 的地位。</p>
<img src="/2018/12/13/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-GaterNet-Dynamic-Filter-Selection-in-Convolutional-Neural-Network-via-a-Dedicated-Global-Gating-Network/7.png" class="">
<p><strong>CIFAR-10 上的结果</strong></p>
<img src="/2018/12/13/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-GaterNet-Dynamic-Filter-Selection-in-Convolutional-Neural-Network-via-a-Dedicated-Global-Gating-Network/8.png" class="">
<p><strong>ImageNet 上的结果</strong></p>
<img src="/2018/12/13/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-GaterNet-Dynamic-Filter-Selection-in-Convolutional-Neural-Network-via-a-Dedicated-Global-Gating-Network/9.png" class="">
<p><br></p>
]]></content>
  </entry>
  <entry>
    <title>【论文阅读】—— Going deeper with convolutions</title>
    <url>/2018/11/29/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Going-deeper-with-convolutions/</url>
    <content><![CDATA[<blockquote>
<p>Title：Going deeper with convolutions<br>Authors：Christian Szegedy et al<br>Session：CVPR, 2015<br>Abstract：这就是传说中的 GoogLeNet（Inception-v1），获得了 ILSVRC 2014 分类任务的冠军。该网络设计了 Inception module 代替人工选择卷积核，通过堆叠 Inception module 得到 GoogLeNet。</p>
</blockquote>
<span id="more"></span>
<p><br></p>
<p><em>preface</em></p>
<p>作者设计 GoogLeNet 的一大动机是想减少网络的参数和计算量，从而应用到实际应用中，但是为了提高模型的性能，网络深度和宽度必须要得以保证；而模型的增大会导致参数的增多，既增大了计算量，又容易使得模型陷入过拟合。而作者设计的 Inception module 参数少且宽，因此通过堆叠 Inception module 既能提高模型的性能，又能减少模型的参数。Inception-v1 虽然有 22 层，但是参数却只有 5 million，相对于 VGG-16 的 138 million 和 AlexNet 的 60 million，有了很大的提升。</p>
<p>一点需要注意的是：GoogLeNet 并非 GoogleNet，虽然作者来自于 Google，GoogLeNet 取名是为了致敬 LeNet；而 Inception 的取名来自于电影 Inception（盗梦空间）里面的一句经典台词 “we need to go deeper”，意味着我们需要搭建更深的网络。</p>
<p><br></p>
<h1 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h1><p>GoogLeNet 有以下几个特点</p>
<ul>
<li><p>将全连接层去掉，用 global pooling 的方式来做最后的 flatten，之前的论文参数过多的一大重要原因就是全连接层的使用，因此作者将其去除，既能减少参数，性能又没有被太多的破坏。</p>
</li>
<li><p>所有的卷积的操作都是 SAME 卷积，即不改变 feature map 的大小，只改变 channel 的数量，而降维留给了 max-pooling。</p>
</li>
<li><p>堆叠 Inception module（如下图），Inception module 省去了选择卷积核的烦恼，作者统一使用三种卷积核 $1 \times 1$、$3 \times 3$ 和 $5\times 5$，并且在每个 Inception module 中加入了 pooling。为了减少卷积核的参数和计算量，作者在 $3 \times 3$ 和 $5\times 5$ 卷积核前面加入 $1 \times 1$ 的卷积核，这样做有两个优点</p>
<ul>
<li>减少计算量和参数量，假如图像维度为 $28 \times 28 \times 192$，用 128 个 $3 \times 3$ 的卷积核去卷积，参数量为 $3 \times 3 \times 192 \times 128 \approx 200k$，而如果先用 96 个 $1 \times 1$ 的卷积核再用 128 个 $3 \times 3$ 的卷积核会得到相同维度的结果，但是参数量则为 $1 \times 1 \times 192 \times 96 + 3 \times 3 \times 96 \times 128 \approx 130k$，所以参数量减少了很多。</li>
<li>两层卷积层能带来更多的 non-linearity。</li>
</ul>
<p>Inception module 的 max-pooling 后同样接了 $1\times1$ 的卷积核，进而减少 channel 数量，因为 pooling 操作是不会改变 channel 数量的。</p>
<img src="/2018/11/29/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Going-deeper-with-convolutions/1.png" class="">
</li>
</ul>
<p>GoogLeNet 总共有 22 层（只算带参数的层），网络结构如下，输入为 $224 \times 224 \times 3$ 的 RGB 图像，SAME 卷积，#$ 3 \times 3 \ reduce$ 指的是 $3 \times 3$ 前面的 $1 \times 1$ 卷积核的个数。</p>
<img src="/2018/11/29/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Going-deeper-with-convolutions/2.png" class="">
<p><strong>注意：</strong> GoogLeNet 中作者认为网络太深可能会影响梯度的反向传递，因此设置了两个辅助输出，分别在 Inception(4b) 和 Inception(4e) 后 接入一个小网络输出结果，因此整个网络会有 3 个 loss，这样可以增加梯度的回传，还有附加的正则化作用。</p>
<h1 id="测试细节"><a href="#测试细节" class="headerlink" title="测试细节"></a>测试细节</h1><p>用的是 multi-crop 的方法，和 AlexNet 一样，但是 GoogLeNet 将这种方法做到了极致。</p>
<ul>
<li>首先将原图像 rescale 到 4 种尺度，最小边分别为 256、288、320 和 352。此时得到 4 种 crop。</li>
<li>然后沿着最小边从左中右三个方位裁剪出 3 张正方形图像。即假设上一步的图像为 $N \times 288$，然后就左中右的裁剪出 3 个 $288 \times 288$ 的图像。此时得到 $4 \times 3$ 种 crop。</li>
<li>对于上面的每一张图，再从 4 个 corner 和中间裁剪出 $224 \times 224$ 的图，外加一张 rescale 到 $224 \times 224$ 的图，比如上一步得到的是 $288 \times 288$，将整张图片缩小到 $224 \times 224$；再将这 6 张图片进行水平镜像。此时得到 $4 \times 3 \times 6 \times 2 = 144$ 种 crop。</li>
</ul>
<p>这种方法相当于 scale jittering，不过做得非常极致</p>
<p><br><br><em>In Conclusion</em></p>
<p>GoogLeNet 首次将网络从深度和宽度上进行了探索，并且尝试不同的卷积 block，为后面的研究奠定了一些基调。</p>
<p><br></p>
]]></content>
  </entry>
  <entry>
    <title>【论文阅读】—— Identity Mappings in Deep Residual Networks</title>
    <url>/2018/12/01/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Identity-Mappings-in-Deep-Residual-Networks/</url>
    <content><![CDATA[<blockquote>
<p>Title：Identity Mappings in Deep Residual Networks<br>Authors：Kaiming He et al<br>Session：ECCV, 2016<br>Abstract：作者在 ResNet-v1 基础上进行了修改，使得表现得到了提升，该网络称为 ResNet-v2.</p>
</blockquote>
<span id="more"></span>
<p><br></p>
<h1 id="ResNet-Analysis"><a href="#ResNet-Analysis" class="headerlink" title="ResNet Analysis"></a>ResNet Analysis</h1><p>ResNet 可以看成是由一个个的 residual unit 堆叠而成，每一个 unit 可以看成下面的两个计算。$W_l$ 看成是多个卷积核，在 ResNet-v1 里面，$h(x_l) = x_l$ 是 identity mapping，$f$ 函数是 ReLu。</p>
<script type="math/tex; mode=display">y_l = h(x_l) + F(x_l, W_l)</script><script type="math/tex; mode=display">x_{l+1} = f(y_l)</script><p>假设 $h$ 和 $f$ 都是 identity mapping，那么上面两个公式可以写成下面的形式。</p>
<script type="math/tex; mode=display">x_{l+1} = x_l + F(x_l, W_l)</script><p>所以某一个 unit 的输出可以由前面某一 unit 的值，加上各个unit 的函数值。</p>
<script type="math/tex; mode=display">x_L = x_l + \sum_l^{L-1} F(x_i, W_i)</script><p>这里可以看出 residual network 和 plain network 一个很大的不同，resnet 的输出是每一个 unit 的 $F(x, W)$ 的相加；而 plain network 的输出是一系列矩阵的相乘（忽略 ReLu 和 Batch Normalization）。</p>
<p>因此损失函数 $\epsilon$ 对于某一个 unit 的梯度就能写成下面的形式。梯度可以分解为两项，一项是直接从 loss 不经过任何 weight 传到 $x_l$，而另外一项是经过 weight 来传播；另外一点是，括号里不可能为 0，因为后一项不可能总是 -1。</p>
<script type="math/tex; mode=display">\frac{\partial \epsilon}{\partial x_l} = \frac{\partial \epsilon}{\partial x_L}\frac{\partial x_l}{\partial x_l} = \frac{\partial \epsilon}{\partial x_L}(1 + \frac{\partial}{\partial x_l}\sum_l^{L-1}F(x_i, W_i))</script><p><strong>注意：</strong> 在resnet-v1，h 函数不一定等于恒等函数，当出现维度不匹配的时候，就不是恒等函数，但是作者认为 projection 在整个 100 多层的 resnet 中也只出现了几个，其他的都是 identity mapping，因此 projection 不会产生太多的影响，所以默认 h 等于恒等函数，来进行后面的推导。</p>
<p>从上面的推导可以看出，信号可以直接从一个 unit 传播到另外一个 unit，不管是前向传递还是反向传播。而这基于两个条件：(1) h 函数是恒等函数，(2) f 函数也是恒等函数。</p>
<p>下面将会尝试不同的 h 和 f 函数，进而观察网络性能的变化。</p>
<p><br></p>
<h1 id="Choice-for-h-function"><a href="#Choice-for-h-function" class="headerlink" title="Choice for $h$ function"></a>Choice for $h$ function</h1><p>作者共提出了 6 种 h 函数（包括原始的 identity mapping），如下图。为了简单起见，省去了 BN 层和激活函数。下面在做测试的时候，固定了 f 函数是 ReLu 激活。</p>
<img src="/2018/12/01/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Identity-Mappings-in-Deep-Residual-Networks/h.png" class="">
<p>结果如下：</p>
<ul>
<li>constant scaling：这相比于 identity mapping 多了一个乘法算子 $\lambda$，因此在前向和反向传递的时候，这个 $\lambda$ 都会累乘，呈指数型增长或下降。因此结果很差。</li>
<li>exclusive gating 和 shortcut-only gating：类似于门电路，效果也一般。</li>
<li>conv shortcut：类似于 resnet-v1 的 projection shortcut 的 C 策略，效果之所以没有 resnet-v1 的好，是因为现在网络深度更深了。</li>
<li>dropout：效果也不好。</li>
</ul>
<img src="/2018/12/01/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Identity-Mappings-in-Deep-Residual-Networks/result_h.png" class="">
<p>综上所述：original 的效果最好，接下来也会直接用 original，相当于直接的 identity mapping。</p>
<p><br></p>
<h1 id="Choice-for-f-function"><a href="#Choice-for-f-function" class="headerlink" title="Choice for $f$ function"></a>Choice for $f$ function</h1><p>在第一小节中提到，当 $f$ 函数是 identity mapping 的时候，信号可以在前向传播和反向传递中直接体现；现在我们需要将 $f$ 函数变成 identity mapping，就需要将 ReLu 和 BN 进行重新排布。如下图</p>
<img src="/2018/12/01/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Identity-Mappings-in-Deep-Residual-Networks/f.png" class="">
<ul>
<li>original：$f$ 函数就是 ReLu 激活函数。</li>
<li>BN after addition：$f$ 函数相当于 BN 加 ReLu。</li>
<li>ReLu before addition：将 original 版本的 ReLu 函数放到了 $F(x,W)$ 里面，这时 $f$ 函数就完完全全变成一个恒等函数了。但是这样存在的问题是：$F(x,W)$ 的输出是非负的，而理论上 $F(x,W)$ 的输出应该是全实数。因此前向传递的信号是单调上升的，因此也影响了整个网络的性能。</li>
<li>后面的两个都属于 pre-activation</li>
</ul>
<p><strong>Post-activation or pre-activation?</strong> </p>
<p>在 original 版本里面，ReLu 函数既作用在 x 这条通道上，也作用在 $F(x,W)$ 上，这种我们称为对称 activation。</p>
<script type="math/tex; mode=display">y_{l+1} = x_l + F(x_l, W_{l+1}) = f(y_l) + F(f(y_l), W_{l+1})</script><p>而作者提出了一种非对称的 activation，即 ReLu 只作用到 $F(x,W)$ 上，不作用到 x 上，我们令这个非对称 activation 的函数为 $\hat{f}$，因此</p>
<script type="math/tex; mode=display">y_{l+1} = x_l + F(x_l, W_{l+1}) = y_l + F(\hat{f}(y_l), W_{l+1})</script><p>变换一下变量</p>
<script type="math/tex; mode=display">x_{l+1} = x_l + F(\hat{f}(x_l),W_l)</script><p>然后我们可以将 post-activation 转成 pre-activation，如下图</p>

<p>而根据 pre-activation 的内容又分了两种类型 ReLu-only pre-activation 和 full pre-activation。</p>
<p>实验的结果如下图，可以看出 full pre-activation 的效果最好。</p>
<img src="/2018/12/01/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Identity-Mappings-in-Deep-Residual-Networks/result_f.png" class="">
<p><br></p>
<h1 id="Analysis"><a href="#Analysis" class="headerlink" title="Analysis"></a>Analysis</h1><p>作者将最优的 $h$ 函数和 $f$ 函数构造的 resnet 和其他模型进行对比，发现性能还能提升。</p>
<img src="/2018/12/01/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Identity-Mappings-in-Deep-Residual-Networks/1.png" class="">
<p>pre-activation 有两点好处</p>
<ol>
<li>网络更容易优化，因为 $f$ 函数是 identity mapping。</li>
<li>pre-activation 的 BN 给模型带来不少正则。</li>
</ol>
<p>ResNet-v1 虽然也有 BN 层，但是 BN 完之后马上和信号相加，因此相当于没有标准化。</p>
<p><br></p>
<p><em>In Conclusion</em></p>
<p>作者将 ResNet-v1 进行了修改，探索了不同的 shortcut，以及 mapping function，最后得出更加好的结果。</p>
<p><br></p>
]]></content>
  </entry>
  <entry>
    <title>【论文阅读】—— Image Super-Resolution Using Knowledge Distillation</title>
    <url>/2019/09/18/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Image-Super-Resolution-Using-Knowledge-Distillation/</url>
    <content><![CDATA[<blockquote>
<p>Title: Image Super-Resolution Using Knowledge Distillation<br>Authors: Qinquan Gao, et al<br>Conference: ACCV, 2018<br>Abstract: 作者第一次将知识蒸馏用到超分任务上，并取得了提升。</p>
</blockquote>
<span id="more"></span>
<h2 id="Framework"><a href="#Framework" class="headerlink" title="Framework"></a>Framework</h2><h3 id="Teacher-network"><a href="#Teacher-network" class="headerlink" title="Teacher network"></a>Teacher network</h3><p>并没有采用比较通用的超分模型，而是自己设计了一个20层的网络，分为三个部分：feature extraction，Non-linear Mapping 以及 reconstruction，结构图如下</p>
<ul>
<li>非线性映射部分采用 residual block 堆叠的方式，每个 block 为两个卷积层组成。</li>
<li>卷积核大小采用 $3 \times 3$，通道数固定为 64.</li>
</ul>
<img src="/2019/09/18/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Image-Super-Resolution-Using-Knowledge-Distillation/1.png" class="" width="500" height="500">
<h3 id="Student-Network"><a href="#Student-Network" class="headerlink" title="Student Network"></a>Student Network</h3><p>和 teacher 网络基本一样，层数也一样，但是为了减少学生网络的参数以及计算开销，每个 residual block 采用的是 depthwise 卷积。</p>
<img src="/2019/09/18/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Image-Super-Resolution-Using-Knowledge-Distillation/2.png" class="" width="500" height="500">
<h3 id="Knowledge-Distillation"><a href="#Knowledge-Distillation" class="headerlink" title="Knowledge Distillation"></a>Knowledge Distillation</h3><p>只采用了特征蒸馏，并没有采用软标签蒸馏，对于特征蒸馏，提出四种特征提取方式</p>
<ul>
<li>$(G<em>{mean})^pF=(\frac{1}{C}\sum</em>{i=1}^CF_i)^p$</li>
<li>$(G^p)<em>{mean}F=\frac{1}{C}\sum</em>{i=1}^C(F_i^p)$</li>
<li>$G<em>{max}F=max</em>{i=1,C}F_i$</li>
<li>$G<em>{min}F=min</em>{i=1,C}F_i$</li>
</ul>
<p>特征位置的选取上，由于教师和学生网络在 NonLinear Mapping 部分均为 10 层，因此作者提取 第 4、7、10 层的特征。模型整体的 Loss 如下，$Y$ 为 HR image。</p>
<script type="math/tex; mode=display">
loss = loss_0(\tilde{Y},Y) + \lambda_1loss_1(s_1,t_1)+\lambda_2loss_2(s_2,t_2)+\lambda_3loss_3(s_3,t_3)</script><p><br></p>
<h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><h3 id="特征选取位置的影响"><a href="#特征选取位置的影响" class="headerlink" title="特征选取位置的影响"></a>特征选取位置的影响</h3><p>实验结果如下，个人觉得这实验结果并没有说服力，因为随机性太强，作者认为最好的结果也没有特别明显，并且在过程中，改变了 $\lambda_0$，才取得最好的结果，而 $\lambda_0$ 和特征位置的选取并没有关系，理应固定；而当固定 $\lambda_0$ 为 1 的时候，可以看出结果并没有特别大的区别，(1, 0, 0) 和 (0, 1, 0) 均为一样的结果。</p>
<img src="/2019/09/18/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Image-Super-Resolution-Using-Knowledge-Distillation/3.png" class="" width="500" height="500">
<h3 id="特征提取方式的影响"><a href="#特征提取方式的影响" class="headerlink" title="特征提取方式的影响"></a>特征提取方式的影响</h3><p>作者比较四种特征提取方式，以及不同的 $p$ 值，实验结果如下。$(G_{mean})^2$ 的结果最好，且蒸馏有效。</p>
<img src="/2019/09/18/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Image-Super-Resolution-Using-Knowledge-Distillation/4.png" class="" width="500" height="500">>





### 蒸馏整体的有效性

<img src="/2019/09/18/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Image-Super-Resolution-Using-Knowledge-Distillation/5.png" class="" width="500" height="500">
<p><br></p>
<p><em>In Conclusion</em></p>
<p>作者确实验证了蒸馏在超分上的有效性，但是网络选择方面没有选择通用的超分网络，而是自己搭建了一个网络，这样，自己搭建的网络是否适用于超分任务也很难说明，另外蒸馏方面只用到特征的蒸馏，而没有采用标签的蒸馏。</p>
<p><br></p>
]]></content>
  </entry>
  <entry>
    <title>【论文阅读】—— Inception-v4, Inception-ResNet and Impact of Residual Connection on Learning</title>
    <url>/2018/12/03/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Inception-v4-Inception-ResNet-and-Impact-of-Residual-Connection-on-Learning/</url>
    <content><![CDATA[<blockquote>
<p>Title：Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning<br>Authors：Christian Szegedy, Sergey Ioffe<br>Session：AAAI, 2017<br>Abstract：作者修改了 Inception-v3 为 Inception-v4，并且将 Inception 和 ResNet 进行结合。两个网络的性能接近。</p>
</blockquote>
<span id="more"></span>
<p><br></p>
<h1 id="Inception-v4"><a href="#Inception-v4" class="headerlink" title="Inception-v4"></a>Inception-v4</h1><img src="/2018/12/03/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Inception-v4-Inception-ResNet-and-Impact-of-Residual-Connection-on-Learning/1.png" class="">
<p><br></p>
<h1 id="Inception-ResNet"><a href="#Inception-ResNet" class="headerlink" title="Inception-ResNet"></a>Inception-ResNet</h1><p>结构和上面基本相同，只是将 Inception-block 改为 Inception-ResNet-block，列举几个 block。作者在论文中提到了两个版本的 Inception-Resnet。</p>
<img src="/2018/12/03/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Inception-v4-Inception-ResNet-and-Impact-of-Residual-Connection-on-Learning/2.png" class="">
<img src="/2018/12/03/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Inception-v4-Inception-ResNet-and-Impact-of-Residual-Connection-on-Learning/3.png" class="">
<p>还有一个 trick 就是作者发现当卷积核的个数超过 1000 的时候，网络前向传播的值会偏向于 0，因此对 residual 进行放缩。</p>
<img src="/2018/12/03/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Inception-v4-Inception-ResNet-and-Impact-of-Residual-Connection-on-Learning/4.png" class="">
<p><br></p>
<h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><p>Inception-ResNet-v1 和 Inceptin-v3 计算代价差不多，Inception-ResNet-v2 和 Inception-v4 计算代价差不多，然而实作上 Inception-v4 慢很多可能是因为层数太多。 而 Inception-ResNet 要收敛更快。</p>
<img src="/2018/12/03/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Inception-v4-Inception-ResNet-and-Impact-of-Residual-Connection-on-Learning/5.png" class="">
<p><br></p>
]]></content>
  </entry>
  <entry>
    <title>【论文阅读】—— MobileNetV2, Inverted Residuals and Linear Bottlenecks</title>
    <url>/2018/12/11/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-MobileNetV2-Inverted-Residuals-and-Linear-Bottlenecks/</url>
    <content><![CDATA[<blockquote>
<p>Title：MobileNetV2: Inverted Residuals and Linear Bottlenecks<br>Authors：Google 团队<br>Session：CVPR, 2018<br>Abstract：作者在 MobileNet-v1 的基础上，提出了两个 tricks：Linear Bottleneck 和 Inverted Residual，并提升了网络的性能。</p>
</blockquote>
<span id="more"></span>
<p><br></p>
<h1 id="depthwise-separable-convolution"><a href="#depthwise-separable-convolution" class="headerlink" title="depthwise separable convolution"></a>depthwise separable convolution</h1><p>深度可分离卷积分为两个部分：depthwise convolution 和 pointwise convolution，前者在 Xception 中提出，深度可分离卷积实现了 spatial 和 channel 之间的解耦，下图对比了传统的卷积和深度可分离卷积，regular 的卷积核是从 spatial 和 channel 两个不同角度去提取特征，而深度可分离卷积，先从 spatial 层面进行过滤，再用点卷积进行特征提取。</p>
<img src="/2018/12/11/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-MobileNetV2-Inverted-Residuals-and-Linear-Bottlenecks/1.png" class="">
<p><br></p>
<h1 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h1><p>先从整体上看 MobileNet-v2 的网络结构，再去仔细分析每个 trick 背后的 intuition。下面先对比一下 MobileNet-v1 和 MobileNet-v2 每个 block 。</p>
<img src="/2018/12/11/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-MobileNetV2-Inverted-Residuals-and-Linear-Bottlenecks/2.png" class="">
<blockquote>
<p>相同点</p>
</blockquote>
<ul>
<li><strong>都采用 Depthwise (DW) 卷积搭配 Pointwise (PW) 卷积的方式来提特征。</strong>这两个操作合起来也被称为 Depthwise Separable Convolution，之前在 Xception 中被广泛使用。这么做的好处是理论上可以成倍的减少卷积层的时间复杂度和空间复杂度。 </li>
<li>在 Depthwise 后都是接 ReLU6 作为激活函数，ReLU6 表示将 ReLU 的最大值设为 6。虽然在 Xception 中已经提到过，Depthwise 卷积后面接线性激活的效果要比非线性激活好，但是 MobileNet 的作者依旧在 Depthwise 卷积后加了 ReLU 激活，无论是 v1 还是 v2。</li>
</ul>
<blockquote>
<p>不同点</p>
</blockquote>
<ul>
<li>MobileNet-v2 在 DW 前加了一个 PW，这么做的原因，是因为 DW 卷积由于本身的计算特性决定它自己没有改变通道数的能力，上一层给它多少通道，它就只能输出多少通道。所以如果上一层给的通道数本身很少的话，DW 也只能在低维空间提特征，因此效果不够好。 MobileNet-v2 为了改善这个问题，给每个 DW 之前都配备了一个 PW，专门用来升维，定义升维系数（expansion ratio）$t = 6$ ，这样不管输入通道数 $C<em>{in}$ 是多是少，经过第一个 PW 升维之后，DW 都是在相对的更高维 ( $t \cdot C</em>{in}$ ) 进行着特征提取。</li>
<li><strong>V2 去掉了第二个 PW 的激活函数</strong>。论文作者称其为 Linear Bottleneck。这么做的原因，是因为作者认为激活函数在高维空间能够有效的增加非线性，而在低维空间时则会破坏特征，不如线性的效果好。由于第二个 PW 的主要功能就是降维，因此按照上面的理论，降维之后就不宜再使用 ReLU6 了。<img src="/2018/12/11/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-MobileNetV2-Inverted-Residuals-and-Linear-Bottlenecks/3.png" class="">
</li>
</ul>
<p><br><br>下面再对比一下 MobileNet-v2 和 ResNet</p>
<img src="/2018/12/11/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-MobileNetV2-Inverted-Residuals-and-Linear-Bottlenecks/4.png" class="">
<blockquote>
<p>相同点</p>
</blockquote>
<ul>
<li>MobileNet-v2 借鉴了 ResNet block 的结构，采用了 $1 \times 1 \rightarrow 3\times 3 \rightarrow 1\times1$</li>
<li>MobileNet-v2 借鉴了 ResNet，同样适用 shortcut 将 block 的输入输出相连（上图未显示）</li>
</ul>
<blockquote>
<p>不同点</p>
</blockquote>
<ul>
<li>ResNet 使用 <strong>标准卷积</strong> 提特征，MobileNet 始终使用 <strong>DW</strong> 提特征。 </li>
<li>ResNet <strong>先降维</strong> (0.25倍)、卷积、再升维，而 MobileNet V2 则是 <strong>先升维</strong> (6倍)、卷积、再降维。作者将 MobileNet-v2 的结构称为 Inverted Residual Block。这么做也是因为使用 DW 卷积而作的适配，希望特征提取能够在高维进行。 </li>
<li>最后的 PW 卷积用线性激活，即上面提到的 Linear Bottleneck。</li>
</ul>
<p>下图中，浅色的代表一个 block 的输出，有斜线的代表用线性激活，下图可以看出来，ResNet 中 shortcut 连接的是两个 channel 数很大的层，而 Inverted residual block 连接的却是两个 bottleneck。</p>
<img src="/2018/12/11/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-MobileNetV2-Inverted-Residuals-and-Linear-Bottlenecks/5.png" class="">
<p><br></p>
<p>再来看整体的网络结构，t 代表 expansion ratio，c 代表输出的 channel 数，n 代表重复的次数，s 代表 stride。</p>
<img src="/2018/12/11/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-MobileNetV2-Inverted-Residuals-and-Linear-Bottlenecks/6.png" class="">
<p>需要注意的是：</p>
<ul>
<li><p>当 n &gt; 1 的时候，即该 bottleneck 重复次数大于 1 次，只有在第一个 bottleneck 的 stride 为 s，其它的 stride 为 1.</p>
</li>
<li><p>只有在 stride 为 1 的情况下，才会有 shortcut 连接。见下图，stride 为 2 的时候没有 shortcut。</p>
</li>
</ul>
<img src="/2018/12/11/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-MobileNetV2-Inverted-Residuals-and-Linear-Bottlenecks/7.png" class="">
<ul>
<li>当 n &gt; 1 时，只在第一个 bottleneck 的输出 channel 为 c，其他 bottleneck 输出的 channel 不变。<br><br><br>例如，对于上图中 $56^2 \times 24$ 的那层。</li>
</ul>
<ol>
<li>n = 3，共有 3 个 bottleneck，只在第一个 bottleneck 使用 stride = 2，后两个bottleneck 的 stride = 1；</li>
<li>第一个瓶颈层由于输入和输出尺寸不一致，因而无 shortcut 连接，后两个由于stride = 1，输入输出特征尺寸一致，会使用 shortcut 将输入和输出特征进行相加；</li>
<li>只有第一个 bottleneck 最后的 $1\times1$ conv 的输出 channel 为 c，后两个 bottleneck 的输出 channel 恒定不变，保持为 c。</li>
</ol>
<p>该层输入特征为$56\times56\times24$，第一个 bottleneck 输出为 $28\times28\times32$（ feature map 尺寸降低，channel 数增加，无 shortcut），第二、三个 bottleneck 的输入和输出均为$28\times28\times32$（此时 c = 32，s=1，有shortcut）。</p>
<p><br></p>
<h1 id="Intuition-behind-MobileNet-v2"><a href="#Intuition-behind-MobileNet-v2" class="headerlink" title="Intuition behind MobileNet-v2"></a>Intuition behind MobileNet-v2</h1><p>MobileNet-v2 正如论文标题所说，两个最大的特点就是 Linear Bottleneck 和 Inverted  Residuals，虽然线性激活函数在 Xception 中已经提到，被放在 DW 卷积后，但是作者并没有那么做，而是把线性激活函数放在 bottleneck 上，下面看一下作者这样做的原因。</p>
<p>对于一组真实图片，作者称激活层后的张量为 manifold of interest，根据之前的研究，manifold of interest 可能仅分布在激活空间的一个低维子空间里，那么我们可以通过减少每一层 channel 的数量来减少空间的维数，MobileNet-v1 就是通过一个宽度乘子来实现的，但是由于非线性变换的存在，如 ReLU，使得降维操作会损失较多的信息，作者做了一个小实验来验证这一点。</p>
<p>下图中输入在二维空间，作者通过一个 $n \times 2$ 的矩阵 T 将输入映射到 n 维空间中，再经过 ReLU 函数，然后再用 $T^{-1}$ 将其映射回二维空间，下图表示当 n 取不同值时候的结果，可以看出当 n 取 2 或 3 的时候，恢复的数据和原来的数据有较大的差异，信息丢失得比较严重；而当 n 比较大的时候，信息能够得到较大的保留，作者给出的解释是如果维度较大，即有较多的 channel，一个 channel 信息的丢失可能会在另外的 channel 中得到保留。</p>
<img src="/2018/12/11/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-MobileNetV2-Inverted-Residuals-and-Linear-Bottlenecks/8.png" class="">
<p>由于网络中每一层激活后的空间都可以理解为 manifold of interest 的一个表征，如果这个激活空间的维数过低，再经过 ReLU 非线性激活，根据上面的结论，信息会丢失较多，那么这个激活空间就不能很好地表征这个 manifold of interest，因此作者才提出用 Linear  Bottleneck 去代替 ReLU。</p>
<p>作者在附录中证明了：如果 input manifold 可以是一个低维激活空间的表征，那么 ReLU 激活函数就可以保留所有的信息，且实验证明了线性层能够防止 ReLU 函数破坏太多的信息。</p>
<p>而上面提到的，高维空间 ReLU 能够保留较多的信息，这也是作者提出 Inverted  Residuals 的原因，升维后使用 ReLU，降维后使用线性激活。</p>
<p><br></p>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ol>
<li><a href="https://ai.googleblog.com/2018/04/mobilenetv2-next-generation-of-on.html">谷歌官方博客</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/33075914">论文笔记1</a></li>
<li><a href="https://blog.ddlee.cn/posts/c9816b0a/">论文笔记2</a></li>
<li><a href="https://blog.csdn.net/u014380165/article/details/79200958">论文笔记3</a></li>
<li><a href="https://www.cnblogs.com/darkknightzh/p/9410574.html">论文笔记4</a></li>
</ol>
<p><br></p>
]]></content>
  </entry>
  <entry>
    <title>【论文阅读】—— Knowledge Distillation Meets Self-Supervision</title>
    <url>/2020/08/06/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Knowledge-Distillation-Meets-Self-Supervision/</url>
    <content><![CDATA[<p><br></p>
<blockquote>
<p>Title: Knowledge Distillation Meets Self-Supervision<br>Conference: ECCV 2020<br>Authors: Guodong Xu, Ziwei Liu, et al<br>Abstract: 作者提出用自监督的方式去提取一些更加 general 的知识，并将它用来知识蒸馏，能大幅度提升不同网络架构之间的蒸馏效果。</p>
</blockquote>
<span id="more"></span> 
<p><em>prefix</em></p>
<p>很多知识蒸馏的文章都将重点放到了如何提取网络中间层的特征，比如提取注意力图、相关性矩阵、分布统计等，但是这些方法提取到的知识都是与网络所处理的任务相关的（task-specific），并且这些知识是与网络的架构相关的（architecture-relative），所以会导致不同网络结构的知识蒸馏效果不佳。</p>
<p>而自监督这种训练方式，能够充分地利用数据本身去学习网络，常见的自监督任务有</p>
<ul>
<li>exemplar：每一个样本是一个类别，训练网络进行分类任务</li>
<li>rotation：把图片进行旋转，旋转的角度有4个，然后进行四分类</li>
<li>jigsaw：将图片分成四块，然后随机打乱顺序，共有24种顺序，进行24分类问题</li>
<li>contrastive learning：将图片进行数据增广，然后让网络判别出原图和增广后的图是positive，与其他图是negative</li>
</ul>
<p>针对知识蒸馏任务，作者认为需要更加通用的知识，因此将自监督作为辅助任务来获得更加general的knowledge</p>
<p><br></p>
<h2 id="Self-Supervision-Knowledge-Distillation-SSKD"><a href="#Self-Supervision-Knowledge-Distillation-SSKD" class="headerlink" title="Self-Supervision Knowledge Distillation(SSKD)"></a>Self-Supervision Knowledge Distillation(SSKD)</h2><h3 id="Contrastive-Learning-as-Self-supervision"><a href="#Contrastive-Learning-as-Self-supervision" class="headerlink" title="Contrastive Learning as Self-supervision"></a>Contrastive Learning as Self-supervision</h3><p>给定有 N 个样本 ${x<em>i}</em>{i=1:N}$ ，有一个 transformation $T$ 集合，任意一个 transformation $t(\cdot) \in T$ 作用到每一个样本上得到 ${\tilde{x}<em>i}</em>{i=1:N}$ 。</p>
<p>contrastive learning 的核心是要最大化 positive pair $(\tilde{x}<em>i, x_i)$ 的相似性，和最小化 negative pairs $(\tilde{x}_i, x_k)</em>{k\neq i}$ 的相似性，相似性的定义为 cosine 相似性如下</p>
<img src="/2020/08/06/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Knowledge-Distillation-Meets-Self-Supervision/1.png" class="">
<p>contrastive prediction loss 如下</p>
<img src="/2020/08/06/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Knowledge-Distillation-Meets-Self-Supervision/2.png" class="">
<h3 id="SSKD-Framework"><a href="#SSKD-Framework" class="headerlink" title="SSKD Framework"></a>SSKD Framework</h3><p>教师网络和学生网络都由三个部分组成</p>
<ul>
<li>backbone $f(\cdot)$</li>
<li>classifier $p(\cdot)$</li>
<li>自监督模块 SS $c(\cdot,\cdot)$</li>
</ul>
<img src="/2020/08/06/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Knowledge-Distillation-Meets-Self-Supervision/3.png" class="">
<p><strong>Training the teacher network</strong></p>
<ul>
<li>第一阶段：标准训练，用正常的数据集去训练 $f(\cdot)$ 和 $p(\cdot)$</li>
<li>第二阶段：固定住 backbone，然后训练 SS 模块，它是一个两层的 MLP，contrastive learning 用到 4 种 transformations，分别为 color dropping、rotation、clipping + resizing、color jittering。</li>
</ul>
<p><strong>Training the student network</strong></p>
<p>四个 loss 组成</p>
<ul>
<li>cross entropy</li>
<li>kd loss，教师的输出和学生的输出</li>
</ul>
<img src="/2020/08/06/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Knowledge-Distillation-Meets-Self-Supervision/4.png" class="">
<ul>
<li>SS loss，约束两个相似性矩阵，采取 KL 散度的形式</li>
</ul>
<img src="/2020/08/06/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Knowledge-Distillation-Meets-Self-Supervision/5.png" class="">
<ul>
<li>在增广的数据上也进行约束</li>
</ul>
<img src="/2020/08/06/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Knowledge-Distillation-Meets-Self-Supervision/6.png" class="">
<p>由于教师网络的自监督模块只由 2 层 MLP 构成，并且训练过程 backbone 固定，所以很有可能会生成不正确的预测，即认为 $(x<em>k,\tilde{x}_i)</em>{k\neq i}$ 为 positive pair，为了避免这种情况，softmax 之后排序，若正确的在 top-k 个，则认为是 positive pair</p>
<p><br></p>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p><strong>Ablation study</strong></p>
<p>感觉是数据增广的用处要比 SS 的大…</p>
<img src="/2020/08/06/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Knowledge-Distillation-Meets-Self-Supervision/7.png" class="">
<p><strong>k值的影响</strong></p>
<p>既要保留一点的错误，也不能容忍全部错误</p>
<img src="/2020/08/06/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Knowledge-Distillation-Meets-Self-Supervision/8.png" class="">
<p><strong>不同自监督任务对比</strong></p>
<img src="/2020/08/06/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Knowledge-Distillation-Meets-Self-Supervision/9.png" class="">
<p><strong>cifar100上的结果</strong></p>
<p>可以看出不同架构的知识蒸馏提升很大</p>
<img src="/2020/08/06/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Knowledge-Distillation-Meets-Self-Supervision/10.png" class="">
<img src="/2020/08/06/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Knowledge-Distillation-Meets-Self-Supervision/11.png" class="">
<p><strong>further study</strong></p>
<p>在 few-shot 和 noisy label 的情况下，SSKD 也能很好地提取知识</p>
<img src="/2020/08/06/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Knowledge-Distillation-Meets-Self-Supervision/12.png" class="">]]></content>
  </entry>
  <entry>
    <title>【论文阅读】—— MobileNets, Efficient Convolutional Neural Networks for Mobile Vision Applications</title>
    <url>/2018/12/04/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications/</url>
    <content><![CDATA[<blockquote>
<p>Title：MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications<br>Authors：Google 团队<br>Abstract：MobileNet 是为移动和嵌入式设备提出的高效模型。 使用了<strong>深度可分离卷积</strong>来构造的轻量级深度神经网络，计算量大幅度减小，且性能能媲美其它网络。</p>
</blockquote>
<span id="more"></span>
<p><br></p>
<h1 id="Depthwise-Separable-Convolution"><a href="#Depthwise-Separable-Convolution" class="headerlink" title="Depthwise Separable Convolution"></a>Depthwise Separable Convolution</h1><p>假设卷积输入维度为 $D_F \times D_F \times M$，卷积核为 $D_K \times D_K \times M \times N$，采取的是 SAME 卷积，因此卷积输出为 $D_F \times D_F \times N$，这种标准的卷积层的计算量为</p>
<script type="math/tex; mode=display">D_K \cdot D_K \cdot M \cdot N \cdot D_F \cdot D_F</script><p>而 Depthwise Separable Convolution 分为两部分：depthwise convolution 和 pointwise convolution。</p>
<p>depthwise 卷积使用 M 个大小为 $D_K \times D_K \times 1$ 的卷积核分别对输入的 M 个 channel 进行卷积，即每个卷积核负责一个 channel，如果采取的是 stride 为 1 的 SAME 卷积，那么卷积出来的结果维度和输入一样 $D_F \times D_F \times M$，depthwise 卷积的计算量为</p>
<script type="math/tex; mode=display">D_K \cdot D_K \cdot M  \cdot D_F \cdot D_F</script><p>pointwise 卷积是对 depthwise 卷积的结果再进行 channel 级别上的 interaction，用 N 个 $1\times 1 \times M$ 的卷积核去处理，得到结果大小为 $D_F \times D_F \times N$，pointwise 卷积的计算量为</p>
<script type="math/tex; mode=display">1 \cdot 1 \cdot M \cdot N \cdot D_F \cdot D_F</script><p>总的 depthwise separable convolution 的计算量为</p>
<script type="math/tex; mode=display">D_K \cdot D_K \cdot M  \cdot D_F \cdot D_F + M \cdot N \cdot D_F \cdot D_F</script><p>对比标准的卷积，如果用 $3 \times 3$ 的卷积核，大概能减少个 8 - 9 倍的计算量。</p>
<script type="math/tex; mode=display">\frac{D_K \cdot D_K \cdot M  \cdot D_F \cdot D_F + M \cdot N \cdot D_F \cdot D_F}{D_K \cdot D_K \cdot M \cdot N \cdot D_F \cdot D_F} = \frac{1}{N} + \frac{1}{D_K^2}</script><p>下图是标准卷积核和深度可分离卷积核的对比</p>
<ul>
<li>standard conv 是既有 feature map 的 local 信息的挖掘，也有 channel 之间信息的挖掘，因为每一个卷积核的深度就等于输入 channel 的大小，但是这也是它计算量大的原因，尤其当输入 channel 比较大的时候，卷积核的 channel 也必须与输入的 channel 大小 match。</li>
<li>depthwise conv 将输入图像的每一个 channel 单独用一个卷积核去过滤，相当于每一个 channel 先进行一个过滤操作，但是它并没有 channel 之间的 interaction。</li>
<li>point-wise conv 就等于 standard conv，组合不同 channel 的信息，来构造新的特征。</li>
</ul>
<p>因此 depthwise conv &amp; pointwise conv 就能比较好地去代替 standard conv。</p>
<img src="/2018/12/04/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications/1.png" class="">
<p><br></p>
<h1 id="MobileNet"><a href="#MobileNet" class="headerlink" title="MobileNet"></a>MobileNet</h1><p>网络结构如下，MobileNet 去掉了用池化层去降维，而是用 stride 为 2 的卷积层，Conv dw / s1 表示 stride 为 1 的 depthwise conv，$3 \times 3 \times 32$ dw 表示有 32 个 $3 \times 3$ 的卷积核。</p>
<img src="/2018/12/04/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications/2.png" class="">
<p>下图展示的是 MobileNet 不同 component 的计算量占比，可以看出 Conv $1 \times 1$ 计算量和参数占比都是最大，那是因为 pointwise conv 实际上就是一个 standard conv，连接全部 channel 来输出一个特征，所以计算量也大。而 depthwise conv 计算量就少很多。</p>
<img src="/2018/12/04/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications/3.png" class="">
<p><br></p>
<h1 id="Two-Multipliers"><a href="#Two-Multipliers" class="headerlink" title="Two Multipliers"></a>Two Multipliers</h1><p>我们把上面的 MobileNet 称为 baseline model。</p>
<p><strong>Width Multiplier：Thinner Models</strong></p>
<p>宽度乘子 $\alpha$ 的作用在于把每一层的 channel 数量进行统一的缩减，因此输入 channel 大小为 M 则变为 $\alpha M$，输出 channel 大小为 N 则变为 $\alpha N$。实验中 $\alpha$ 取值 1，0.75，0.5，0.25，$\alpha = 1$ 则为 baseline model，加上宽度乘子后的计算量为</p>
<script type="math/tex; mode=display">D_K \cdot D_K \cdot \alpha M  \cdot D_F \cdot D_F + \alpha M \cdot \alpha N \cdot D_F \cdot D_F</script><p><strong>Resolution Multiplier：Reduced Representation</strong></p>
<p>分辨率说的就是 feature map 的大小，分辨率乘子 $\rho$ 作用于 feature map 的宽和高，使得 feature map 的大小得到缩减，作者实验中取 feature map 大小为 224，192，160 和 128 来模拟不同的 $\rho$ 值，224 则为 $\rho = 1$ 的情况，加上分辨率乘子的计算量为</p>
<script type="math/tex; mode=display">D_K \cdot D_K \cdot \alpha M  \cdot \rho D_F \cdot \rho D_F + \alpha M \cdot \alpha N \cdot \rho D_F \cdot \rho D_F</script><p>假设输入为 $14 \times 14 \times 512$，standard conv 的卷积核为 $3 \times 3 \times 512 \times 512$，计算量为 $14 \cdot 14 \cdot 512 \cdot 3 \cdot 3 \cdot 512 \approx 462M$，而 depthwise separable conv 的计算量降了接近十倍，加上乘子后进一步减少计算量。</p>
<img src="/2018/12/04/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications/4.png" class="">
<p><br></p>
<h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><p><strong>depthwise separable conv vs full conv</strong></p>
<p>depthwise 虽然准确率不如 full，但是参数量和计算量大幅度的减少</p>
<img src="/2018/12/04/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications/5.png" class="">
<p><strong>thinner vs shallower</strong></p>
<p>thinner 指用宽度乘子为 0.75，shallower 指将原网络结构的 5  层 feature map 大小为 $14 \times 14 \times 512$ 的卷积层去掉。两者的参数量和计算量都差不多，但是明显 thinner 要比 shallower 要好。</p>
<img src="/2018/12/04/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications/6.png" class="">
<p><strong>乘子不同取值比较</strong></p>
<p>当宽度乘子不断减小，channel 数量不断减小，准确率不断下降，计算量也大幅度下降，同理分辨率乘子。</p>
<img src="/2018/12/04/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications/7.png" class="">
<p>宽度乘子和分辨率乘子各取 4 个值，共得到 16 个模型，作者比较这 16 个模型的计算量、参数量和准确率之间的关系，结果如下，毫无疑问，计算量越大或者参数越多，准确率越高，但是作者需要寻找的是他们两者之间的一个 tradeoff，找到一个既能保证准确率，计算量又不算太大的模型。</p>
<img src="/2018/12/04/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications/8.png" class="">
<p>同时作者对比了 baseline mobileNet 和 GoogLeNet，VGG 在 ImageNet 上的性能，mobileNet 比 GoogLeNet 要好，比 VGG 稍微差一点，但是计算量和参数却是另外两个模型无法比拟的。</p>
<img src="/2018/12/04/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications/9.png" class="">
<p>作者对比缩减版的 mobileNet 和其它模型，也是相同的结论。</p>
<img src="/2018/12/04/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications/10.png" class="">
<p>作者还在其它的分类数据集上进行了实验，还在其它的视觉应用上进行了实验，如人脸识别，face attribute，用到了蒸馏技术。</p>
<p><br></p>
<p><em>In Conclusion</em></p>
<p>mobileNet 提供给我们的是一种轻量级别的深层神经网络，更加值得我们思考的是，我们是否需要一味地追求准确率，还是需要在准确率和计算性能上进行平衡。</p>
<p><br></p>
]]></content>
  </entry>
  <entry>
    <title>【论文阅读】—— Rethinking the Inception Architecture for Computer Vision</title>
    <url>/2018/12/03/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Rethinking-the-Inception-Architecture-for-Computer-Vision/</url>
    <content><![CDATA[<blockquote>
<p>Title：Rethinking the Inception Architecture for Computer Vision<br>Authors：Christian Szegedy et al.<br>Session：CVPR, 2016<br>Abstract：Inception 的作者进一步地优化网络结构，提出了非对称卷积的方法，使得网络的训练速度和性能得到进一步的增强，该版本被称为 Inception-v3。</p>
</blockquote>
<span id="more"></span>
<p><br></p>
<h1 id="Inception-v2"><a href="#Inception-v2" class="headerlink" title="Inception-v2"></a>Inception-v2</h1><p>作者先提出了一些修改，并将其命名为 Inception-v2，然后再 Inception-v2 的基础上再提出 Inception-v3，下面先列举 Inception-v2 相对于 Inception-v1 的改变。</p>
<ul>
<li><p>将大卷积核分解</p>
<ul>
<li>将一个 $5 \times 5$ 的卷积核分解为两个 $3 \times 3$ 的卷积核，和 VGG 一样。</li>
</ul>
</li>
</ul>
<img src="/2018/12/03/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Rethinking-the-Inception-Architecture-for-Computer-Vision/1.png" class="">
<ul>
<li><p>采取非对称卷积。将 $3\times1$ 的卷积和 $1\times3$ 的卷积串联起来，与直接进行 $3\times3$ 卷积的结果是等价的。这种卷积方式大大降低了参数量，从 $n\times n$ 降到了 $2\times n$，所以当 n 越大，降低得越多。但是也并不是适用于所有的卷积方式，论文说明，在实践中在 feature map 为 $12\times 12$ ~ $20\times20$ 时效果较好，也就是在较深层使用时效果好，浅层不太行，并且使用 $7\times1$ 和 $1\times7$ 卷积的串联可以得到很好的效果。</p>
<p>下图的最后边的 Inception block 中，$1 \times 1$ 的卷积核后面，并联了两个卷积核，分别是 $1 \times 3$ 和 $3 \times 1$，注意他们的卷积方式是 SAME，因此不会出现 dimension dismatch 的情况。 </p>
</li>
</ul>
<img src="/2018/12/03/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Rethinking-the-Inception-Architecture-for-Computer-Vision/2.jpg" class="">
<ul>
<li><p>auxiliary classifier 重新解释</p>
<ul>
<li>作者认为辅助的分类器并不能帮助网络收敛，实验表明有无辅助的分类器，网络在收敛前几乎一样，也就是意味着辅助分类器并不是像 Inceptioin-v1 论文里面提到的解决梯度问题，且没有辅助分类器的网络准确率更高。</li>
<li>作者重新解释，认为辅助分类器可能起到正则的作用。</li>
</ul>
</li>
<li><p>feature map size reduction</p>
<ul>
<li><p>在 Inception-v1 中作者通过 Inception block 之间插入 pooling layer 来降维。</p>
</li>
<li><p>而 Inception-v2 里，作者像 Inception-bn 那样，通过在某一个 Inception block 里面调整卷积核和池化层的 stride 来进行降维</p>
<img src="/2018/12/03/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Rethinking-the-Inception-Architecture-for-Computer-Vision/3.png" class="">
</li>
</ul>
</li>
</ul>
<p>整个网络的结构如下图，第一个 Inception 采用的就是大卷积核转两个小卷积核，第二第三的 Inception 采用的是非对称卷积。</p>
<img src="/2018/12/03/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Rethinking-the-Inception-Architecture-for-Computer-Vision/4.png" class="">
<p><br></p>
<h1 id="Inception-v3"><a href="#Inception-v3" class="headerlink" title="Inception-v3"></a>Inception-v3</h1><p>作者在 Inception-v2 的基础上，对辅助分类器加上 batchNorm，得到最好的分类结果，并称其为 Inception-v3。</p>
<p>简单理解就是：<strong>Inception-v3 = Inception-v2 + Factorization + Batch Normalization</strong> </p>
<img src="/2018/12/03/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Rethinking-the-Inception-Architecture-for-Computer-Vision/5.png" class="">
<p><br></p>
]]></content>
  </entry>
  <entry>
    <title>【论文阅读】—— Regularizing Class-wise Predictions via Self-knowledge Distillation</title>
    <url>/2020/06/24/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Regularizing-Class-wise-Predictions-via-Self-knowledge-Distillation/</url>
    <content><![CDATA[<blockquote>
<p>Title : Regularizing Class-wise Predictions via Self-knowledge Distillation<br>Authors : Sukmin Yun, Jongjin Park, et al<br>Conference : CVPR 2020<br>Abstract : 作者提出了一个简单的自蒸馏策略，通过约束相同类别的样本要输出尽可能相似的结果，得到不错的提升。</p>
</blockquote>
<span id="more"></span>
<p><br></p>
<h2 id="Class-wise-Self-knowledge-Distillation-CS-KD"><a href="#Class-wise-Self-knowledge-Distillation-CS-KD" class="headerlink" title="Class-wise Self-knowledge Distillation (CS-KD)"></a>Class-wise Self-knowledge Distillation (CS-KD)</h2><p>在之前的文章里面也有提到，蒸馏更像是对网络的一个正则项，这里的 CS-KD 同样也是作为一个正则的方法，通过约束相同类别的样本之间的输出，从而减少类别内样本输出的方差</p>
<img src="/2020/06/24/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Regularizing-Class-wise-Predictions-via-Self-knowledge-Distillation/1.png" class="" width="500" height="500">
<p>上图同样为鸟类别的两张图片，同时输入到一个网络，然后约束他们的输出概率要尽可能地类似，这里相当如引入了一个虚拟教师网络，而这个教师网络就是学生网络本身，并且和一般的 KD 不同点在于，两个网络的输入不同，下面是自蒸馏损失，$\tilde{\theta}$ 是 $\theta$ 的一个 copy，$x^\prime$ 和 $x$ 具有相同标签。</p>
<img src="/2020/06/24/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Regularizing-Class-wise-Predictions-via-Self-knowledge-Distillation/2.png" class="">
<p>整体的训练流程如下，对于每一个 batch 的图片，会抽样出另一个 batch，并且拥有相同的标签</p>
<img src="/2020/06/24/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Regularizing-Class-wise-Predictions-via-Self-knowledge-Distillation/3.png" class="">
<p><br></p>
<h2 id="Effects-of-class-wise-regularization"><a href="#Effects-of-class-wise-regularization" class="headerlink" title="Effects of class-wise regularization"></a>Effects of class-wise regularization</h2><p>作者提出的 CS-KD 能够实现两个目标</p>
<ul>
<li>避免 over-confident 的预测，因为它拿其他样本的输出作为软标签，这比 label smoothing regularization 构造的人工软标签要更加真实。对于误分类的样本，用 CS-KD 训练的网络，会在正确类别上有更加高的概率</li>
</ul>
<img src="/2020/06/24/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Regularizing-Class-wise-Predictions-via-Self-knowledge-Distillation/4.png" class="">
<img src="/2020/06/24/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Regularizing-Class-wise-Predictions-via-Self-knowledge-Distillation/5.png" class="">
<ul>
<li>减少类别内输出概率的方差，这也是损失函数最直接的目标</li>
</ul>
<p><br></p>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h4 id="对比其他的正则化方法"><a href="#对比其他的正则化方法" class="headerlink" title="对比其他的正则化方法"></a>对比其他的正则化方法</h4><img src="/2020/06/24/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Regularizing-Class-wise-Predictions-via-Self-knowledge-Distillation/6.png" class="">
<h4 id="对比其他自蒸馏的方法"><a href="#对比其他自蒸馏的方法" class="headerlink" title="对比其他自蒸馏的方法"></a>对比其他自蒸馏的方法</h4><ul>
<li>DDGSD : data-distortion guided self-distillation 是一种一致性正则，约束不同数据增广版本的图片输出一致</li>
<li>BYOT : be your own teacher 用深层特征指导浅层特征</li>
</ul>
<img src="/2020/06/24/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Regularizing-Class-wise-Predictions-via-Self-knowledge-Distillation/7.png" class="">
<h4 id="和其他方法融合"><a href="#和其他方法融合" class="headerlink" title="和其他方法融合"></a>和其他方法融合</h4><p>这里做了和 mixup 还有 kd 融合的结果，均是可以与其他方法相容的</p>
<img src="/2020/06/24/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Regularizing-Class-wise-Predictions-via-Self-knowledge-Distillation/8.png" class="">
<img src="/2020/06/24/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Regularizing-Class-wise-Predictions-via-Self-knowledge-Distillation/9.png" class="">
<p><br></p>
]]></content>
  </entry>
  <entry>
    <title>【论文阅读】——— ShuffleNet, An Extremely Efficient Convolutional Neural Network for Mobile</title>
    <url>/2018/12/11/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-ShuffleNet,%20An%20Extremely%20Efficient%20Convolutional%20Neural%20Network%20for%20Mobile/</url>
    <content><![CDATA[<blockquote>
<p>Title：ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile<br>Authors：Face++团队<br>Session：CVPR, 2018<br>Abstract：作者提出一个效率极高的网络结构 ShuffleNet，专门应用于计算力受限的移动设备。网络利用两个操作：pointwise group convolution 和 channel shuffle，与现有先进模型相比在类似的精度下大大降低计算量。在 ImageNet 和 MS COCO 上 ShuffleNet 表现出比其他先进模型的优越性能。</p>
</blockquote>
<span id="more"></span>
<p><br></p>
<p><em>preface</em></p>
<p>现许多 CNN 模型的发展方向是更大更深，这让深度网络模型难以运行在移动设备上，针对这一问题，许多工作的重点放在对现有预训练模型的修剪、压缩或使用低精度数据表示。 </p>
<p>论文中提出的 ShuffleNet 是探索一个可以满足受限的条件的高效基础架构。论文的 Insight 是：现有的先进 basic 架构如 Xception 和 ResNeXt 在小型网络模型中效率较低，因为大量的 $1 \times 1$ 卷积耗费很多计算资源，论文提出了逐点群卷积 (pointwise group convolution) 帮助降低计算复杂度；但是使用逐点群卷积会有幅作用，故在此基础上，论文提出通道混洗 (channel shuffle) 帮助信息流通。基于这两种技术，作者构建一个名为 ShuffleNet 的高效架构，相比于其他先进模型，对于给定的计算复杂度预算，ShuffleNet 允许使用更多的特征映射通道，在小型网络上有助于编码更多信息。</p>
<p><br></p>
<h1 id="ShuffleNet"><a href="#ShuffleNet" class="headerlink" title="ShuffleNet"></a>ShuffleNet</h1><p>ResNeXt 在每个 residual block 中应用到了 group convolution 的技术，但是只用到 $3 \times 3$ 的卷积核，并没有用到 $1 \times 1$ 的卷积核上，从下图可以看出，bottleneck layer 还是一个 standard convolution，而 ResNeXt 的论文也表明，$1 \times 1$ 的 standard conv 占了 93.4% 的计算量。</p>
<img src="/2018/12/11/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-ShuffleNet,%20An%20Extremely%20Efficient%20Convolutional%20Neural%20Network%20for%20Mobile/1.png" class="">
<p>因此作者第一个想法就是在 bottleneck layer 上也应用 group convolution，把输入 channel 被分成不同的 group，在每个 group 的卷积中，输入输出 channel 数也成倍地减少，因此能达到减少计算量的效果。但是使用 group convolution 有一个缺点就是，standard convolution中，卷积核跨越的是全部的 channel，而在 group convolution，卷积核跨越的是一个 group 里面的 channel，即输入的信息变少了，这样就导致了卷积的结果只来源于一个 group 的信息，group 之间并没有 interaction，这就阻碍了信息的流通。而反观为什么 ResNeXt 可以使用 group convolution 来代替原来的操作呢？那是因为 ResNeXt 本身就分成了多条 path，而每条 path 的信息 channel 本身就等于一个 group 的 channel 大小，因此使用 group convolution 正好等于原来的多条 path 分别卷积，并且能利用 group convolution 并行运算的好处。</p>
<p>针对上面 group convolution 阻碍 channel 之间信息流通的问题，作者提出了第二个想法：channel shuffle，将 group convolution 输出结果进行重排，如下图，(a) 为一般的 group convolution，组内进行卷积，(b) 演示了什么叫做 channel shuffle。</p>
<img src="/2018/12/11/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-ShuffleNet,%20An%20Extremely%20Efficient%20Convolutional%20Neural%20Network%20for%20Mobile/2.png" class="">
<p>channel shuffle 在实现上如下，假设有 g 个组，每个组的 channel 数为 n，因此整个 group convolution 的输出 channel 大小为 g * n。shuffle 流程如下</p>
<ul>
<li>将 $[1, g \times n]$ 的向量 reshape 成 $[g,n]$</li>
<li>将 $[g,n]$ 的矩阵进行转置得到 $[n,g]$</li>
<li>将 $[n,g]$ 的矩阵重新 reshape 成 $[1, n \times g]$ </li>
</ul>
<p>根据上面的两点想法：group convolution 和 channel shuffle，作者能够解决 bottleneck layer 计算量开销大的问题，仿照 ResNet，作者提出了 ShuffleNet 的一个 shuffle unit，如下，每个 unit 采用的是 两条 path 的模式，一条是 identity mapping，另一条是 residual function。</p>
<ul>
<li>(a)：将 $3 \times 3$ 的 standard convolution 改成 depthwise convolution（Xception 和 MobileNet 中提到），且 depthwise  convolution 后不加 ReLU 激活（Xception 中验证了其有效性）。</li>
<li>(b)：将 $1 \times1$ 的 standard convolution 改成 pointwise group convolution，并在第一个 bottleneck 和 $3 \times 3$ 卷积之间加上 channel shuffle。</li>
<li>(c)：该版本的 unit 用于维度变换，$3 \times 3$ 的卷积 stride 为 2，使得 feature map 大小减半，同时 identity mapping 也改为 stride 为 2 的 avg-pooling，作者认为在减小 feature map 的同时，需要增大 channel 数，所以把最后的 add 操作改为了 concat 操作，实现 feature map 减半，channel 数翻倍的效果。</li>
</ul>
<img src="/2018/12/11/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-ShuffleNet,%20An%20Extremely%20Efficient%20Convolutional%20Neural%20Network%20for%20Mobile/3.png" class="">
<p>假设输入维度大小为 $h \times w \times c$，bottleneck layer 的 channel 为 m，下面对比 ResNet，ResNeXt 和 ShuffleNet  一个单元的计算量（忽略 add，BN，ReLu）。</p>
<ul>
<li>ResNet：三个卷积均为 standard convolution，三层卷积层的卷积核大小分别为 $1 \times 1 \times c \times m$，$3 \times 3 \times m \times m$，$1 \times 1 \times m \times c$，图片大小变化为 $h \times w \times c$，$h \times w \times m$，$h \times w \times m$，$h \times w \times c$。因此三次卷积的计算量总和为：$hwmc + hw9m^2 + hwcm = hw(2cm + 9m^2)$</li>
<li>ResNeXt：bottleneck layer 的卷积为 standard convolution，$3 \times 3 $ 的卷积为 group convolution，因此首尾两层的计算量没变，group convolution 执行卷积操作的时候，实际上接收的图片大小为 $h \times w \times \frac{m}{g}$，卷积核的大小为 $3 \times 3 \times \frac{m}{g} \times \frac{m}{g}$，因此一个 group 的卷积计算量为 $9hw\frac{m}{g}\frac{m}{g}$，共有 g 组卷积，因此计算量为 $hw9m^2/g$，因此三次卷积的计算量总和为：$hw(2cm + 9m^2/g)$</li>
<li>ShuffleNet：bottleneck layer 改为 group convolution，$3 \times 3$ 卷积改为 depthwise convolution，按照上面的计算方法，group convolution 能把原计算量减低为 1/g，因此首尾两层的计算量降为 $2hwcm/g$，depthwise convolution 每个卷积核只负责一个 channel，共有 m 个卷积核，每个卷积核为 $3\times 3$，因此计算量为 $3 \times 3 \times h \times w \times m = hw9m$，因此三次卷积的计算量总和为：$hw(2cm/g+9m)$</li>
</ul>
<p>因此 ShuffleNet 的计算量最少，给定一个计算限制，ShuffleNet 可以使用更宽的特征映射。</p>
<p>基于上面的 shuffle unit，作者提出 ShuffleNet 的架构如下</p>
<ul>
<li>shuffleNet 分成了三个 stage，每个 stage 的 channel 数会翻倍，用的就是上面 shuffle unit (c)</li>
<li>bottleneck layer 的 channel 大小设为输出的 1/4</li>
<li>作者按照 group 的大小分成了五个网络，每个网络的 channel 大小都不同，但最后整体的 complexity 接近。</li>
<li>下面的网络称为 ShuffleNet 1 $\times$，作者称 ShuffleNet $s\times$ 为整体网络每个 channel 大小都乘上 s 的网络。</li>
</ul>
<img src="/2018/12/11/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-ShuffleNet,%20An%20Extremely%20Efficient%20Convolutional%20Neural%20Network%20for%20Mobile/4.png" class="">
<p><br></p>
<h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><p><strong>On the importance of Pointwise Group Convolution</strong></p>
<p>实验对比了 g 取不同值下的模型在 ImageNet 的错误率，g = 1，即没有 group convolution，网络变成像 Xception 的样子，结果如下；从结果来看，有 group convolution 的一致比没有 group convolution (g=1) 的效果要好。注意 group convolution 可允许获得更多通道的信息，我们假设性能的收益源于更宽的特征映射，这帮助我们编码更多信息。并且，较小的模型的特征映射通道更少，这意味着能多的从特征映射上获取收益。</p>
<p>对于一些模型，随着 g 增大，性能上有所下降。意味 group 增加，每个卷积滤波器的输入通道越来越少，损害了模型表示能力。</p>
<p>值得注意的是，对于小型的 ShuffleNet 0.25×，组数越大性能越好，这表明对于小模型更宽的特征映射更有效。</p>
<img src="/2018/12/11/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-ShuffleNet,%20An%20Extremely%20Efficient%20Convolutional%20Neural%20Network%20for%20Mobile/5.png" class="">
<p><strong>Channel Shuffle vs. No Shuffle</strong></p>
<p>在三个不同复杂度下带 shuffle 的都表现出更优异的性能，尤其是当 group 更大 (g = 8)，具有 shuffle 操作性能提升较多，这表现出 shuffle 操作的重要性。 </p>
<img src="/2018/12/11/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-ShuffleNet,%20An%20Extremely%20Efficient%20Convolutional%20Neural%20Network%20for%20Mobile/6.png" class="">
<p><strong>Comparision with other models</strong></p>
<p>作者对比不同模型 unit 之间的性能差异，如 residual block，Inception block，用各个 unit 取替换 stage 2 - 4 之间的 shuffle unit，调整通道数保证不同模型的复杂度接近。可以看出 Shufflenet 在相同 complexity 下，表现最好。</p>
<img src="/2018/12/11/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-ShuffleNet,%20An%20Extremely%20Efficient%20Convolutional%20Neural%20Network%20for%20Mobile/7.png" class="">
<img src="/2018/12/11/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-ShuffleNet,%20An%20Extremely%20Efficient%20Convolutional%20Neural%20Network%20for%20Mobile/8.png" class="">
<p><strong>Comparision with MobileNet</strong></p>
<ul>
<li>作者发现在不同 complexity 下，ShuffleNet 都比 MobileNet 要好</li>
<li>作者还尝试了更浅的网络，因为 MobileNet 是 26 层，而 ShffuleNet 为 50 层，实验表明更浅的 ShuffleNet 依旧比 MobileNet 要好。</li>
</ul>
<img src="/2018/12/11/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-ShuffleNet,%20An%20Extremely%20Efficient%20Convolutional%20Neural%20Network%20for%20Mobile/9.png" class="">
<p><strong>Actual Speedup Evaluation</strong></p>
<p>三种分辨率输入做测试，由于内存访问和其他开销，原本理论上 4 倍的加速降低到了 2.6 倍左右。但是比 AlexNet 快了十几倍。</p>
<img src="/2018/12/11/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-ShuffleNet,%20An%20Extremely%20Efficient%20Convolutional%20Neural%20Network%20for%20Mobile/10.png" class="">
<p><br></p>
]]></content>
  </entry>
  <entry>
    <title>【论文阅读】—— SMPL, A Skinned Multi-Person Linear Model</title>
    <url>/2020/09/16/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-SMPL-A-Skinned-Multi-Person-Linear-Model/</url>
    <content><![CDATA[<blockquote>
<p>Title：SMPL: A Skinned Multi-Person Linear Model<br>Authors：Matthew Loper, et al<br>Conferences：ToG 2015<br>Abstract：作者提出了一个参数化的人体蒙皮模型，通过 23 个关节点的旋转以及 10 个 shape 参数就可以生成人体蒙皮。</p>
</blockquote>
<span id="more"></span>
<p><em>prefix</em></p>
<p>2D 数据的表示非常单一，一般就用 image 来表示；而3D 数据的表示有多种，比如 multi-view image、point cloud、mesh、voxel，这里我们用到的数据为 mesh 数据，一个 mesh 由多个 vertices 组成，每个 vertices 是一个三维坐标 (x,y,z)，除了有 vertices 外，生成 mesh 的时候还需要有一个 triangle list，一个 3D object 外观上有多个 triangle 拼接而成，而 list 里面存储的就是每个 triangle 所用到 vertices index。这里我们关注的是 3D 人体模型，同样的人体由多个 vertices 组成，而 joint 是一些特殊的 vertices，用来表征人体的一些特殊关节点，如肘、肩等，实质上就是一个 vertice，但这些 joint 有可能在人体 mesh 的 vertice 里面，也可能不在。</p>
<p>Linear Blending Skinning (LBS) 指的是通过操纵骨骼来使得 mesh 发生形变，LBS 定义骨骼点（joint）和 mesh 的 vertices 之间的关系是线性关系，一个 vertice 的位置会受到其他 joint 共同的加权影响，具体操作是会先计算 joint 的位置，然后根据 joint 的位置，求其他 vertices 的位置，所以可以通过操纵骨骼来改变整个 mesh。</p>
<p><br></p>
<h3 id="Model-Formulation"><a href="#Model-Formulation" class="headerlink" title="Model Formulation"></a>Model Formulation</h3><p>SMPL 模型的 mesh 由 $N=6890$ 个 vertices 和 $K = 23$ 个 joints 组成，参数为 $\theta$ 和 $\beta$，分别表示 23 个关节点的旋转以及人体 shape 的参数，后者是一个 10 维的向量，前者向量大小取决于旋转用什么方式来表征，比如旋转向量、旋转矩阵等。</p>
<p><strong>Notation</strong></p>
<ul>
<li>mean template $\bar{T}\in \mathbb{R}^{3N}$，可以理解为初始的一个 mesh，rest pose</li>
<li>zero pose $\theta^\ast$，初始 mesh 下各个关节点的旋转角度，这里的旋转角度实际上是相对于每个点的父节点的旋转角度，比如肘部的父节点是肩部，所以肘部的旋转值是相对于肩部来确定的</li>
<li>blend weight $\mathcal{W} \in \mathbb{R}^{N\times K}$，表示的是一个关节点 k 的旋转对于所有 N 个 vertices 的影响</li>
<li>blend shape function $B_S(\beta) : \mathbb{R}^{|\beta|} \rightarrow \mathbb{R}^{3N}$，输入 shape 参数，输出 vertices offset</li>
<li>joint prediction function $J(\beta) : \mathbb{R}^{|\beta|} \rightarrow \mathbb{R}^{3K}$，输入 shape 参数，输出 joint 的位置</li>
<li>pose-dependent blend shape function $B_P(\theta) : \mathbb{R}^{|\theta|} \rightarrow \mathbb{R}^{3N}$，输入 $\theta$，输出 vertices offset</li>
<li>linear blend skinning function $W(\cdot)$ is applied to rotate the vertices around the estimated joint</li>
<li>SMPL model $M(\beta,\theta;\Phi):\mathbb{R}^{|\theta| \times |\beta|} \rightarrow \mathbb{R}^{3N}$</li>
</ul>
<p><strong>Blend Skinning</strong></p>
<p>骨骼蒙皮通过骨骼来控制整个 mesh 的变化，$\theta$ 由 K 个关节点的旋转向量组成 $\theta = [w_0, …, w_K]^T$，$w_k \in \mathbb{R}^3$ denotes the axis-angle representation of the relative rotation of part k with respect to its parent in the kinematic tree. $|\theta| = 3 \times 23 + 3 = 72$ 个参数，每个关节点 3 个参数外加整体的 root orientation，首先旋转向量会通过 Rodrigues formula 转换成 $3\times3$ 的旋转矩阵 $exp(w_k)$，标准的 LBS 为以下的函数</p>
<script type="math/tex; mode=display">
W(\bar{T}, J,\theta, \mathcal{W}):\mathbb{R}^{3N\times 3K \times |\theta| \times |\mathcal{W}|} \rightarrow \mathbb{R}^{3N}</script><p>函数会输入 rest pose $\bar{T}$，rest pose 下的 joint location $J$，pose $\theta$，blend weight $\mathcal{W}$，最后输出 posed vertices，对于每一个 vertices 的 transformation 如下，实际做的就是对 rest pose 下的每一个 joint 计算变换，然后再将计算出来的变换应用到每一个 vertice 上。</p>
<img src="/2020/09/16/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-SMPL-A-Skinned-Multi-Person-Linear-Model/1.png" class="">
<p>$G<em>k(\theta,J)$ 表示第 k 个关节点的 world transformation，这里的 world 是指世界坐标，所以用到的是齐次坐标系，$\bar{t}_i$ 和 $\bar{t}_i^\prime$ 分别表示 T-pose 下和变换后的第 i 个vertice，也都是齐次坐标系，4 维向量。$A(k)$ 表示的是第 k 个关节点的所有父节点，比如手腕的父节点分别有手肘、肩部等，因此在计算第 k 个关节点的变换的时候，依次乘以父节点的旋转矩阵，最后得到 $G_k(\theta, J)$，$G_k(\theta^\ast,J)$ 表示的是得到 rest pose 所需要的关节点的旋转变换，因此 $G^\prime_k(\theta,J)$ 是减去得到 rest pose 的变换，相当于得到一个相对的变换矩阵，然后变换矩阵乘以 rest pose 下的第 i 个 vertices 就可以得到变换后的第 i 个 vertice，然后乘上 $w</em>{k,i}$ 分量，表征第 k 个关节点对第 i 个 vertice 的影响。</p>
<p>SMPL 模型对 LBS 进行了修改，具体的流程可以看下图，首先会根据 $\beta$ 调整 T-pose，然后再根据 $\theta$ 调整（看臀部的位置，这是为后面的抬腿动作做准备），最后一步就是 LBS。</p>
<script type="math/tex; mode=display">
M(\beta,\theta) = W(T_P(\beta,\theta), J(\beta), \theta, \mathcal{W})</script><script type="math/tex; mode=display">
T_P(\beta, \theta) = \bar{T} + B_S(\beta) + B_P(\theta)</script><img src="/2020/09/16/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-SMPL-A-Skinned-Multi-Person-Linear-Model/2.png" class="">
<p><strong>Shape blend shapes</strong></p>
<p>$S = [S<em>1, …, S</em>{|\beta|}] \in \mathbb{R}^{3N\times |\beta|}$，表示 shape 每个参数对 vertices 的影响，S 是通过数据学习出来的</p>
<script type="math/tex; mode=display">
B_S(\beta; S) = \sum_{n=1}^{|\beta|} \beta_n S_n</script><p><strong>Pose blend shapes</strong></p>
<p>每个 pose 参数都用旋转矩阵表示，所以是 9K，同样的 $P\in \mathbb{R}^{3N \times 9K}$ 矩阵通过数据学习出来</p>
<script type="math/tex; mode=display">
B_P(\theta; P) = \sum_{n=1}^{9K}(R_n(\theta) - R_n(\theta^\ast))P_n</script><p><strong>Joint locations</strong></p>
<p>这里谈到的 joint 和上面用到的 $J$ 也都是在 rest pose 下的 joint 的 3D location，SMPL 在计算 joint 的时候引入了 $\beta$，更加精确，$\mathcal{J}$ is a matrix that transforms rest vertices into rest joints，同样通过数据学习出来。</p>
<script type="math/tex; mode=display">
J(\beta; \mathcal{J}, \bar{T}, S) = \mathcal{J}(\bar{T} + B_S(\beta;S))</script><p>上面提到需要学习的参数有 $\Phi = {\bar{T}, \mathcal{W}, S, \mathcal{J}, P }$，也都是一些 regressor，或者说是矩阵，这里不介绍模型的训练过程，有兴趣的可以仔细看原论文。</p>
<p><br></p>
<h3 id="代码分析"><a href="#代码分析" class="headerlink" title="代码分析"></a>代码分析</h3><p>作者共提供了三个预训练模型，分别是 male、female 和 neutral，下面以 <code>SMPL_NEUTRAL.pkl</code> 为例子，讲一下模型里面的参数，pkl 文件里面是一个字典，关键的 key 如下</p>
<ul>
<li><code>J_regressor</code>: (24, 6890)，从 rest pose 中回归出 joint，上面的 $\mathcal{J}$ 矩阵</li>
<li><code>f</code>: (13776, 3)，faces，上面我们说到 mesh 除了有 vertices 组成，还有一个 triangle list，这里就是这个 list，可以看出人体共有 13776 个 triangle，每个 triangle 由三个 vertices index 组成，所以 faces 最大的数字就是 6889，因为共 6890 个点。</li>
<li><code>kintree_table</code>: (2, 24)，一般取第一行，这就是上面提到的每个点的父节点</li>
<li><code>weights</code>: (6890, 24)，blend weight，上面的 $\mathcal{W}$ 矩阵</li>
<li><code>posedirs</code>: (6890, 3, 207)，上面的 $P$ 矩阵</li>
<li><code>shapedirs</code>: (6890, 3, 10)，上面的 $S$ 矩阵</li>
<li><code>v_template</code>: (6890, 3)，上面的 $\bar{T}$</li>
</ul>
<p>smplx 的库文件里面包含三个模型，分别是 SMPL、SMPLH 和 SMPLX，这里只分析最基础的 SMPL 的代码里面的 lbs 函数</p>
<img src="/2020/09/16/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-SMPL-A-Skinned-Multi-Person-Linear-Model/3.png" class="">
<ol>
<li>计算 shape blend shape offset</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">blend_shapes</span>(<span class="params">betas, shape_disps</span>):</span><br><span class="line">    blend_shape = torch.einsum(<span class="string">&#x27;bl,mkl-&gt;bmk&#x27;</span>, [betas, shape_disps])</span><br><span class="line">    <span class="keyword">return</span> blend_shape</span><br><span class="line"></span><br><span class="line">v_shaped = v_template + blend_shapes(betas, shapedirs)</span><br></pre></td></tr></table></figure>
<ol>
<li>计算 rest pose 下的 joint</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">vertices2joints</span>(<span class="params">J_regressor, vertices</span>):</span><br><span class="line">    <span class="keyword">return</span> torch.einsum(<span class="string">&#x27;bik,ji-&gt;bjk&#x27;</span>, [vertices, J_regressor])</span><br><span class="line"></span><br><span class="line">J = vertices2joints(J_regressor, v_shaped)</span><br></pre></td></tr></table></figure>
<ol>
<li>计算 pose blend shape offset，如果输入的 pose 是旋转向量，那先需要转成旋转矩阵，去掉第一个点，因为是 root，至于为什么旋转矩阵要再减去一个 identity matrix，暂时不清楚</li>
</ol>
<img src="/2020/09/16/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-SMPL-A-Skinned-Multi-Person-Linear-Model/4.png" class="">
<ol>
<li>计算旋转后的关节点，输入有旋转矩阵，rest pose 下的 joint，每个 joint 的parent<ul>
<li>rel_joints: relative joints，指的是每个关节点其父节点指向自己的向量，维度是 (b, 24, 3)</li>
<li>transforms_mat：变换矩阵，上面提到的未连乘起来的 $G$，旋转矩阵的维度是 (b, 24, 3, 3)，和 relative joint 一起拼成 (b, 24, 4, 4) 的齐次变换矩阵</li>
<li>transform_chain：这里实现的应该是每个节点只有一个 parent 节点，所以 transform_chain 实际就是将 transform_mat 中的每个关节点的变换矩阵乘上其父节点的变换矩阵</li>
<li>posed_joints：不是很懂为什么 transform_mat 相乘后会将 rel_joints 直接也进行变换，但可能就是会得到变换后的点吧</li>
<li>rel_transform：不是很懂</li>
</ul>
</li>
</ol>
<img src="/2020/09/16/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-SMPL-A-Skinned-Multi-Person-Linear-Model/5.png" class="">
<img src="/2020/09/16/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-SMPL-A-Skinned-Multi-Person-Linear-Model/6.png" class="">
<ol>
<li>最后一步就是 skinning，<ul>
<li>首先将 blend weight 扩展成维度 (b, 6890, 24)</li>
<li>A 矩阵就是上面函数得到的 rel_transform 变换矩阵，blend weight 乘以变换矩阵得到 T</li>
<li>vposed_homo 就是将 vertices 转成齐次坐标，然后和 T 矩阵相乘得到最后变换后的 vertices，取前三个回到正常坐标。</li>
</ul>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># W is N x V x (J + 1)</span></span><br><span class="line">W = lbs_weights.unsqueeze(dim=<span class="number">0</span>).expand([batch_size, -<span class="number">1</span>, -<span class="number">1</span>])</span><br><span class="line"><span class="comment"># (N x V x (J + 1)) x (N x (J + 1) x 16)</span></span><br><span class="line">num_joints = J_regressor.shape[<span class="number">0</span>]</span><br><span class="line">T = torch.matmul(W, A.view(batch_size, num_joints, <span class="number">16</span>)) .view(batch_size, -<span class="number">1</span>, <span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">homogen_coord = torch.ones([batch_size, v_posed.shape[<span class="number">1</span>], <span class="number">1</span>], dtype=dtype, device=device)</span><br><span class="line">v_posed_homo = torch.cat([v_posed, homogen_coord], dim=<span class="number">2</span>)</span><br><span class="line">v_homo = torch.matmul(T, torch.unsqueeze(v_posed_homo, dim=-<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">verts = v_homo[:, :, :<span class="number">3</span>, <span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p><br></p>
]]></content>
  </entry>
  <entry>
    <title>【论文阅读】—— Revisiting Knowledge Distillation via Label Smoothing Regularization</title>
    <url>/2020/06/22/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Revisiting-Knowledge-Distillation-via-Label-Smoothing-Regularization/</url>
    <content><![CDATA[<blockquote>
<p>Title : Revisiting Knowledge Distillation via Label Smoothing Regularization<br>Authors : Li Yuan, et al<br>Conference : CVPR 2020<br>Abstract : 作者做了两组 demo 实验，一是用学生网络蒸馏教师网络，二是用未完全训练的教师网络（性能比学生网络还差）去蒸馏学生网络，两组实验的结果均有效。从这两组反常的实验中，作者提出了知识蒸馏的另一个解释：label smoothing regularization，并提出了两个 teacher-free 的知识蒸馏方法，实验表明均有效。</p>
</blockquote>
<span id="more"></span>
<p><br></p>
<h2 id="Exploratory-Experiments-and-Counterintuitive-Observations"><a href="#Exploratory-Experiments-and-Counterintuitive-Observations" class="headerlink" title="Exploratory Experiments and Counterintuitive Observations"></a>Exploratory Experiments and Counterintuitive Observations</h2><p>我们一般对知识蒸馏的印象是：通过一个复杂的教师网络，把教师网络的输出对学生网络进行指导，从而提升学生网络的性能。而对分类任务中，知识蒸馏的有效性解释一般为软标签中蕴含了类别之间的相关性信息，这是软标签对比于硬标签的优势。</p>
<p>这里作者探索了两种特殊的知识蒸馏设定</p>
<ul>
<li>学生网络指导教师网络，叫做 reversed knowledge distillation (Re-KD)</li>
<li>poorly-trained 的教师网络指导学生网络，这里的 poorly-trained 指的是教师网络只训练了几个迭代，结果远比学生网络差，叫做 defective knowledge distillation (De-KD)</li>
</ul>
<img src="/2020/06/22/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Revisiting-Knowledge-Distillation-via-Label-Smoothing-Regularization/1.png" class="">
<p>按照我们对知识蒸馏的理解，上面这两种知识蒸馏都不会有效，可是作者进行了大量的实验证明，这两种蒸馏均是有效果的，这驱使作者去思考知识蒸馏背后的解释。</p>
<img src="/2020/06/22/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Revisiting-Knowledge-Distillation-via-Label-Smoothing-Regularization/2.png" class="">
<img src="/2020/06/22/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Revisiting-Knowledge-Distillation-via-Label-Smoothing-Regularization/3.png" class="">
<p><br></p>
<h2 id="Knowledge-Distillation-and-Label-Smoothing-Regularization"><a href="#Knowledge-Distillation-and-Label-Smoothing-Regularization" class="headerlink" title="Knowledge Distillation and Label Smoothing Regularization"></a>Knowledge Distillation and Label Smoothing Regularization</h2><p>上面的实验结果也反映了，知识蒸馏有效性的解释不止在于软标签中蕴含的类别相似性信息，因为学生网络或者 poorly-trained 的教师网络他们的软标签里面相似性信息肯定不足，并且甚至很多软标签本身分类就是错误的，这样的软标签竟然有用，作者给出的解释是，这里的软标签就类似于一种标签平滑的正则，下面先回顾一下 label smoothing regularization (LSR)。</p>
<p>在 LSR 里面，标签并不是一个 one-hot 的硬标签，而是经过一定的平滑，没有 LSR 的时候，网络的损失函数如下，其中 $q(k)$ 就是 one-hot 的硬标签。</p>
<script type="math/tex; mode=display">
H(q,p)=-\sum_{k=1}^Kq(k)log(p(k))</script><p>而 LSR 是把 $q(k)$ 进行平滑，加入了一个均匀分布的 $u(k)=1/K$， 使得</p>
<script type="math/tex; mode=display">
q^{\prime}(k)=(1-\alpha)q(k)+\alpha u(k)</script><p>此时的损失函数变为</p>
<img src="/2020/06/22/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Revisiting-Knowledge-Distillation-via-Label-Smoothing-Regularization/4.png" class="">
<p>由于 $H(u)$ 是一个常数</p>
<script type="math/tex; mode=display">
L_{LS}=(1-\alpha)H(q,p)+\alpha D_{KL}(u,p)</script><p>而知识蒸馏的设定里面，学生网络的损失函数同样包含两项，$\tau$ 是温度 temperature</p>
<script type="math/tex; mode=display">
L_{KD}=(1-\alpha)H(q,p)+\alpha D_{KL}(p_{\tau}^t, p_\tau)</script><p>对比 $L<em>{LS}$ 和 $L</em>{KD}$，唯一不同在于 $u$ 和 $p_\tau ^t$，我们可以将 KD 看成是一个特殊的 LSR，特殊点在于用于平滑的分布是一个学习出来的教师网络，而不是一个预先设定好的分布；或者我们可以将 LSR 看成一个特殊的知识蒸馏，$u$ 是一个虚拟的教师网络，对于每个类别都给出相同的概率。</p>
<p>从下面的可视化图看出，当增大温度，网络的输出就更加趋向于一个均匀分布，KD 也就越加取向 LSR</p>
<img src="/2020/06/22/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Revisiting-Knowledge-Distillation-via-Label-Smoothing-Regularization/5.png" class="">
<p>因此 Re-KD 和 De-KD 可以解释为这些 poorly-trained 的网络输出的软标签在高温度下能给学生网络提供一个正则的作用，因此上面的实验结果有效更多是在高温度的设定下进行的，作者在附录中也给出了参数，温度设为 20.</p>
<p><br></p>
<h2 id="Teacher-free-Knowledge-Distillation-Tf-KD"><a href="#Teacher-free-Knowledge-Distillation-Tf-KD" class="headerlink" title="Teacher-free Knowledge Distillation (Tf-KD)"></a>Teacher-free Knowledge Distillation (Tf-KD)</h2><p>基于上面的结论，软标签更多的作用在于正则项，而不是类别相似性，因此我们可以压根不需要教师网络来提供这样的一个正则项，作者提出了两种 Tf-KD，第一种为 $Tf-KD_{self}$，这个更多像 born again network，先训练一个网络，再用该网络作为教师网络，重新训练一个学术网络。</p>
<p>第二种为 $Tf-KD_{reg}$，通过自己设定了一个教师网络的输出，使得网络在正确类别上有着相当高的概率，这里的 $a$ 通常设置为 0.9 以上</p>
<img src="/2020/06/22/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Revisiting-Knowledge-Distillation-via-Label-Smoothing-Regularization/6.png" class="">
<p>同样的，两种设定下的温度系数都是 $\tau \ge 20$，使得软标签更像 LSR</p>
<p><br></p>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><img src="/2020/06/22/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Revisiting-Knowledge-Distillation-via-Label-Smoothing-Regularization/7.png" class="">
<img src="/2020/06/22/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Revisiting-Knowledge-Distillation-via-Label-Smoothing-Regularization/8.png" class="">
<p><br></p>
]]></content>
  </entry>
  <entry>
    <title>【论文阅读】—— VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION</title>
    <url>/2018/11/29/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-VERY-DEEP-CONVOLUTIONAL-NETWORKS-FOR-LARGE-SCALE-IMAGE-RECOGNITION/</url>
    <content><![CDATA[<blockquote>
<p>Title：VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION<br>Authors：Karen Simonyan, Andrew Zisserman<br>Session：ICLR, 2015<br>Abstract：VGGNet 代表了牛津大学的 Visual Geometry Group，该网络赢得了 ILSVRC 2014分类比赛的亚军、定位比赛的冠军。VGG 首次将网络深度提升到了 19 层，并且使用较小的卷积核，深刻印证网络深度对模型性能的影响。</p>
</blockquote>
<span id="more"></span>
<p><br></p>
<h1 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h1><p>ZFNet 和 OverFeat 两个网络都是将 AlexNet 的第一层卷积层进行了修改，将卷积核缩小以求挖掘更加细致的信息，这也说明了小卷积核的优势，因此 VGG 清一色的将卷积核设为 $3 \times 3$，并且所有的卷积操作都遵循 SAME 原则，即不改变 feature map 的维度，只改变 channel 数量；而降维操作均发生在池化层，同样清一色的采取 stride 为 2 的 $2 \times 2$ 的pooling，即每次 pooling 发生，feature map 的 height 和 width 就会减半。</p>
<p>而小卷积核对比大卷积核的优势究竟是什么呢？作者提到主要的优势有两点。</p>
<ol>
<li>从下图可以看出，串联两个 $3 \times 3$ 的卷积层和一个 $5 \times 5$ 的卷积层拥有相同的receptive field（感受野）；而串联三个 $3 \times 3$ 的卷积层和一个 $ 7 \times 7$ 的卷积层拥有相同的receptive field。虽然感受野相同，那么串联多个小卷积核的好处在于它带来更多的non-linearity，因为经过了多层的激活函数。</li>
</ol>
<img src="/2018/11/29/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-VERY-DEEP-CONVOLUTIONAL-NETWORKS-FOR-LARGE-SCALE-IMAGE-RECOGNITION/1.png" class="">
<ol>
<li>小卷积核会减少模型的参数量和计算量，由于 VGG 在卷积操作的时候不改变 channel 数，因此我们假设其为 C。串联三个 $3 \times 3$ 的卷积层总共的参数为 $3(3^2C^2)$，而一层 $7 \times 7$ 的卷积层就有 $7^2C^2$ 的参数，可见小而深的网络比宽而浅的网络参数要少。</li>
</ol>
<p>作者共涉及了 6 个 VGG网络，DE 版本就是我们常见的 VGG-16 和 VGG-19，不同网络间除了网络深度不同，其它完全一样。注意作者定义网络层数的时候，只算有参数的层，即卷积层和全连接层，池化层不算在内。里面提到 conv 均为 $3 \times 3$ 的 SAME 卷积，max-pooling 均为 stride为 2 的 $2\times 2$ 的池化。</p>
<img src="/2018/11/29/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-VERY-DEEP-CONVOLUTIONAL-NETWORKS-FOR-LARGE-SCALE-IMAGE-RECOGNITION/2.png" class="">
<p>A-LRN 是在 A 版本下加入了 AlexNet 的 LRN，但是实验结果表明，LRN并没有带来提升，反而会使得模型的计算量增大。C 版本尝试性地使用了两个 $1 \times 1$ 的卷积核，作者受到当时 “Network in Network” 中的启发，也使用了这样的卷积核，$1 \times 1$ 的卷积核可以看成是和每个 pixel 进行內积，它只挖掘一个 pixel 在不同 channel 之间的关系，而不去管一个 feature map 中不同 pixel 之间的 connection。</p>
<p><br></p>
<h1 id="训练和测试的细节"><a href="#训练和测试的细节" class="headerlink" title="训练和测试的细节"></a>训练和测试的细节</h1><h2 id="训练阶段"><a href="#训练阶段" class="headerlink" title="训练阶段"></a>训练阶段</h2><p>输入的图像是 $224 \times 224 \times 3$ 的 RGB 图像，其他参数，如 batch size、momentum、weight decay 基本和 AlexNet 一致。作者发现 VGG 收敛要比其它网络要快，分析可能的原因是 1）深度增加和卷积核变小带来的 implicit regualarization。2）某些层的预初始化。</p>
<p><strong>预初始化：</strong></p>
<ol>
<li>BCDE 版本网络的初始化均来自于 pre-trained 的版本 A 网络，由于网络 A 较小，因此收敛较快，先训练一个网络 A，然后将其训练好的参数作为后面网络的初始值（前面四个卷积层和最后的三个全连接层）</li>
<li>其它的参数进行随机初始化，从 $(0, 0.01)$ 的高斯分布中进行初始化。</li>
</ol>
<p><strong>数据增强</strong></p>
<ul>
<li><p>随机水平翻转</p>
</li>
<li><p>随机 RGB 颜色调整</p>
</li>
<li><p>scale jittering，原始图像首先会被调整到某一个尺度，如 $N\times S$，$S &lt; N$，然后我们再从中 crop 出 $224 \times 224$ 的图片作为网络的输入；如果 $S = 224$，那么这个 crop 就相当于把图像的全部信息作为输入了；而如果 $S &gt; 224$，那么输入图片为原图像的一部分。作者提出了两种方法来设定 $S$ 值。</p>
<ul>
<li>固定 $S = 256 \ or \  384$，256 是 AlexNet 和 ZFNet 使用的数值。</li>
<li>设定 $S \in [256, 512]$ 之间的随机数值。</li>
</ul>
<p>因此对于一张图像，首先将其调整到 $N \times S$，再将它随机裁剪出 $224 \times 224$ 的部分。</p>
</li>
</ul>
<h2 id="测试阶段"><a href="#测试阶段" class="headerlink" title="测试阶段"></a>测试阶段</h2><p>同样，先将测试图像调整到尺度 $N \times Q$，$Q &lt; N$，注意 $Q$ 不一定等于 $S$，两者没有任何关系。下面对比两种测试的方法。</p>
<p><strong>Dense Evaluation</strong></p>
<p>这种测试方法在 OverFeat 里面被提出来，在 OverFeat 中被称为 multi-scale classification，训练完网络后，将全连接层转成全卷积神经网络，从而令网络可以接收任意尺度的输入。输出会是一个 class score map，然后再处理这个 map，得到最后的预测结果。</p>
<p><strong>Multi-crop Evaluation</strong></p>
<p>这种测试方法在 AlexNet 中被提出来，在 AlexNet 中被称为 10 views，从测试图片中 crop 出多张 $224 \times 224$ 的图片，再进行水平镜像等操作后输入网络中，得到多个预测结果，取平均或最大作为最后的预测。</p>
<p>作者基于 dense evaluation 提出了两种评测方式</p>
<ul>
<li>single-scale evaluation：顾名思义，测试图片的尺度是固定的</li>
<li>multi-scale evaluation：选择多个 Q 值，从而对于一张图片产生多个测试图片。这里需要注意的是，Q 值的选择需要参考 S 值，如果 S 值是固定的数值，如 256 或者 384，那么 Q 值就取 $ {S - 32, S, S+ 32 }$ 三个数值；如果 S 是 $[256, 512]$ 中的随机数，那么 Q 值就取 ${ 256, 384,512 }$ 三个数值。</li>
</ul>
<p>multi-scale evaluation 会得到 3 个 Q 值 class score map，然后综合考虑，决定出最后的预测结果。</p>
<p><br></p>
<h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><h2 id="Single-scale-Evaluation"><a href="#Single-scale-Evaluation" class="headerlink" title="Single-scale Evaluation"></a>Single-scale Evaluation</h2><p>在单尺度评测下，Q 值选择和 S 值 相同，从实验结果可以看出</p>
<ul>
<li>LRN 没有什么作用</li>
<li>深度增大，准确率提高</li>
<li>$1 \times 1$ 的卷积核并没有提升网络性能</li>
<li>随机选择 S 值要比固定一个 S 值要好</li>
</ul>
<img src="/2018/11/29/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-VERY-DEEP-CONVOLUTIONAL-NETWORKS-FOR-LARGE-SCALE-IMAGE-RECOGNITION/3.png" class="">
<h2 id="Multi-scale-Evaluation"><a href="#Multi-scale-Evaluation" class="headerlink" title="Multi-scale Evaluation"></a>Multi-scale Evaluation</h2><p>在测试时候采取多尺度的评测（相当于 scale jittering），会提升网络的性能</p>
<img src="/2018/11/29/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-VERY-DEEP-CONVOLUTIONAL-NETWORKS-FOR-LARGE-SCALE-IMAGE-RECOGNITION/4.png" class="">
<h2 id="Dense-Evaluation-vs-Multi-crop-Evaluation"><a href="#Dense-Evaluation-vs-Multi-crop-Evaluation" class="headerlink" title="Dense Evaluation vs Multi-crop Evaluation"></a>Dense Evaluation vs Multi-crop Evaluation</h2><ul>
<li>对于 VGG-19，multi-crop 比 dense evaluation 要好一点</li>
<li>模型融合可以得到更好的结果</li>
</ul>
<img src="/2018/11/29/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-VERY-DEEP-CONVOLUTIONAL-NETWORKS-FOR-LARGE-SCALE-IMAGE-RECOGNITION/5.png" class="">
<img src="/2018/11/29/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-VERY-DEEP-CONVOLUTIONAL-NETWORKS-FOR-LARGE-SCALE-IMAGE-RECOGNITION/6.png" class="">
<p><br></p>
<p><em>In Conclusion</em></p>
<p>引用 CS231n 的一句话</p>
<blockquote>
<p>The runner-up in ILSVRC 2014 was the network from Karen Simonyan and Andrew Zisserman that became known as the VGGNet. Its main contribution was in showing that the depth of the network is a critical component for good performance. Their final best network contains 16 CONV/FC layers and, appealingly, features an extremely homogeneous architecture that only performs 3x3 convolutions and 2x2 pooling from the beginning to the end. Their pretrained model is available for plug and play use in Caffe. A downside of the VGGNet is that it is more expensive to evaluate and uses a lot more memory and parameters (140M). Most of these parameters are in the first fully connected layer, and it was since found that these FC layers can be removed with no performance downgrade, significantly reducing the number of necessary parameters.</p>
</blockquote>
<p>不难提炼出如下结论：</p>
<ol>
<li>深度提升性能；</li>
<li>最佳模型：VGG16，从头到尾只有3x3卷积与2x2池化。简洁优美；</li>
<li><p>开源pretrained model。与开源深度学习框架Caffe结合使用，助力更多人来学习；</p>
</li>
<li><p>卷积可代替全连接。整体参数达1亿4千万，主要在于第一个全连接层，用卷积来代替后，参数量下降。</p>
</li>
</ol>
<p><br><br><em>references</em></p>
<ol>
<li><a href="https://blog.csdn.net/qq_40027052/article/details/79015827">某博客</a></li>
<li><a href="http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture9.pdf">cs231n lecture 9 ppt</a></li>
<li><a href="http://www.robots.ox.ac.uk/~karen/pdf/ILSVRC_2014.pdf">VGG 会议 ppt</a></li>
</ol>
<p><br></p>
]]></content>
  </entry>
  <entry>
    <title>【论文阅读】—— Wide Residual Networks</title>
    <url>/2018/12/03/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Wide-Residual-Networks/</url>
    <content><![CDATA[<blockquote>
<p>Title：Wide Residual Networks<br>Authors：Sergey Zagoruyko, Nikos Komodakis<br>Session：BMVC, 2016<br>Abstract：传统的 ResNet 都是以深度为主，作者从宽度（channel 大小）出发，提出了通过拓宽网络的 channel 数，也能达到和深层 ResNet 一样的效果，且训练更快。</p>
</blockquote>
<span id="more"></span>
<p><br></p>
<p><em>preface</em></p>
<p>ResNet 的提出证明了深层网络的强大，但是随着网络不断地加深，有时候为了少量的精度需要把网络层数翻倍。且太深的网络会导致特征的重用减小，即梯度在反向传递的，只会从 skip-connection 中传递，而不会往 residual block 中走，只有很少的 block 能学到有用的信息，且 block 之间的共同信息很少。并且作者认为 ResNet 最强大的地方在于 residual block，而不在于深度，深度只是 ResNet 强大之处的一个补充而已。因此，<strong>作者从“宽度”的角度入手，提出了 WRN，16 层的 WRN 性能就比之前的 ResNet 效果要好</strong>。 </p>
<p><br></p>
<h1 id="WRN"><a href="#WRN" class="headerlink" title="WRN"></a>WRN</h1><p>下图的 basic 和 bottleneck 是原始 ResNet 提出的两种 residual block，bottleneck 会在深层的 ResNet 中用到，主要用来减少参数和计算量。注意一点，BN 和 ReLu 在卷积层之前，对应了 ResNet-v2 的 full pre-activation。</p>
<img src="/2018/12/03/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Wide-Residual-Networks/1.png" class="">
<p>作者基于 basic 的 residual block，引入两个因子：深度因子 $l$ 和 宽度因子 $k$，深度因子表示一个 residual block 里卷积层的层数，宽度因子表示卷积核的数量翻倍的倍数，如原来的卷积层为 $[3 \times 3, 16]$，表示 16 个 $3 \times 3$ 的卷积核，假如 $k = 2$，那么卷积层则为 $[3\times 3, 16 \times 2]$，即增加了卷积核的数量，拓宽了 feature map 的个数，或 channel 数。</p>
<p>可以看出，basic 的 residual block 对应 WRN 中 $l = 2$、$k = 1$，WRN 整体的网络结构如下</p>
<img src="/2018/12/03/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Wide-Residual-Networks/2.png" class="">
<p>作者再从下面几个方面去探索 WRN 的性能</p>
<ul>
<li>residual block 中卷积的类型：$B(M)$ 表示 block 中卷积的类型，如 $B(3,1)$ 表示先一个 $3 \times 3$ 的卷积，接着一个 $1 \times 1$ 的卷积</li>
<li>深度因子 $l$：在改变深度因子的时候，保持网络整体参数量不变，即增大深度因子，block 数量就减少。</li>
<li>宽度因子 $k$：WRN-n-k 表示网络共有 n 层，宽度因子为 k。</li>
<li>residual block 加入 dropout：在卷积之后加入 dropout。</li>
</ul>
<p><br></p>
<h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><p><strong>block 的卷积类型 &amp; 深度因子 $l$</strong></p>
<ul>
<li>参数量和准确率都差不多，因此选择 $B(3,3)$</li>
<li>不同深度，如 $B(3,3,3)$ 和 $B(3,3,3,3)$ 都不及 $B(3,3)$ </li>
</ul>
<img src="/2018/12/03/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Wide-Residual-Networks/3.png" class="">
<p><strong>residual block width</strong></p>
<ul>
<li>k 增大，错误率下降，depth 增大，错误率不一定会下降，可以看 WRN-40-8 与 WRN-22-8</li>
</ul>
<img src="/2018/12/03/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Wide-Residual-Networks/4.png" class="">
<p><strong>对比 ResNet</strong></p>
<ul>
<li>WRN 比 ResNet-1001 都要好 0.92%</li>
<li>且 WRN 训练要比 ResNet 快很多，尽管 WRN 的参数要比 ResNet 要多</li>
</ul>
<img src="/2018/12/03/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Wide-Residual-Networks/5.png" class="">
<p><strong>dropout的作用</strong></p>
<img src="/2018/12/03/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Wide-Residual-Networks/6.png" class="">
<p><br></p>
<p><em>In Conclusion</em></p>
<p>网络的深度和宽度都可以使得网络性能变好，之前的工作大多集中在加深网络层数，网络的宽度也是一个不可小看的角度。</p>
<p><br></p>
]]></content>
  </entry>
  <entry>
    <title>【论文阅读】—— Visualizing and Understanding Convolution Networks</title>
    <url>/2018/11/26/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Visualizing-and-Understanding-Convolution-Networks/</url>
    <content><![CDATA[<blockquote>
<p>Title：Visualizing and Understanding Convolutional Networks<br>Authors：Matthew D. Zeiler，Rob Fergus<br>Session：ECCV, 2014<br>Abstract：作者提出了一种可视化卷积神经网络的方法，并且根据可视化结果，将 AlexNet 网络参数进行调整，在 ImageNet 上的结果有明显提升，并且得到了 ILSVRC 2013 的冠军，并将网络取名 ZFNet（两位作者的名字首字母）。</p>
</blockquote>
<span id="more"></span>
<p><br></p>
<h1 id="AlexNet-as-prerequisite"><a href="#AlexNet-as-prerequisite" class="headerlink" title="AlexNet as prerequisite"></a>AlexNet as prerequisite</h1><p>AlexNet 是由 Alex Krizhevsky 提出并获得 ILSVRC 2012的冠军，网络结构如下，网络采用了5个卷积层和2个全连接层，并且用ReLu作为激活函数，采用了Dropout等正则手段。</p>
<img src="/2018/11/26/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Visualizing-and-Understanding-Convolution-Networks/AlexNet.png" class="">
<p><br></p>
<h1 id="ZFNet"><a href="#ZFNet" class="headerlink" title="ZFNet"></a>ZFNet</h1><p>ZFNet 在 AlexNet基础上进行了微妙的调整，将第一个卷积层的 filter 从 $11 \times 11$ 调整为 $7 \times 7$，并且将 stride 从 4 减小到 2，除此之外，ZFNet 和 AlexNet 完全一样，有人会觉得 ZFNet 就是将 AlexNet 做了一个调参工作，然后就拿了一个比赛冠军且发了一篇论文，事实确实是这样的。。</p>
<img src="/2018/11/26/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Visualizing-and-Understanding-Convolution-Networks/ZFNet.png" class="">
<p>但是这篇文章的主要贡献并不在于 ZFNet 的提出，而是在于作者是如何可视化卷积神经网络，从而找到调整 AlexNet 的方向，这才是这篇论文的精髓所在。</p>
<p><br></p>
<h1 id="Visualization-Approach"><a href="#Visualization-Approach" class="headerlink" title="Visualization Approach"></a>Visualization Approach</h1><p>作者想探索输入图片的每个 pixel 和网络中间的卷积层输出的联系，卷积层输出又称为 feature map，因此作者提出了一种能将中间卷积层的输出逆映射到输入空间的方法，采取的方法是deconvolution network。</p>
<p>正常的 convolution network 由卷积层、激活函数、池化层组成，deconvolution network要进行逆映射，将前向传递的过程逆转。deconvnet 包含三个部分</p>
<ul>
<li><p>Unpool：作者在 convnet 中采用的是 max-pooling，在进行反转的时候，我们需要知道最大值所在的位置，因此作者在 convnet 中保存了一个 switch，用来指示最大值所在的 pixel 位置。</p>
<img src="/2018/11/26/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Visualizing-and-Understanding-Convolution-Networks/unpool.png" class="">
</li>
<li><p>Rectification：convnet 中用的是 Relu 作为非线性激活函数，在 deconvnet 中同样采用 Relu。</p>
</li>
<li><p>Filtering：convnet 中用 filter 来对上一层的输出进行卷积操作，而 deconvnet 用转置的 filter 对 rectified unpooled map 进行卷积。</p>
</li>
</ul>
<img src="/2018/11/26/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Visualizing-and-Understanding-Convolution-Networks/deconvnet.png" class="">
<p><br></p>
<h1 id="Visualization-Discoveries"><a href="#Visualization-Discoveries" class="headerlink" title="Visualization Discoveries"></a>Visualization Discoveries</h1><h2 id="Feature-Visualization"><a href="#Feature-Visualization" class="headerlink" title="Feature Visualization"></a>Feature Visualization</h2><ul>
<li>对于每一层的 feature map，在数据集上选取激活值最强的 9 张图，画成一个九宫格。把它们映射回输入空间后可以看到不同结构的重建特征图（灰色图），以及这些特征图对应图像块（彩色图）。</li>
<li>灰色图并不是模型的样本，灰色图展示了使得该层激活值较大的图的pattern，也变相说明了每一个卷积层究竟在挖掘什么样的特征。</li>
<li>从结果可以看出，前面的层学习一些基本的形状，后面的层学习的内容更加高级，越多样性。</li>
</ul>
<img src="/2018/11/26/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Visualizing-and-Understanding-Convolution-Networks/1.png" class="">
<h2 id="Feature-Evolution-during-Training"><a href="#Feature-Evolution-during-Training" class="headerlink" title="Feature Evolution during Training"></a>Feature Evolution during Training</h2><ul>
<li>下图是随着训练的迭代，feature map 的变化，每一层的八列分别代表不同 epoch 时的特征图</li>
<li>可以看出，低层的 feature map 收敛较快，高层的收敛较慢，要到 40-50 个 epoch 才开始收敛。</li>
</ul>
<img src="/2018/11/26/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Visualizing-and-Understanding-Convolution-Networks/2.png" class="">
<h2 id="Architecture-Selection"><a href="#Architecture-Selection" class="headerlink" title="Architecture Selection"></a>Architecture Selection</h2><p>下面展示的是作者为什么调整 AlexNet 第一层的原因，可以看出作者调整过后的 feature map 要更加的清晰，意味着 Alexnet 的卷积核过大，忽略了一些细节信息。</p>
<img src="/2018/11/26/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Visualizing-and-Understanding-Convolution-Networks/3.png" class="">
<h2 id="Occlusion-Sensitivity"><a href="#Occlusion-Sensitivity" class="headerlink" title="Occlusion Sensitivity"></a>Occlusion Sensitivity</h2><ul>
<li>作者通过遮挡图片的不同位置，观察 feature map 以及最后分类结果的变化，从而推断模型是否真的学习到 object 的位置。</li>
<li>结果表明，模型知道物体的位置，因为当遮挡住物体的时候，分类结果迅速下降。</li>
<li>b 图展示的是第五层的激活数值，可以看出当遮住小狗的脸的时候，激活值迅速下降。</li>
<li>d、e 图和 b 图差不多，当遮住小狗的脸的时候，分类的正确率迅速下降。</li>
</ul>
<img src="/2018/11/26/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Visualizing-and-Understanding-Convolution-Networks/4.png" class="">
<p><br></p>
]]></content>
  </entry>
  <entry>
    <title>【论文阅读】——Be Your Own Teacher, Improve the Performance of Convolutional Neural Networks via Self Distillation</title>
    <url>/2019/11/08/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94Be-Your-Own-Teacher-Improve-the-Performance-of-Convolutional-Neural-Networks-via-Self-Distillation/</url>
    <content><![CDATA[<blockquote>
<p>Title：Be Your Own Teacher, Improve the Performance of Convolutional Neural Networks via Self Distillation<br>Author：Linfeng Zhang, et al.<br>Session：ICCV, 2019<br>Abstract：作者提出一种新的 self-distillation 的策略，用网络深层的特征和预测结果去监督浅层网络的学习。</p>
</blockquote>
<span id="more"></span>
<p><br></p>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p>作者的方法比较直观简单，如下框图，作者共提出了三种监督信息，全都是从网络深层传至网络浅层，网络共分为四块，每一块得到一个特征图，特征图后接全连接网络和 softmax 得到预测结果，作者称其为浅层 classifier 和深层 classifier。三种监督信息分别是</p>
<ul>
<li>特征监督，深层的特征图对比浅层特征图。</li>
<li>标签监督，网络最后 softmax 输出的软标签对比前面 3 个浅层分类器得到的软标签。</li>
<li>真实数据监督，one-hot 对比各个分类器的输出。</li>
</ul>
<p>整体的 loss 如下，C 为分类器的个数，即将网络切分的个数。</p>
<script type="math/tex; mode=display">
loss = \sum_{i=1}^C (1-\alpha)\cdot CrossEntropy(q^i,y)+\alpha \cdot KL(q^i,q^C)+\lambda \cdot ||f_i-F_C||_2^2</script><img src="/2019/11/08/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94Be-Your-Own-Teacher-Improve-the-Performance-of-Convolutional-Neural-Networks-via-Self-Distillation/1.png" class="">
<p><br></p>
<h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><p>作者展示了四个分类器的结果，以及集成的效果，对比 baseline 和 classifier 4/4，可见自蒸馏的效果。</p>
<img src="/2019/11/08/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94Be-Your-Own-Teacher-Improve-the-Performance-of-Convolutional-Neural-Networks-via-Self-Distillation/2.png" class="">
<p>对比其他特征蒸馏的方法，也是一致的提升。</p>
<img src="/2019/11/08/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94Be-Your-Own-Teacher-Improve-the-Performance-of-Convolutional-Neural-Networks-via-Self-Distillation/3.png" class="">
<p><br></p>
<h2 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h2><p>作者的另外一篇文章， SCAN: A Scalable Neural Networks Framework Towards Compact and Efficient Models，两者用的是同一个框架和训练思路，但是出发点不同，这篇文章的出发点在于自蒸馏，而 SCAN 出发点在于 adaptive computation，即自适应地去选择需要经过的网络层，如果浅层分类的结果还令人满意，就不再经过后面的网络，从而实现加速。网络结构基本一样，除了加入了注意力模块。</p>
<img src="/2019/11/08/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94Be-Your-Own-Teacher-Improve-the-Performance-of-Convolutional-Neural-Networks-via-Self-Distillation/4.png" class="">
<p>accuracy 和 acceleration的 tradeoff 如下。</p>
<img src="/2019/11/08/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94Be-Your-Own-Teacher-Improve-the-Performance-of-Convolutional-Neural-Networks-via-Self-Distillation/5.png" class="">
<p><br></p>
]]></content>
  </entry>
  <entry>
    <title>【论文阅读】——Learning Lightweight Lane Detection CNNs by Self Attention Distillation</title>
    <url>/2019/08/30/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94Learning-Lightweight-Lane-Detection-CNNs-by-Self-Attention-Distillation/</url>
    <content><![CDATA[<blockquote>
<p>Title：Learning Lightweight Lane Detection CNNs by Self Attention Distillation<br>Author：Yuenan Hou, Zheng Ma, et al.<br>Session：ICCV, 2019<br>Abstract：作者提出了一种新的蒸馏方式：自注意力机制蒸馏网络，该蒸馏不需要教师网络作为监督信息，而是将自身网络深层的 attention map 作为浅层的监督信息，从而指导浅层网络的学习。</p>
</blockquote>
<span id="more"></span>
<p><em>preface</em></p>
<p>本篇论文将聚焦 Lane Detection task，该任务用于自动驾驶领域，给定一张图片，需要判断出有多少条路径，且路径的位置在哪，本质上是一个图像分割的任务。而为了降低分割的难度，在网络中设计出另外一条 path 来判断图像中是否存在 lane。</p>
<p><br></p>
<h2 id="Self-Attention-Distillation-SAD"><a href="#Self-Attention-Distillation-SAD" class="headerlink" title="Self-Attention Distillation(SAD)"></a>Self-Attention Distillation(SAD)</h2><p>类似 attention transfer 论文，提出了三种 attention map 的计算方式</p>
<ul>
<li>$G<em>{sum}(A_m)=\sum</em>{i=1}^{C_m}|A_m|$</li>
<li>$G<em>{sum}^p(A_m)=\sum</em>{i=1}^{C_m}|A_m|^p$</li>
<li>$G<em>{max}^p(A_m)=\max</em>{i=1,C_m}|A_m|^p$</li>
</ul>
<p>网络的整体框架如下，AT-GEN 模块将计算到的 attention map 再经过 bilinear upsampling 后，通过一个 spatial softmax。</p>
<img src="/2019/08/30/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94Learning-Lightweight-Lane-Detection-CNNs-by-Self-Attention-Distillation/1.png" class="" width="650" height="650">
<p>在原有的分割损失上，加上自注意力蒸馏损失，$M$ 为网络层数。</p>
<script type="math/tex; mode=display">
L_{distill}=\sum_{m=1}^{M-1}L(AT-GEN(A_m),AT-GEN(A_{m+1}))</script><p>训练过程为先无蒸馏情况下，预训练网络，然后再加入 SAD，下面为一些可视化结果。</p>
<img src="/2019/08/30/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94Learning-Lightweight-Lane-Detection-CNNs-by-Self-Attention-Distillation/2.png" class="" width="650" height="650">
<p><br></p>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>下图为 SAD 在 TuSimple 和 BDD100K 两个数据集的结果，SAD 都取得最好的结果。</p>
<img src="/2019/08/30/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94Learning-Lightweight-Lane-Detection-CNNs-by-Self-Attention-Distillation/3.png" class="" width="500" height="500">
<img src="/2019/08/30/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94Learning-Lightweight-Lane-Detection-CNNs-by-Self-Attention-Distillation/4.png" class="" width="500" height="500">
<p>一些 ablation study 如下，作者比较不同的 path，比如让 block1 去 mimic block4，或者 block2 去 mimic block3等。实验结果表明，浅层网络最好不用 SAD，因为浅层网络挖掘低层特征，不需要深层网络信息的指导。而近邻 mimic 要比跨层 mimic 要好。</p>
<img src="/2019/08/30/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94Learning-Lightweight-Lane-Detection-CNNs-by-Self-Attention-Distillation/5.png" class="" width="500" height="500">
<p>作者还尝试了在不同时间点引入 SAD，虽然最后收敛的结果都近似，但是预训练越久，attention map 越能表征信息，对于 SAD 的作用就越大，收敛也就越快。 </p>
<img src="/2019/08/30/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94Learning-Lightweight-Lane-Detection-CNNs-by-Self-Attention-Distillation/6.png" class="" width="500" height="500">
<p><br></p>
]]></content>
  </entry>
  <entry>
    <title>【论文阅读】——Distilling Knowledge From a Deep Pose Regressor Network</title>
    <url>/2019/08/28/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94Distilling-Knowledge-From-a-Deep-Pose-Regressor-Network/</url>
    <content><![CDATA[<blockquote>
<p>Title：Distilling Knowledge From a Deep Pose Regressor Network<br>Author：Muhamad Risqi U. Saputra, et al.<br>Session：ICCV, 2019<br>Abstract：作者发现知识蒸馏都基本应用在分类任务上，而回归任务的特殊性导致软标签不 work，因此作者提出自适应的蒸馏法来解决 visual odemetry 任务。</p>
</blockquote>
<span id="more"></span>
<p><em>Motivation</em></p>
<p>Knowledge Distillation(KD) 提出来后，绝大多数的研究都在分类任务上，KD 中提到的 dark knowledge 描述了分类任务中不同类别之间的相关性，因此即使教师网络误分类了某个样本，由于软标签的存在，也能给学生网络提供一定的指导信息。</p>
<p>而在回归任务上，神经网络输出的是连续的，假如教师网络对于某个样本的输出和真实标签发生了较大的区别，那么这个错误的样本对于学生网络就不存在任何的指导作用，不像分类任务的软标签那样。而考虑到回归任务的输出可能是一个无法预知的分布（分类任务的输出是封闭的，各个样本的概率，0-1之间的数值），因此错误所带来的代价可能会很大。</p>
<p>我们无法确保教师网络对于任何样本的输出都是合理的正确的，因此作者提出了一种自适应的知识蒸馏方法，针对不同的样本，学生网络需要判断是否需要接受教师网络的指导。</p>
<p><br></p>
<h2 id="Blending-Teacher-Student-and-Imitation-Loss"><a href="#Blending-Teacher-Student-and-Imitation-Loss" class="headerlink" title="Blending Teacher, Student and Imitation Loss"></a>Blending Teacher, Student and Imitation Loss</h2><p>T 指教师网络，S 指学生网络，GT 指真实标签</p>
<ul>
<li>Teacher Loss: MSE(T, GT)</li>
<li>Student Loss: MSE(S, GT)</li>
<li>Imitation Loss: Loss(T, S)</li>
</ul>
<p>作者提出了多种监督标签(supervised label)的方法：</p>
<ul>
<li>Minimum of student and imitation：对于每个样本，选择 student loss 和 imitation loss 中较小的</li>
</ul>
<script type="math/tex; mode=display">
L_{reg}=\frac{1}{n}\sum_{t=1}^nmin(||p_S-p_{gt}||^2, ||p_S-p_T||^2)</script><ul>
<li>Imitation Loss as an additional loss：对于每个样本，既有 student loss，又有 imitation loss</li>
</ul>
<script type="math/tex; mode=display">
L_{reg}=\frac{1}{n}\sum_{t=1}^n \alpha||p_S-p_{gt}||^2+(1-\alpha)||p_S-p_T||^2</script><ul>
<li>Teacher loss as an upper bound：NIPS2017一篇目标检测的 KD 文章提出，教师损失作为上界，即对于某个样本，教师损失比学生损失还要大的时候，就不允许教师网络指导学生网络，也比较合情合理。</li>
</ul>
<script type="math/tex; mode=display">
L_{reg}=\frac{1}{n}\sum_{t=1}^n \alpha||p_S-p_{gt}||^2+(1-\alpha)L_{imit}</script><script type="math/tex; mode=display">
L_{imit}=\left\{
\begin{aligned}
||p_S-p_T||^2 & & if \ ||p_S-p_{gt}||^2>||p_T-p_{gt}||^2\\
0 && otherwise\\
\end{aligned}
\right.</script><ul>
<li>Probabilistic imitation loss：用教师网络和学生网络的条件概率作为 imitation loss，假设该概率服从拉普拉斯分布，也可以假设为高斯分布。</li>
</ul>
<script type="math/tex; mode=display">
P(p_S|p_T,\sigma)=\frac{1}{2\sigma}exp\frac{-||p_S-p_T||}{\sigma}</script><script type="math/tex; mode=display">
L_{imit}=-logP(p_S|p_T,\sigma)=\frac{||p_S-p_T||}{\sigma}+log\sigma+const</script><ul>
<li>Attentive imitation loss(AIL)：作者提出的 AIL 自适应地选择是否信任教师网络，或者信任的程度是多少。该信任程度定义为 $\Phi_i$，用教师损失来定义，教师损失越大，信任程度越低。在整个数据集范围内，计算每个样本的教师损失，并且根据损失为权重确定信任程度。</li>
</ul>
<img src="/2019/08/28/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94Distilling-Knowledge-From-a-Deep-Pose-Regressor-Network/1.png" class="" width="500" height="500">
<p><br></p>
<h2 id="Learning-Intermediate-Representations"><a href="#Learning-Intermediate-Representations" class="headerlink" title="Learning Intermediate Representations"></a>Learning Intermediate Representations</h2><p>仿照 Hint Training(HT)，从教师网络的中间层抽取某一层的输出作为监督信息指导学生网络训练，同样采用自适应的方法，加入信任程度 $\Phi_i$，记为 attentive hint training(AHT)。 </p>
<img src="/2019/08/28/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94Distilling-Knowledge-From-a-Deep-Pose-Regressor-Network/2.png" class="" width="500" height="500">
<p><br></p>
<h2 id="训练细节"><a href="#训练细节" class="headerlink" title="训练细节"></a>训练细节</h2><p>训练分为两个步骤：</p>
<ol>
<li>先用监督特征 $L_{hint}$ 进行训练网络的前面层</li>
<li>固定监督特征 $L_{hint}$ 所训练的层，再用监督标签进行训练</li>
</ol>
<p><br></p>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><h3 id="监督标签的有效性"><a href="#监督标签的有效性" class="headerlink" title="监督标签的有效性"></a>监督标签的有效性</h3><p>对比提出的五种监督标签的损失函数，发现 AIL 的效果最好</p>
<img src="/2019/08/28/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94Distilling-Knowledge-From-a-Deep-Pose-Regressor-Network/3.png" class="" width="500" height="500">
<h3 id="监督特征的有效性"><a href="#监督特征的有效性" class="headerlink" title="监督特征的有效性"></a>监督特征的有效性</h3><p>对比没有监督特征，加了监督特征后显著提升（提升得很多，有点夸张）。而用 AHT 的比 HT 的效果也有一定提升。</p>
<img src="/2019/08/28/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94Distilling-Knowledge-From-a-Deep-Pose-Regressor-Network/4.png" class="" width="500" height="500">
<h3 id="对比其它知识蒸馏方法"><a href="#对比其它知识蒸馏方法" class="headerlink" title="对比其它知识蒸馏方法"></a>对比其它知识蒸馏方法</h3><img src="/2019/08/28/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94Distilling-Knowledge-From-a-Deep-Pose-Regressor-Network/5.png" class="" width="500" height="500">
<p><br></p>
]]></content>
  </entry>
  <entry>
    <title>【论文阅读】——MotioNet, 3D Human Motion Reconstruction from Monocular Video with Skeleton Consistency</title>
    <url>/2020/08/30/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94MotioNet-3D-Human-Motion-Reconstruction-from-Monocular-Video-with-Skeleton-Consistency/</url>
    <content><![CDATA[<blockquote>
<p>Title: MotioNet: 3D Human Motion Reconstruction from Monocular Video with Skeleton Consistency<br>Author: Mingyi Shi, et al<br>Conference: ToG, 2020(transaction of graphics)<br>Abstract: 作者提出了一种 data-driven 的 3 维人体运动重构模型，该模型很好地解决了脚贴地、绝对位移等问题。</p>
</blockquote>
<span id="more"></span>
<p><em>prefix</em></p>
<p>3D 人体重构方面主要分为两类，一种是预测 pose，另一种是预测 shape。首先 3D pose estimation 的任务是预测 3D 的关节点，它只能作为 human motion reconstruction 的一个子任务，因为 3D 的关节点并没有关节旋转的信息，比方说，相同的一套 3D 关节点可以对应着无数个真正的姿势，因为关节点的内旋不会导致关节点物理位置的移动，并且由于预测上误差的存在，会导致骨骼的长短发生抖动。</p>
<p>3D 人体重构一般分为两种方法，一种是从 3D joint position 中预测出关节点的旋转；另外一种是 model-based，比如基于著名的 SMPL 人体蒙皮模型，用网络去预测 SMPL 模型的参数。</p>
<p>本文基于的方法是第一种方法，从 2D joint position 中预测关节点旋转、root 移动、骨骼长度、脚部是否接触地面，不基于任何人体模型，完全 data-driven 的。</p>
<p><br></p>
<h4 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h4><p>这里只讲一下整个网络的推理过程，下面的是网络结构，2D 的关节点作为输入，作者用的是 openpose 的结果，并且会将关节点的 confidence 组成向量输入到网络，注意网络的输入是一串时间 T 内的 keypoint。</p>
<img src="/2020/08/30/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94MotioNet-3D-Human-Motion-Reconstruction-from-Monocular-Video-with-Skeleton-Consistency/1.png" class="">
<p>然后分为两个 encoder，如下图，$E_Q$ 采取的是 temporal 的卷积，然后分成三条 branch，分别预测每一帧的 joint rotations、global root position 和 foot contact labels；而 $E_S$ 首先会将 T 个时间的输入整合到一起再采取普通的卷积操作，输出 bone length，因为对于一个人来说，骨骼长度是固定的，并不需要时间的信息，也不会每一帧会有不同的骨骼长度，所以也不需要 temporal 卷积。输出的 bone length 相当于预测了 SMPL 里面的 shape 参数，表征一个人的高矮胖瘦，然后会用预测的 bone 生成一个 T-pose。</p>
<img src="/2020/08/30/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94MotioNet-3D-Human-Motion-Reconstruction-from-Monocular-Video-with-Skeleton-Consistency/2.png" class="">
<p>FK(forward kinematics) 模块的输入是 T-pose、joint rotation、root position，$\tilde{s}_{init} \in R^{3J}$ 相当于这个人的初始 pose，J 为关节点数，然后每一帧，将 T-pose 根据 joint rotation $\tilde{q}^t \in R^{4J}$ 进行旋转，关节旋转用四元数表示。</p>
<img src="/2020/08/30/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94MotioNet-3D-Human-Motion-Reconstruction-from-Monocular-Video-with-Skeleton-Consistency/3.png" class="">
<p>对于每一帧，J 个关节点遵循从 root 到 leaf 的规则，$\tilde{P}<em>n^t=\tilde{P}^t</em>{parent(n)}+R^t_n\tilde{s}_n$，$\tilde{P}_n^t \in R^3$ 是第 n 个关节点在时间 t 的位置，关节 n 的 parent 指的是骨骼关系里的 parent，比如手肘是手腕的 parent， $R_n^t$ 是第 n 个关节点的旋转矩阵，$\tilde{s}_n \in R^3$ 是第 n 个关节点相对其 parent 的offset，手腕的位置等于手肘的位置加上旋转乘以手腕相对手肘的位置偏移</p>
<img src="/2020/08/30/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94MotioNet-3D-Human-Motion-Reconstruction-from-Monocular-Video-with-Skeleton-Consistency/4.png" class="">
<p>最后再加上根节点的移动</p>
<img src="/2020/08/30/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94MotioNet-3D-Human-Motion-Reconstruction-from-Monocular-Video-with-Skeleton-Consistency/5.png" class="">
<p>上面是整个推理流程，在训练阶段，从网络图看出是有一个判别器的，主要用来判别动作是否真实，但它并不是直接拿 joint rotation 输入到判别器，而是拿相邻 rotation 的差值，相当于求了一个差分。其他的 loss 包括 joint rotation loss、bone length loss、root position loss、foot contact loss</p>
<p><br></p>
<p>模型结果可以看下论文主页和油管视频</p>
<ul>
<li><a href="https://rubbly.cn/publications/motioNet/">https://rubbly.cn/publications/motioNet/</a></li>
<li><a href="https://www.youtube.com/watch?time_continue=2&amp;v=8YubchlzvFA&amp;feature=emb_logo">https://www.youtube.com/watch?time_continue=2&amp;v=8YubchlzvFA&amp;feature=emb_logo</a></li>
</ul>
<p><br></p>
]]></content>
  </entry>
  <entry>
    <title>【论文阅读】——OpenPose, Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields</title>
    <url>/2020/08/30/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94OpenPose-Realtime-Multi-Person-2D-Pose-Estimation-using-Part-Affinity-Fields/</url>
    <content><![CDATA[<blockquote>
<p>Title: OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields<br>Author: Zhe Cao<br>Conference: CVPR 2017（后扩展为期刊）<br>Abstract: 作者提出了一种实时的 2D 关节点检测的算法，并将关节点扩展为手部、脚部、脸部共 135 个点。</p>
</blockquote>
<span id="more"></span>
<h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><p>openpose 是一种 bottom-up 的方法，所谓的 bottom-up，就是从一张图片中先找出所有的关节点，然后再对关节点进行重组，这种方法不受图片中人数的影响，runtime 比较固定，因此可以做到实时，openpose 整体的流程如下。整张图片作为输入，经过神经网络会输出两种图，一种是 heatmap（part confidence maps，openpose里面将 keypoint 称为 part），另一种是骨骼连接图（part affinity fields，也就是连接不同关节点之间的骨骼），然后再对得到的关节点进行 match 和 parse。</p>
<img src="/2020/08/30/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94OpenPose-Realtime-Multi-Person-2D-Pose-Estimation-using-Part-Affinity-Fields/1.png" class="">
<h4 id="Confidence-Maps"><a href="#Confidence-Maps" class="headerlink" title="Confidence Maps"></a>Confidence Maps</h4><p>在 top-down 方法里面，ground-true 的关节点的热力图是以关节点为原点的一个二维高斯分布，热力图最大值的点即为关节点；而对于 bottom-up 的方法，一张图片会有多个人，因此对于某一个关节点，如脖子，热力图上会有多个 peak，是多个高斯分布的叠加，这里不同高斯分布采取的是取最大值的做法，防止关节点隔得太近，所以不能取平均，$S^{\ast}_j $ 表示的是第 j-th 个关节点的 GT 热力图</p>
<img src="/2020/08/30/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94OpenPose-Realtime-Multi-Person-2D-Pose-Estimation-using-Part-Affinity-Fields/2.png" class="">
<h4 id="Part-Affinity-Fields-for-Part-Association"><a href="#Part-Affinity-Fields-for-Part-Association" class="headerlink" title="Part Affinity Fields for Part Association"></a>Part Affinity Fields for Part Association</h4><p>假设一个人体有 17 个关节点，那么 17 个点，就会有 16 根连线，这里的一根连线对应一个 part affinity field，PAF 是一个 $w\times h \times 2$ 的矩阵，每个 pixel 对应一个 2D 向量，指示连线的方向。对于 limb c 上的点，PAF 为一个指示单位向量。至于如何找到 limb c 上的点，作者连接两个关节点成一条线，距离该线在一定范围内的点即为 limb c 上的点。一张图会有 k 个人，所以会有 k 个 limb c，因此最终对于 limb c 的 GT PAF $L^\ast _c$ 是单独一个人 PAF 的平均。 </p>
<img src="/2020/08/30/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94OpenPose-Realtime-Multi-Person-2D-Pose-Estimation-using-Part-Affinity-Fields/3.png" class="">
<img src="/2020/08/30/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94OpenPose-Realtime-Multi-Person-2D-Pose-Estimation-using-Part-Affinity-Fields/4.png" class="">
<p>测试的时候，会预测出一堆的关节点，以及一堆的 limb，就比如图1的例子，两个人，预测出两个肩膀点和两个手肘点，这里可以自由组合出 4 个 limb，然后预测的 PAF 会有两条 limb，如果会用组合的 4 个 limb 去对预测的两条 limb 做积分，来算一个能量值，若组合出来的某一条 limb 的方向和预测的方向几乎一致，那么这个能量值就会很高，反之很低，然后就会用这个能量值作为 keypoint parsing 的依据</p>
<img src="/2020/08/30/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94OpenPose-Realtime-Multi-Person-2D-Pose-Estimation-using-Part-Affinity-Fields/5.png" class="">
<h4 id="network-architecture"><a href="#network-architecture" class="headerlink" title="network architecture"></a>network architecture</h4><p>在原 openpose 论文里面（cvpr会议），网络结构是采取 simultaneous 的形式，分两条支路分别预测 part confidence 和 part affinity field，但后面改成期刊后，采取了 serial detection and association，先预测 limb 再预测 part。</p>
<img src="/2020/08/30/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94OpenPose-Realtime-Multi-Person-2D-Pose-Estimation-using-Part-Affinity-Fields/6.png" class="">
<p>首先图像经过一个 VGG-19 作为特征提取器，得到特征图 F，特征图后有先经过 $T_P$ 个 stage 的 limb 预测，每个 stage 的预测上一个 stage 的预测结果以及特侦图 F 作为输入</p>
<img src="/2020/08/30/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94OpenPose-Realtime-Multi-Person-2D-Pose-Estimation-using-Part-Affinity-Fields/7.png" class="">
<p>$T_P$ 个 stage 的 part affinity field 后是 $T_C$ 个 stage 的 part confidence maps，预测关节点的时候会将预测的 PAF 作为输入</p>
<img src="/2020/08/30/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94OpenPose-Realtime-Multi-Person-2D-Pose-Estimation-using-Part-Affinity-Fields/8.png" class="">
<p>网路训练引入了 intermediate supervision，每个 stage 的输出都用来进行监督</p>
<img src="/2020/08/30/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94OpenPose-Realtime-Multi-Person-2D-Pose-Estimation-using-Part-Affinity-Fields/9.png" class="">
<h4 id="Multi-person-Parsing-using-PAFs"><a href="#Multi-person-Parsing-using-PAFs" class="headerlink" title="Multi-person Parsing using PAFs"></a>Multi-person Parsing using PAFs</h4><p>我们网络预测得到 candidate parts 和 candidate limbs，组合过程采取启发式的方法，对于一条 limb c，连接它的两个关节点分别是 $j<em>1$ 和 $j_2$，检测到的两种关节点数量为 $D</em>{j<em>1}$ 和 $D</em>{j_2}$，这两个数量不一定相等，因为存在遮挡或者半身情况，然后建立一个二分图，边的权重为上面提到的能量值，然后对于一条 limb c，我们需要找出一个使得边权重最大的二分图，然后这就是最可能的 limb。</p>
<p>对于整个 body，分别找出每条 limb 的组合，再拼接起来即为最终结果。</p>
<h4 id="foot-detection"><a href="#foot-detection" class="headerlink" title="foot detection"></a>foot detection</h4><p>openpose 将 coco 的 17 个点扩展成 25 个点，其中增加了 6 个脚部的点，脚部的数据集是部分 coco 数据集中标注出来的，共 1.5k 张。叫上脚部检测后，解决了遮挡的问题，左图的 b 和 c。</p>
<img src="/2020/08/30/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94OpenPose-Realtime-Multi-Person-2D-Pose-Estimation-using-Part-Affinity-Fields/10.png" class="">
<p>除此之外，openpose 还训练了手部和脸部关节点的检测器，整体效果如下</p>
<img src="/2020/08/30/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94OpenPose-Realtime-Multi-Person-2D-Pose-Estimation-using-Part-Affinity-Fields/11.png" class="">
<p><br></p>
]]></content>
  </entry>
  <entry>
    <title>【高级管理学】—— consumer theory</title>
    <url>/2019/10/04/%E3%80%90%E9%AB%98%E7%BA%A7%E7%AE%A1%E7%90%86%E5%AD%A6%E3%80%91%E2%80%94%E2%80%94-consumer-theory/</url>
    <content><![CDATA[<blockquote>
<p>课程名称：高级管理学<br>授课人：Ying Kong<br>本章梗概：介绍了消费者的相关理论，如效用函数，需求曲线，弹性等概念。</p>
</blockquote>
<span id="more"></span>
<h2 id="Preference"><a href="#Preference" class="headerlink" title="Preference"></a>Preference</h2><p>bundle：多个不同种类商品的集合，比如 bundle x 为 5 个商品 1 和 10 个商品 2 组成。</p>
<p>preference：对于不同的 bundle，消费者可以选择其中的一个，对于消费者，两个 bundle 之间会有选择偏好，这就是 preference。对于两个 bundle $x$ 和 $y$，他们之间有三种关系，（由于找不到相关的经济学符号，因此用数学符号来代替）</p>
<ul>
<li><p>strict preference：$x$ is more preferred than $y$, denoted as $x&gt;y$</p>
</li>
<li><p>weak preference：$x$ is as at least preferred as y, denoted as $x \geq y$</p>
</li>
<li><p>indifference：$x$ is exactly as preferred as y, denoted as $x \sim y$</p>
</li>
</ul>
<p>preference relations</p>
<ul>
<li><p>$(x \geq y) \ and\ (y\geq x)\ \rightarrow \ (x\sim y)$</p>
</li>
<li><p>$(x \geq y)\ and \ (not\  y \geq x) \ \rightarrow \ (x &gt; y)$</p>
</li>
<li><p>For any two bundles $x$ and $y$, $(x \geq y)\ or \ (y \geq x)$</p>
</li>
<li><p>Any bundle $x$ is always at least as preferred as itself, $x \geq x$</p>
</li>
<li><p>$(x \geq y) \  and \ (y \geq x) \ \rightarrow \ (x \geq z)$</p>
</li>
</ul>
<p><br></p>
<h2 id="Indifference-Curve"><a href="#Indifference-Curve" class="headerlink" title="Indifference Curve"></a>Indifference Curve</h2><p>假设一个 bundle 为 $x$，indifference curve 指的是所有可能的 bundle $y$ 使得 $y \sim x$，即 ${y|y\sim x}$，因此也叫 indifference set。当我们的 bundle 只考虑两种商品 $x_1$ 和 $x_2$ 时，indifference curve 为二维平面上的一条线，如下图：</p>
<img src="/2019/10/04/%E3%80%90%E9%AB%98%E7%BA%A7%E7%AE%A1%E7%90%86%E5%AD%A6%E3%80%91%E2%80%94%E2%80%94-consumer-theory/1.png" class="" width="500" height="500">
<p>一条曲线代表一条 indifference curve，curve 上的任意两个点 $x = (x_1,x_2)$ 和 $y = (x_1^\prime, x_2^\prime)$ 都满足 $x\sim y$，而不同的两条线上的点必定满足 strict preference 的关系，越往右上方的点，越会被 preferred，因此 $l_1$ 上的点都 strict preferred 于 $l_2$ 上的点。</p>
<p><em>Note：</em>任意两条 indifference curve 必定是无交点的</p>
<h3 id="Two-Extreme-Cases-of-Indifference-Curve"><a href="#Two-Extreme-Cases-of-Indifference-Curve" class="headerlink" title="Two Extreme Cases of Indifference Curve"></a>Two Extreme Cases of Indifference Curve</h3><h4 id="Perfect-Substitute"><a href="#Perfect-Substitute" class="headerlink" title="Perfect Substitute"></a>Perfect Substitute</h4><p>同样是考虑两种商品组成的 bundle，如果消费者认为商品一和商品二是等同的，那么这两个商品就是 perfect  substitute，由于两者等同，因此在比较不同 bundle 哪个更加被 preferred 的时候，只需考虑两种商品的总数量即可。indifference curve 如下，不同的 curve 的斜率均为 -1。</p>
<img src="/2019/10/04/%E3%80%90%E9%AB%98%E7%BA%A7%E7%AE%A1%E7%90%86%E5%AD%A6%E3%80%91%E2%80%94%E2%80%94-consumer-theory/2.png" class="" width="500" height="500">
<h4 id="Perfect-Complement"><a href="#Perfect-Complement" class="headerlink" title="Perfect Complement"></a>Perfect Complement</h4><p>商品一和商品二总是以固定的比例出现，比如对于一辆小汽车，总是 4 个车轮和 1 个发动机，因此商品一和商品二的比例总是 4:1，即使一个 bundle 里面有再多的车轮或者再多的发动机，我们也只能考虑能够成对的数量，作为是否被 preferred 的依据 (only the number of pairs of units of the two commodities determines the preference rank-order of bundles)</p>
<p>下面的例子中，商品一和商品二总是以 1:1 的比例出现，因此 (9,5)、(5,5) 和 (5,9) 都是 equally preferred，因为他们都有 5 pairs，而 (9,9) 则 strict preferred 于前面的三个点，因为它有 9 pairs，indifference curve 的形状为直角。</p>
<img src="/2019/10/04/%E3%80%90%E9%AB%98%E7%BA%A7%E7%AE%A1%E7%90%86%E5%AD%A6%E3%80%91%E2%80%94%E2%80%94-consumer-theory/3.png" class="" width="500" height="500">
<h3 id="Marginal-Rate-of-Substitution-MRS"><a href="#Marginal-Rate-of-Substitution-MRS" class="headerlink" title="Marginal Rate-of-Substitution(MRS)"></a>Marginal Rate-of-Substitution(MRS)</h3><p>一条 indifference curve 的斜率就叫做 MRS，它所表示的意思是，我们愿意牺牲多少数量的商品一，去换取商品二，从而使得 equally preferred。对于 perfect substitute 来说，其 MRS 均为 -1，因为两者等同，因此少一个商品二，就会多一个商品一。</p>
<p><br></p>
<h2 id="Utility-Function"><a href="#Utility-Function" class="headerlink" title="Utility Function"></a>Utility Function</h2><p>在前面，我们一直用 preference 来衡量两个 bundle 之间的关系，而 utility function（效用函数） 就是用来代表 preference 的</p>
<ul>
<li><p>若 $x &gt; y$，则 $U(x) &gt; U(y)$ （前者是 strict preference 符号，后者是数学符号）</p>
</li>
<li><p>若 $x \sim y$，则 $U(x) = U(y)$</p>
</li>
</ul>
<p>因此一条 indifference curve 上的所有点，我们都可以说他们的效用是一样的。</p>
<p><em>Note：</em>效用值是一个 ordinal 的概念，即只关注两个 bundle 间效用的大小关系，至于效用值具体数值并不是关键，比如 x 的效用值为 6，y 的效用值为 2，只能说明 x 比 y 更加 preferred，并不表示 x 比 y 三倍的 preferred。因此假设 x 比 y 更为 preferred，我们可以赋值给 $U(x)$ 和 $U(y)$ 任何值，只要满足 $U(x) &gt; U(y)$ 即可。</p>
<p>现在我们可以将 indifference curve 和 utility function 放在同一个坐标系里面，xOy 平面依旧是 indifference curve，z 维度表示效用值。</p>
<img src="/2019/10/04/%E3%80%90%E9%AB%98%E7%BA%A7%E7%AE%A1%E7%90%86%E5%AD%A6%E3%80%91%E2%80%94%E2%80%94-consumer-theory/4.png" class="" width="300" height="300">
<p>indifference map：所有的 indifference curve 的集合，其等同于 utility function。</p>
<p>几种不同的 utility function：</p>
<ul>
<li><p>perfect substitute：$U(x_1,x_2)=x_1+x_2$</p>
</li>
<li><p>perfect complement：$U(x_1,x_2)=min(k_1x_1,k_2x_2)$</p>
</li>
<li><p>quasi-linear：$U(x_1,x_2)=f(x_1)+x_2$</p>
</li>
<li>Cobb-Douglas：$U(x_1,x_2)=x_1^ax_2^b$，indifference curve 都是双曲线状</li>
</ul>
<h3 id="Marginal-Utility"><a href="#Marginal-Utility" class="headerlink" title="Marginal Utility"></a>Marginal Utility</h3><p>The marginal utility of commodity $i$ is the rate-of-change of total utility as the quantity of commodity $i$ consumed changes.</p>
<script type="math/tex; mode=display">
MU_i=\frac{\partial U}{\partial x_i}</script><p><strong>Marginal Utility &amp; Marginal Rate-of-Substitution</strong></p>
<p>考虑具体的一条 indifference curve：$U(x_1, x_2)=k$</p>
<p>全微分后得：</p>
<script type="math/tex; mode=display">
\frac{\partial U}{\partial x_1}dx_1+\frac{\partial U}{\partial x_2}dx_2=0</script><p>化简后得到 MRS 和 MU 的关系</p>
<script type="math/tex; mode=display">
MRS=\frac{dx_2}{dx_1}=-\frac{\partial U/\partial x_1}{\partial U/\partial x_2}=-\frac{MU_1}{MU_2}</script><p><strong>Monotonic transformation &amp; MRS</strong> </p>
<p>假设 $V=f(U)$，对于新的效用函数 $V$，其对应的 MRS 为</p>
<script type="math/tex; mode=display">
MRS=-\frac{\partial V/\partial x_1}{\partial V/\partial x_2}=-\frac{f^\prime(U) \times\partial U/\partial x_1}{f^\prime(U) \times\partial U/\partial x_2}=-\frac{\partial U/\partial x_1}{\partial U/\partial x_2}</script><p>因此 MRS 在经过一个正单向变换后，并不会改变</p>
<p><br></p>
<h2 id="Optimal-Choice-and-Demand-Function"><a href="#Optimal-Choice-and-Demand-Function" class="headerlink" title="Optimal Choice and Demand Function"></a>Optimal Choice and Demand Function</h2><h3 id="Rational-Constrained-Choice"><a href="#Rational-Constrained-Choice" class="headerlink" title="Rational Constrained Choice"></a>Rational Constrained Choice</h3><p>作为消费者，我们当然想选择 utility 尽可能高的 bundle，但同时我们受商品价格以及预算的约束，假设商品一的价格为 $p_1$，商品二的价格为 $p_2$，预算为 $m$，那么我们选择的 bundle 必须满足 $p_1x_1+p_2x_2\leq m$，在这个约束下，我们再去考虑 utility 最大的 bundle。</p>
<p>假设 $p_1=p_2=1$，$m=4$，在约束下的为 affordable bundles，然后找到一条刚好与约束线相切的 indifference curve，相交点即为 most preferred affordable bundle，记为 $(x_1^{\ast},x_2^{\ast})$，又称为 ordinary demand。</p>
<img src="/2019/10/04/%E3%80%90%E9%AB%98%E7%BA%A7%E7%AE%A1%E7%90%86%E5%AD%A6%E3%80%91%E2%80%94%E2%80%94-consumer-theory/5.png" class="" width="300" height="300">
<p>对于 ordinary demand，其满足两个条件：</p>
<ul>
<li><p>预算用完：$p_1x_1^\ast+p_2x_2^\ast=m$</p>
</li>
<li><p>$(x_1^\ast,x_2^\ast)$ 处的斜率等于 constraint 的斜率 $-p_1/p_2$</p>
</li>
</ul>
<img src="/2019/10/04/%E3%80%90%E9%AB%98%E7%BA%A7%E7%AE%A1%E7%90%86%E5%AD%A6%E3%80%91%E2%80%94%E2%80%94-consumer-theory/6.png" class="" width="300" height="300">
<h3 id="几种不同效用函数下的-ordinary-demand"><a href="#几种不同效用函数下的-ordinary-demand" class="headerlink" title="几种不同效用函数下的 ordinary demand"></a>几种不同效用函数下的 ordinary demand</h3><h4 id="Cobb-Douglas"><a href="#Cobb-Douglas" class="headerlink" title="Cobb-Douglas"></a>Cobb-Douglas</h4><p>$U(x_1,x_2)=x_1^ax_2^b$，该效用函数的 indifference curve 为双曲线，就像上面的图，因此我们需要找到切点，用 MU 计算 MRS，让其等于约束方程的斜率，即可算到 ordinary demand 为</p>
<script type="math/tex; mode=display">
(x_1^*,x_2^*)=(\frac{am}{(a+b)p_1},\frac{bm}{(a+b)p_2})</script><h4 id="Perfect-Substitute-1"><a href="#Perfect-Substitute-1" class="headerlink" title="Perfect Substitute"></a>Perfect Substitute</h4><p>简单的通过图示法即可得到 ordinary demand，因为 perfect substitute 的 indifference curve 均为斜率 -1 的线，找到效用最大的即可。根据 $p_1$ 和 $p_2$ 的大小关系分为三种情况：</p>
<ul>
<li>$p_1 &gt; p_2$，$(x_1^\ast,x_2^\ast)=(0,\frac{m}{p_2})$</li>
<li>$p_1 &lt; p_2$，$(x_1^\ast,x_2^\ast)=(\frac{m}{p_1},0)$</li>
<li>$p_1 = p_2$，$(x_1^\ast,x_2^\ast)$ 为 budget constraint 上的点</li>
</ul>
<img src="/2019/10/04/%E3%80%90%E9%AB%98%E7%BA%A7%E7%AE%A1%E7%90%86%E5%AD%A6%E3%80%91%E2%80%94%E2%80%94-consumer-theory/7.png" class="" width="700" height="700">
<h4 id="Perfect-Complement-1"><a href="#Perfect-Complement-1" class="headerlink" title="Perfect Complement"></a>Perfect Complement</h4><p>同样用图示法去解，ordinary demand 必定在直角拐点处，因此可以得到 ordinary demand 为</p>
<script type="math/tex; mode=display">
(x_1^*,x_2^*)=(\frac{m}{p_1+ap_2},\frac{am}{p_1+ap_2})</script><img src="/2019/10/04/%E3%80%90%E9%AB%98%E7%BA%A7%E7%AE%A1%E7%90%86%E5%AD%A6%E3%80%91%E2%80%94%E2%80%94-consumer-theory/8.png" class="" width="500" height="500">
<h3 id="Demand-Function"><a href="#Demand-Function" class="headerlink" title="Demand Function"></a>Demand Function</h3><p>又叫 ordinary demand funtion，研究 $(x_1^\ast,x_2^\ast)$ 会随着 $p_1$、$p_2$ 和 $m$ 的变化而怎么变化，由于 $p_1$、$p_2$ 和 $m$ 会影响到 budget constraint，因此研究 demand function 可以让我们更快更直接的得到 ordinary demand。</p>
<p>在这里我们重点关注 own-price changes，比如在 $p_2$ 和 $m$ 不变的情况下，只改变 $p_1$，如下图，右边的即为 demand function，描述 $x_1^\ast$ 和 $p_1$ 之间的关系；</p>
<p>左边绿色的线是所有 ordinary demand 的连线，p1 price offer curve 指的是固定 $p_2$ 和 $m$， 只改变 $p_1$ 下的 ordinary demand 的变化线。因此可以从每个点中，算出对应的 $p_1$。<br><img src="/2019/10/04/%E3%80%90%E9%AB%98%E7%BA%A7%E7%AE%A1%E7%90%86%E5%AD%A6%E3%80%91%E2%80%94%E2%80%94-consumer-theory/9.png" class="" width="500" height="500"></p>
<p><br></p>
<h2 id="Market-Demand-amp-Elasticity"><a href="#Market-Demand-amp-Elasticity" class="headerlink" title="Market Demand &amp; Elasticity"></a>Market Demand &amp; Elasticity</h2><p>市场的 demand function 等于市场中每个消费者各自的 demand function 的和，即整个市场的需求量等于各个个体需求量之和</p>
<script type="math/tex; mode=display">
X_j(p_1,p_2,m^1,...,m^n)=\sum_{i=1}^nx_j^{*i}(p_1,p_2,m^i)</script><p>弹性的一般定义如下，研究的是一个变量 x 会随着另一个变量 y 如何变化</p>
<script type="math/tex; mode=display">
\epsilon_{x,y}=\frac{\%dx}{\%dy}=\frac{dx/x}{dy/y}</script><p><strong>Own-Price Elasticity：</strong>Own-Price Elasticity 指的是价格 $p$ 和需求 $x^\ast$ 之间的弹性，它是一个点概念，一般我们不说某一条 demand function 的弹性，而说 demand function 的某一点的弹性，因为市场中我们一般处在 demand function 中的某一点，我们需要关注的是在这一点的基础上，增加或减少价格所带来需求的变化。其定义如下</p>
<script type="math/tex; mode=display">
\epsilon_{X_i^*,p_i}=\frac{p_i}{X_i^*}\times\frac{dX_i^*}{dp_i}</script><p>demand function 中每个点的弹性可能都不一样，下面的例子中，根据弹性的大小分为三类：</p>
<ul>
<li>$-\infty &lt; \epsilon &lt; -1$，own-price elastic</li>
<li>$\epsilon=-1$，own-price unit elastic</li>
<li>$-1 &lt; \epsilon &lt; 0$，own-price inelastic</li>
</ul>
<img src="/2019/10/04/%E3%80%90%E9%AB%98%E7%BA%A7%E7%AE%A1%E7%90%86%E5%AD%A6%E3%80%91%E2%80%94%E2%80%94-consumer-theory/10.png" class="" width="500" height="500">
<p><strong>revenue &amp; own-price elasticity</strong></p>
<p>销售者的 revenue 为：$R(p)=p \times X^*(p)$</p>
<p>revenue 随着价格 $p$ 的变化率为：</p>
<script type="math/tex; mode=display">
\frac{dR}{dp}=X^*(p)+p\frac{dX^*}{xp}=X^*(p)(1+\frac{p}{X^*(p)}\frac{dX^*}{dp})=X^*(p)(1+\epsilon)</script><p>因此：</p>
<ul>
<li>$-\infty &lt; \epsilon &lt; -1$，$\frac{dR}{dq}&lt;0$，升价会导致销售者收入减少</li>
<li>$\epsilon=-1$，$\frac{dR}{dq}=0$，升价不会影响销售者收入</li>
<li>$-1 &lt; \epsilon &lt; 0$，$\frac{dR}{dq}&gt;0$，升价会导致销售者收入增加</li>
</ul>
<p>我们考虑需求函数的反函数 $p(q)$，其代表当销售者卖出 $q$ 件商品时的价格 $p$，因此 revenue 可以表示为 $R(q) = p(q)\times q$</p>
<p>同理 mariginal revenue 为：</p>
<script type="math/tex; mode=display">
MR(q)=\frac{dp(q)}{dq}q+p(q)=p(q)(q+\frac{q}{p(q)}\frac{dp(q)}{dq})=p(q)(1+\frac{1}{\epsilon})</script><p>因此：</p>
<ul>
<li>$-\infty &lt; \epsilon &lt; -1$，$MR(q)&gt;0$，需求增多会提高收入</li>
<li>$\epsilon=-1$，$MR(q)=0$，需求增多不会影响收入</li>
<li>$-1 &lt; \epsilon &lt; 0$，$MR(q)&lt;0$，需求增多会减少收入</li>
</ul>
<p><strong>Note：</strong>$\frac{dR}{dp}$ 不能称为 marginal revenue，所有边界的概念都是指增加一单位的物品所带来的另外变量的变化，因此 $dp(q)/dq$ 才是 marginal revenue，而 $dR/dp$ 只能叫做 revenue 随着价格的变化率。</p>
<p>两种表达方式所表述的东西是一样的，假设 $\epsilon &lt; -1$，那么增加一单位的价格带来的需求的减少量必定超过一单位，因此收入减少；而增加一单位的商品，价格的减少量是小于一单位的，因此收入增多。</p>
<p><br></p>
<h2 id="Consumer-Surplus"><a href="#Consumer-Surplus" class="headerlink" title="Consumer Surplus"></a>Consumer Surplus</h2><p>对于 demand curve $p_1=v^\prime(x_1)$，当消费者计划消费 $x_1^\prime$ 个商品一的时候，其消费者剩余为</p>
<script type="math/tex; mode=display">
CS=\int_0^{x_1^\prime}v^\prime(x_1)dx_1-p_1^\prime x_1^\prime</script><img src="/2019/10/04/%E3%80%90%E9%AB%98%E7%BA%A7%E7%AE%A1%E7%90%86%E5%AD%A6%E3%80%91%E2%80%94%E2%80%94-consumer-theory/11.png" class="" width="300" height="300">
<p>消费者剩余指的是：消费者本身对商品有一个预期的价格，而市场价格比预期价格要低，因此出现了剩余，从公式中可以看出，第二项为实际支付的钱，而第一项则为需求在 $x_1^\prime$ 下的该消费者的平均预期所需要支付的钱，我们可以将积分项拆开</p>
<script type="math/tex; mode=display">
\int_0^{x_1^\prime}v^\prime(x_1)dx_1=lim_{n\rightarrow \infty}\sum_{i=1}^nv^\prime(x_1)\frac{x_1^\prime}{n}=lim_{n\rightarrow \infty}\frac{1}{n}\sum_{i=1}^nv^\prime(x_1)x_1^\prime</script><p>$v^\prime(x_1)x_1^\prime$ 为价格为 $v^\prime(x_1)$ 下购买 $x_1^\prime$ 个商品所需要支付的钱，将不同价格下所需要支付的钱进行一个平均，即为该消费者平均预期所需要支付的钱。</p>
<p><br></p>
]]></content>
  </entry>
  <entry>
    <title>【高级管理学】—— producer theory</title>
    <url>/2019/10/16/%E3%80%90%E9%AB%98%E7%BA%A7%E7%AE%A1%E7%90%86%E5%AD%A6%E3%80%91%E2%80%94%E2%80%94-producer-theory/</url>
    <content><![CDATA[<blockquote>
<p>课程名称：高级管理学<br>授课人：Ying Kong<br>本章梗概：本章讲述生产者理论，从利润最大化，成本最小化角度进入，并介绍纯粹竞争以及垄断者模型，最后介绍博弈论模型。</p>
</blockquote>
<span id="more"></span>
<h2 id="Production-Function"><a href="#Production-Function" class="headerlink" title="Production Function"></a>Production Function</h2><p>生产者理论和消费者理论很接近，下表总结两者的异同</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">生产者理论</th>
<th style="text-align:center">消费者理论</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">production function: $y = f(x_1, …,x_n)$，，$x_i$ 为 input level，即每个输入单位数，$y$ 为输出的单位数</td>
<td style="text-align:center">utility function：$U=f(x_1,…,x_n)$</td>
</tr>
<tr>
<td style="text-align:center">y output unit isoquant：生产单位为 $y$ 的各种 input 组合</td>
<td style="text-align:center">indifference curve：效用为 U 的各种 bundle</td>
</tr>
<tr>
<td style="text-align:center">perfect substitute：$y=a_1x_1+…+a_nx_n$</td>
<td style="text-align:center">perfect substitute：$U=x_1+x_2$</td>
</tr>
<tr>
<td style="text-align:center">perfect complement：$y=min(a_1x_1,…,a_nx_n)$</td>
<td style="text-align:center">perfect complement：$U=min(k_1x_1,k_2x_2)$</td>
</tr>
<tr>
<td style="text-align:center">marginal product：增加一个单位的 $x_i$ 所带来的输出单位数的增加，$MP_i=\frac{\partial P}{\partial x_i}$</td>
<td style="text-align:center">marginal utility：增加一个单位的 $x_i$ 所带来的效用的增加，$MU_i = \frac{\partial U}{\partial x_i}$</td>
</tr>
<tr>
<td style="text-align:center">technical rate-of-substitution：isoquant 某个点的斜率，$TRS=-\frac{MP_1}{MP_2}$</td>
<td style="text-align:center">marginal rate-of-substitution(MRS)：indifference curve 某个点的斜率，$MRS=-\frac{MU_1}{MU_2}$</td>
</tr>
<tr>
<td style="text-align:center">long run：生产函数中每一个输入 $x_i$ 都可变，即生产过程不受任何输入约束；short run：生产过程至少一个输入变量为常数。</td>
</tr>
</tbody>
</table>
</div>
<p><br></p>
<h2 id="Profit-and-Cost-theory"><a href="#Profit-and-Cost-theory" class="headerlink" title="Profit and Cost theory"></a>Profit and Cost theory</h2><h3 id="Profit-Maximization"><a href="#Profit-Maximization" class="headerlink" title="Profit Maximization"></a>Profit Maximization</h3><p>生产者在生产过程需要注意的几个变量：</p>
<ul>
<li>输入原料：$x_1,…,x_m$</li>
<li>输出产品：$y_1,…,y_n$</li>
<li>输入原料价格：$w_1,…,w_m$</li>
<li>输出产品价格：$p_1,…,p_n$</li>
</ul>
<p>生产者利润 profit 的定义：</p>
<script type="math/tex; mode=display">
\Pi = p_1y_1 + ...+ p_ny_n - w_1x_1 - ... - w_mx_m</script><p>假设现在输入只有 $x_1$ 和 $x_2$，输出只有一个 $y$，并且输入为 long run，需要求利润 $\Pi$ 的最大值。问题中，变量为 $x_1, x_2,y$，常量为 $p,w_1,w_2$，利润为 $\Pi=py-w_1x_1-w_2x_2$，而 $y = f(x_1,x_2)$，因此变量减少为两个，分别对 $x_1,x_2$ 求偏导，并令其为 0，则可求得最大值。 </p>
<script type="math/tex; mode=display">
\frac{\partial \Pi}{\partial x_1}=p \times \frac{\partial y}{\partial x_1}-w_1=p \times MP_1 - w_1 = 0</script><script type="math/tex; mode=display">
\frac{\partial \Pi}{\partial x_2}=p \times MP_2 - w_2 = 0</script><p>可得</p>
<script type="math/tex; mode=display">
p\times MP_1=w_1, \ p\times MP_2=w_2</script><p>$p\times MP$ 为 marginal revenue，因此 profit 最大的时候，marginal revenue with respect to $x_i$ 等于 marginal cost with respect to $x_i$，这里的 marginal cost with respect to $x_i$ 代表的是增加一单位的输入$x_i$ 所增加的 cost，那明显是等于输入单价 $w_i$</p>
<h3 id="Cost-Minimization"><a href="#Cost-Minimization" class="headerlink" title="Cost Minimization"></a>Cost Minimization</h3><p>当生产者需要生产 $y$ （已知常数）个产品时，应该以怎样的输入组合才会实现花费最少？这个问题的解法思路和消费者理论中寻求在 budget 约束下的最大效用一样。</p>
<p>画出 y output unit isoquant，$cost=w_1x_1+w_2x_2$，这个问题就是寻找一个跟曲线相切的点，这个点即为成本最低的能生产 y 个产品的输入 bundle。</p>
<img src="/2019/10/16/%E3%80%90%E9%AB%98%E7%BA%A7%E7%AE%A1%E7%90%86%E5%AD%A6%E3%80%91%E2%80%94%E2%80%94-producer-theory/1.png" class="" width="500" height="500">
<h3 id="Cost-Function"><a href="#Cost-Function" class="headerlink" title="Cost Function"></a>Cost Function</h3><p>我们研究一个企业的开销成本，只考察 cost 和 output level $y$ 的关系，不考察其和 input level $x_i$ 的关系，因此 cost function 定义为 $c(y)$，其可以表示为以下形式</p>
<script type="math/tex; mode=display">
c(y)=F+c_v(y)</script><ul>
<li><p>$F$ 为生产过程中固定的费用，无论生产多少个产品都固定的费用</p>
</li>
<li><p>$c_v(y)$ 为 variable cost function，它随着 input level 而改变</p>
</li>
</ul>
<img src="/2019/10/16/%E3%80%90%E9%AB%98%E7%BA%A7%E7%AE%A1%E7%90%86%E5%AD%A6%E3%80%91%E2%80%94%E2%80%94-producer-theory/2.png" class="" width="500" height="500">
<p>将 output level 加入 cost function 可得 averaged cost function(AC 或 ATC)</p>
<script type="math/tex; mode=display">
AC(y)=\frac{c(y)}{y}=\frac{F}{y}+\frac{c_v(y)}{y}=AFC(y)+AVC(y)</script><p>随着 y 不断增大，AFC 会趋近于 0，AVC 趋近 AC</p>
<img src="/2019/10/16/%E3%80%90%E9%AB%98%E7%BA%A7%E7%AE%A1%E7%90%86%E5%AD%A6%E3%80%91%E2%80%94%E2%80%94-producer-theory/3.png" class="" width="500" height="500">
<h3 id="marginal-cost-function"><a href="#marginal-cost-function" class="headerlink" title="marginal cost function"></a>marginal cost function</h3><p>这里的边界成本指的时 output level 变化时所增加的 cost，而上面提到的 marginal cost with respect to $x_i$，是某个输入发生变化时的带来 cost 的变化，其为一个恒定的值，等于 $w_i$，与这里提到的 marginal cost 不同，marginal cost 的定义如下</p>
<script type="math/tex; mode=display">
MC(y)=\frac{\partial c(y)}{\partial y}=\frac{\partial c_v(y)}{\partial y}</script><p>由于 $MC(y)$ 为 $c_v(y)$ 的导数，那么 $c_v(y)$ 则为 $MC(y)$ 的不定积分</p>
<script type="math/tex; mode=display">
c_v(y^\prime)=\int_0^{y^\prime}MC(z)dz</script><p><strong>MC 和 AVC 的关系</strong></p>
<script type="math/tex; mode=display">
\frac{\partial AVC(y)}{\partial y}=\frac{y\times MC(y)-c_v(y)}{y^2}</script><ul>
<li><p>$\frac{\partial AVC(y)}{\partial y}&gt;0$，$MC(y)&gt;\frac{c_v(y)}{y}=AVC(y)$</p>
</li>
<li><p>$\frac{\partial AVC(y)}{\partial y}&lt;0$，$MC(y)&lt;\frac{c_v(y)}{y}=AVC(y)$</p>
</li>
<li>$\frac{\partial AVC(y)}{\partial y}=0$，$MC(y)=\frac{c_v(y)}{y}=AVC(y)$</li>
</ul>
<p>即 AVC 的最低点是其和 MC 的交点，当 AVC 上升的时候，MC 比它大，当 AVC 下降的时候，MC 比它小</p>
<p><strong>MC 和 ATC 的关系</strong></p>
<script type="math/tex; mode=display">
\frac{\partial ATC(y)}{\partial y}=\frac{y\times MC(y)-c(y)}{y^2}</script><ul>
<li><p>$\frac{\partial ATC(y)}{\partial y}&gt;0$，$MC(y)&gt;\frac{c(y)}{y}=ATC(y)$</p>
</li>
<li><p>$\frac{\partial ATC(y)}{\partial y}&lt;0$，$MC(y)&lt;\frac{c(y)}{y}=ATC(y)$</p>
</li>
<li>$\frac{\partial ATC(y)}{\partial y}=0$，$MC(y)=\frac{c(y)}{y}=ATC(y)$</li>
</ul>
<p>同样的结论，即 ATC 的最低点是其和 MC 的交点，当 ATC 上升的时候，MC 比它大，当 ATC 下降的时候，MC 比它小</p>
<img src="/2019/10/16/%E3%80%90%E9%AB%98%E7%BA%A7%E7%AE%A1%E7%90%86%E5%AD%A6%E3%80%91%E2%80%94%E2%80%94-producer-theory/4.png" class="" width="500" height="500">
<p><br></p>
<h2 id="Competition-theory"><a href="#Competition-theory" class="headerlink" title="Competition theory"></a>Competition theory</h2><h3 id="Pure-Competition"><a href="#Pure-Competition" class="headerlink" title="Pure Competition"></a>Pure Competition</h3><blockquote>
<p>Also regarded as perfect competition, pure competition is a situation in which the market for a product is populated with so many consumers and producers that no one entity has the ability to influence the price of the product sufficiently to cause a fluctuation. Within this type of market setting, sellers are considered to be price takers, indicating that they are not in a position to set the price for their products outside a certain range, given the fact that so many other producers are active within the market. At the same time, consumers have little influence over the prices offered by the producers, since there is no singular group of consumers that dominates the demand. </p>
</blockquote>
<p>一个企业对某样产品的市场价格没有影响，而这个产品的市场价格由市场上所有的公司共同决定，其为市场需求和市场供给的交点，如果一个企业将产品价格设高于市场价格，那么它的需求量为 0，因为没有人会买；而如果将产品价格设低于市场价格，那么该企业的需求量为市场的总需求量。</p>
<img src="/2019/10/16/%E3%80%90%E9%AB%98%E7%BA%A7%E7%AE%A1%E7%90%86%E5%AD%A6%E3%80%91%E2%80%94%E2%80%94-producer-theory/5.png" class="" width="500" height="500">
<p>因此我们可以得到一个企业对于某个产品的需求曲线如下，它是一条平行于 x 轴的直线，高度等于市场价格，意味着企业如果定价等于市场价格，它的需求是没有限制的，而如果价格高于市场价，需求为0。</p>
<img src="/2019/10/16/%E3%80%90%E9%AB%98%E7%BA%A7%E7%AE%A1%E7%90%86%E5%AD%A6%E3%80%91%E2%80%94%E2%80%94-producer-theory/6.png" class="" width="500" height="500">
<p>对于给定的市场价格 $p$，一个企业应该如何决定生产量 $y$ 使得利润最大化呢，问题定义为</p>
<script type="math/tex; mode=display">
max_{y\ge0}\Pi_s(y)=py-c_S(y)</script><p>对于利润 $\Pi_S(y)$，唯一自变量为 $y$，求利润最大值需要解下面两个条件</p>
<ul>
<li><p>一阶导数为 0，得出极值点，$\frac{d\Pi_S(y)}{dy}=p-MC_S(y)=0$</p>
</li>
<li><p>二阶导数小于 0，得出极大值点，$\frac{d^2\Pi_S(y)}{dy^2} = -\frac{dMC_S(y)}{dy}&lt;0$</p>
</li>
</ul>
<p>从上面条件，我们可以得出利润最大化的 $y_S^\ast$ 应该满足</p>
<ul>
<li>$MC_S(y)$ 与 p 的交点</li>
<li>交点处于 $MC_S(y)$ 的上升阶段</li>
</ul>
<p>上面的步骤只是求得利润的最大值，但并非最大值一定是正的，也可能为负值，当为负值的时候，证明企业在亏损，我们可以比较在交点处的 AC 值和 p 的大小关系，来判断取到最大值时，利润是否为正</p>
<script type="math/tex; mode=display">
\Pi_S(y)=p(y-\frac{C_S(y)}{y})=p(y-AC(y))</script><p>若 $AC(y_S^\ast) \le p$，则不亏， 若 $AC(y_S^\ast)=p$，我们称其为 breakeven price，保本价格，因为无亏损也无盈利，若 $AC(y_S^\ast)&gt;p$，则利润为负，再把成本拆开</p>
<script type="math/tex; mode=display">
\Pi_S(y)=p(y-\frac{C_v(y)}{y})-F=p(y-AVC(y))-F</script><p>若 $AVC(y_S^\ast)&lt;p&lt;AC(y_S^\ast)$，则企业勉强可以生存，虽然依旧亏损，但是起码能支付得起 $c_v(y)$</p>
<p>若 $AVC(y_S^\ast)&gt;p$，则企业需要关闭，称 $AVC(y_S^\ast)=p$ 的点为 breakdown price</p>
<p><br></p>
<h3 id="Producer-surplus-amp-Profit"><a href="#Producer-surplus-amp-Profit" class="headerlink" title="Producer surplus &amp; Profit"></a>Producer surplus &amp; Profit</h3><p>$PS=p\times y^\ast(p)-\int_0^{y^\ast(p)}MC_S(y)=revenue-c_v(y^\ast(p))$</p>
<p>$\Pi=revenue-c(y^\ast(p))=revenue-c_v(y^\ast(p))-F$</p>
<p>$PS=Profit+F$</p>
<img src="/2019/10/16/%E3%80%90%E9%AB%98%E7%BA%A7%E7%AE%A1%E7%90%86%E5%AD%A6%E3%80%91%E2%80%94%E2%80%94-producer-theory/7.png" class="" width="500" height="500">
<p><br></p>
<h3 id="Long-Run-Market-Equilibrium-Price"><a href="#Long-Run-Market-Equilibrium-Price" class="headerlink" title="Long-Run Market Equilibrium Price"></a>Long-Run Market Equilibrium Price</h3><p>假设 market demand 曲线固定不变，而 market supply 由市场中的企业所决定，假设一开始只有两个企业，那么市场价格为两条曲线的交点 $p_2$，在 $p_2$ 下企业会盈利，就会导致有第三个企业进入市场，这时候 market supply 曲线发生改变，市场价格 $p_3$ 也因此降低，但此时企业依旧盈利，第四个企业进入，市场价格为 $p_4$，此时企业亏损，有企业会离开市场。</p>
<img src="/2019/10/16/%E3%80%90%E9%AB%98%E7%BA%A7%E7%AE%A1%E7%90%86%E5%AD%A6%E3%80%91%E2%80%94%E2%80%94-producer-theory/8.png" class="">
<p>以此类推，当市场中的企业足够的多和小的话，那么 market supply 曲线将会是一条水平线，水平线的高度为 $min\ AC(y)$，所以 long run market price $p^e=min\ AC(y)$</p>
<img src="/2019/10/16/%E3%80%90%E9%AB%98%E7%BA%A7%E7%AE%A1%E7%90%86%E5%AD%A6%E3%80%91%E2%80%94%E2%80%94-producer-theory/9.png" class="" width="300" height="300">
<p><br></p>
<h2 id="Monopoly-theory"><a href="#Monopoly-theory" class="headerlink" title="Monopoly theory"></a>Monopoly theory</h2><h3 id="Pure-monopoly"><a href="#Pure-monopoly" class="headerlink" title="Pure monopoly"></a>Pure monopoly</h3><blockquote>
<p>A market in which one company has control over the entire market for a product, usually because of a barrier to entry such as a technology only available to that company.</p>
</blockquote>
<p>pure competition 中的企业都是 market price taker，而pure monopoly 的市场价格完全由垄断者所决定，因此垄断者的利润定义为</p>
<script type="math/tex; mode=display">
\Pi(y)=p(y)y-c(y)</script><p>价格由自己的 demand function 所决定</p>
<img src="/2019/10/16/%E3%80%90%E9%AB%98%E7%BA%A7%E7%AE%A1%E7%90%86%E5%AD%A6%E3%80%91%E2%80%94%E2%80%94-producer-theory/10.png" class="" width="500" height="500">
<p>要使得利润 $\Pi(y)$ 最大，求其对 output level $y$ 的偏导，利润最大值下的 output level $y^\ast$ 满足 marginal revenue 等于 marginal cost。</p>
<script type="math/tex; mode=display">
\frac{d \Pi(y)}{dy}=\frac{d}{dy}(p(y)y)-\frac{dc(y)}{dy}=MR(y)-MC(y)=0</script><p>例子：</p>
<ul>
<li><p>$p(y)=a-by$，$R(y)=p(y)y=ay-by^2$，$MR(y)=a-2by$</p>
</li>
<li><p>$c(y)=F+\alpha y+\beta y^2$，$MC(y)=\alpha+2\beta y$</p>
</li>
</ul>
<img src="/2019/10/16/%E3%80%90%E9%AB%98%E7%BA%A7%E7%AE%A1%E7%90%86%E5%AD%A6%E3%80%91%E2%80%94%E2%80%94-producer-theory/11.png" class="" width="600" height="600">
<p><br></p>
<h3 id="Monopolistic-Pricing-amp-Own-Price-Elasticity"><a href="#Monopolistic-Pricing-amp-Own-Price-Elasticity" class="headerlink" title="Monopolistic Pricing &amp; Own-Price Elasticity"></a>Monopolistic Pricing &amp; Own-Price Elasticity</h3><script type="math/tex; mode=display">
MR(y)=\frac{d}{dy}p(y)y=p(y)+y\frac{dp(y)}{dy}=p(y)(1+\frac{1}{\epsilon})</script><p>假设 $MC(y) = k$，$k$ 为一个常数，那么</p>
<script type="math/tex; mode=display">
p(y^\ast)=\frac{k}{1+\frac{1}{\epsilon}}</script><p>当 $\epsilon$ 往 -1 增大时，垄断者会调整 output level 使得市场价格上升，并且垄断者为了使得自己的 marginal revenue 为正值，会选择弹性商品，即 $\epsilon &lt; -1$</p>
<p><strong>markup pricing：</strong>市场价格和成本之差</p>
<script type="math/tex; mode=display">
markup = p(y^\ast)-MC(y^\ast)=\frac{k}{1+\frac{1}{\epsilon}}-k=-\frac{k}{1+\epsilon}</script><p>所以当 $\epsilon$ 往 -1 增大时，markup price 也随之增大</p>
<p><br></p>
<h2 id="Two-player-Game-Theory"><a href="#Two-player-Game-Theory" class="headerlink" title="Two-player Game Theory"></a>Two-player Game Theory</h2><p>A game consists of</p>
<ul>
<li>a set of players</li>
<li>a set of strategies for each player</li>
<li>the payoffs to each player for every possible choice of strategies by the players. Payoff matrix is always used to represent the relationship.</li>
</ul>
<p>我们只研究两个 player 且每个 player 两种决策的博弈论</p>
<h3 id="Pure-Strategy"><a href="#Pure-Strategy" class="headerlink" title="Pure Strategy"></a>Pure Strategy</h3><p>每个 player 只能从自己的决策选项中选择一个，考虑下面的例子</p>
<img src="/2019/10/16/%E3%80%90%E9%AB%98%E7%BA%A7%E7%AE%A1%E7%90%86%E5%AD%A6%E3%80%91%E2%80%94%E2%80%94-producer-theory/12.png" class="" width="500" height="500">
<p><strong>Nash Equilibrium(NE)：</strong>A play of the game where each strategy is a best reply to the other.</p>
<p>上面例子中 (U,L) 和 (D,R) 都是 NE，比如当 player A 选择 U 的时候，player B 肯定会选择 L，反过来一样。</p>
<p>一个 game 可能会有多个 NE，上面的例子，player A 和 B 是同时做选择的，称为 simultaneous play games，我们很难判断这两个 NE 哪个较好；而 sequential play games 通常是先后做选择，先做选择的为 leader，后作选择的为 follower，当选择有先后时，就比较容易判断哪个 NE 较好。下面的图表示如果 A 先做选择，(U,L) 会较好。</p>
<img src="/2019/10/16/%E3%80%90%E9%AB%98%E7%BA%A7%E7%AE%A1%E7%90%86%E5%AD%A6%E3%80%91%E2%80%94%E2%80%94-producer-theory/13.png" class="" width="500" height="500">
<h3 id="Mixed-Strategy"><a href="#Mixed-Strategy" class="headerlink" title="Mixed Strategy"></a>Mixed Strategy</h3><p>有些 game 如果只用 pure strategy 的话，可能得不到 NE，即无论 player A 怎么选择，player B 总能选出更好的，使得 player A 接着改变选择。而 Mixed strategy 下，player 可以以某个概率去选择某个 strategy，使得整体的 expected payoff 达到 NE。</p>
<p>下面的例子没有 pure strategy，但是有 Mixed strategy</p>
<img src="/2019/10/16/%E3%80%90%E9%AB%98%E7%BA%A7%E7%AE%A1%E7%90%86%E5%AD%A6%E3%80%91%E2%80%94%E2%80%94-producer-theory/14.png" class="" width="500" height="500">
<p>对于有限个玩家且玩家的决策有限的情况，至少会有一个 NE，如果没有 pure strategy NE，那么肯定有 mixed strategy NE。</p>
<p><br></p>
<h2 id="Best-Responses-amp-Nash-Equilibrium"><a href="#Best-Responses-amp-Nash-Equilibrium" class="headerlink" title="Best Responses &amp; Nash Equilibrium"></a>Best Responses &amp; Nash Equilibrium</h2><p>上面用 payoff matrix 来找 NE，下面将介绍 best response 的方法来寻找 NE。</p>
<h3 id="Pure-strategy"><a href="#Pure-strategy" class="headerlink" title="Pure strategy"></a>Pure strategy</h3><p>假设 player A 的两个 action 分别为 $a_1^A$ 和 $a_2^A$，player B 的两个 action 分别为 $a_1^B$ 和 $a_2^B$，payoff matrix 写成类似效用函数的形式</p>
<ul>
<li>$U^A(a_1^A,a_1^B)=6$ and $U^B(a_1^A,a_1^B)=4$</li>
<li>$U^A(a_1^A,a_2^B)=3$ and $U^B(a_1^A,a_2^B)=5$</li>
<li>$U^A(a_2^A,a_1^B)=4$ and $U^B(a_2^A,a_1^B)=3$</li>
<li>$U^A(a_2^A,a_2^B)=5$ and $U^B(a_2^A,a_2^B)=7$</li>
</ul>
<p>如果 A 选择 $a_1^A$，B 的 best response 为 $a_2^B$；如果 A 选择 $a_2^A$，B 的 best response 为 $a_2^B$。</p>
<p>如果 B 选择 $a_1^B$，A 的 best response 为 $a_1^A$；如果 B 选择 $a_2^B$，B 的 best response 为 $a_2^A$。</p>
<p>第一幅图为 B 选择变化时，A 的 best response；第二幅图为 A 选择变化时，B 的 best response，注意横纵坐标不同，将两幅图放在一起，交点即为 NE，即 $(a_2^A,a_2^B)$</p>
<img src="/2019/10/16/%E3%80%90%E9%AB%98%E7%BA%A7%E7%AE%A1%E7%90%86%E5%AD%A6%E3%80%91%E2%80%94%E2%80%94-producer-theory/15.png" class="">
<h3 id="Mixed-Strategy-1"><a href="#Mixed-Strategy-1" class="headerlink" title="Mixed Strategy"></a>Mixed Strategy</h3><p>假设 payoff matrix 如下</p>
<img src="/2019/10/16/%E3%80%90%E9%AB%98%E7%BA%A7%E7%AE%A1%E7%90%86%E5%AD%A6%E3%80%91%E2%80%94%E2%80%94-producer-theory/16.png" class="" width="500" height="500">
<p>$\pi^A_1$ 为 A 选择 $a_1^A$ 的概率，$1-\pi^A_1$ 为 A 选择 $a_2^A$ 的概率；$\pi^B_1$ 为 B 选择 $a_1^B$ 的概率，$1-\pi^B_1$ 为 B 选择 $a_2^B$ 的概率。假设给定 $\pi_1^B$，我们可以得到 A 选择不同 action 下的平均 payoff</p>
<img src="/2019/10/16/%E3%80%90%E9%AB%98%E7%BA%A7%E7%AE%A1%E7%90%86%E5%AD%A6%E3%80%91%E2%80%94%E2%80%94-producer-theory/17.png" class="" width="400" height="400">
<p>当给定 $\pi_1^B$ 时，A 的 best response 为</p>
<img src="/2019/10/16/%E3%80%90%E9%AB%98%E7%BA%A7%E7%AE%A1%E7%90%86%E5%AD%A6%E3%80%91%E2%80%94%E2%80%94-producer-theory/18.png" class="" width="400" height="400">
<p>对应的 best response curve 为</p>
<img src="/2019/10/16/%E3%80%90%E9%AB%98%E7%BA%A7%E7%AE%A1%E7%90%86%E5%AD%A6%E3%80%91%E2%80%94%E2%80%94-producer-theory/19.png" class="" width="400" height="400">
<p>同理，给定 $\pi_1^A$ 时，B 的情况</p>
<img src="/2019/10/16/%E3%80%90%E9%AB%98%E7%BA%A7%E7%AE%A1%E7%90%86%E5%AD%A6%E3%80%91%E2%80%94%E2%80%94-producer-theory/20.png" class="" width="400" height="400">
<p>B 的 best response curve 为</p>
<img src="/2019/10/16/%E3%80%90%E9%AB%98%E7%BA%A7%E7%AE%A1%E7%90%86%E5%AD%A6%E3%80%91%E2%80%94%E2%80%94-producer-theory/21.png" class="" width="400" height="400">
<p>找两个曲线的交点，即为 NE，有三个 NE，两个为 pure strategy NE，一个为 mixed strategy NE。</p>
<img src="/2019/10/16/%E3%80%90%E9%AB%98%E7%BA%A7%E7%AE%A1%E7%90%86%E5%AD%A6%E3%80%91%E2%80%94%E2%80%94-producer-theory/22.png" class="">
<p><br></p>
]]></content>
  </entry>
  <entry>
    <title>用hexo搭建个人网站</title>
    <url>/2018/08/14/%E7%94%A8hexo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99/</url>
    <content><![CDATA[<img src="/2018/08/14/%E7%94%A8hexo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99/header.jpg" class="" width="800" height="400">
<p>该文章简述如何使用hexo搭建个人博客</p>
<span id="more"></span>
<h1 id="1-hexo介绍"><a href="#1-hexo介绍" class="headerlink" title="1. hexo介绍"></a>1. hexo介绍</h1><p>hexo是基于Git和Node.js的静态网站搭建框架，通过hexo命令可以自动生成静态网站的html和css，然后可以将其部署于服务器，生成个人网站。<br><img src="/2018/08/14/%E7%94%A8hexo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99/hexo_homepage.png" class="" width="800" height="400"><br><br></p>
<h1 id="2-hexo本地安装与运行"><a href="#2-hexo本地安装与运行" class="headerlink" title="2. hexo本地安装与运行"></a>2. hexo本地安装与运行</h1><ol>
<li><p>安装Git和Node.js:   由于hexo是基于Git和Node.js的，所以在安装hexo之前，需要先安装<a href="https://git-scm.com/">Git</a>和<a href="https://nodejs.org/en/">Node.js</a>.</p>
</li>
<li><p>安装hexo：输入下面这条语句，就会自动地安装hexo。</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">$ npm install hexo-cli -g</span><br></pre></td></tr></table></figure>
</li>
<li><p>初始化blog： 输入下面这些语句，hexo会自动从网上拷贝所需要的dependency和package，生成一个project，存储在<code>Vincent-Hoo.github.io</code>这个文件夹里面，<code>Vincent-Hoo.github.io</code>是博客的名字，可以取其它的名字，但是由于我之后要部署到Github上，所以就取这个名字。</p>
 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ hexo init Vincent-Hoo.github.io</span><br><span class="line">$ cd Vincent-Hoo.github.io</span><br><span class="line">$ npm install</span><br></pre></td></tr></table></figure>
</li>
<li><p>在本地运行blog：输入以下的命令，hexo会自动生成一个静态网站在<a href="http://localhost:4000。">http://localhost:4000。</a></p>
 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p> 当目前为止，hexo已经为我们搭建好了一个个人的博客，里面有最基本的功能，如博客发表，搜索，导航，分享等。<code>Hello World</code>是hexo初始化时候自动生成的第一篇博客。</p>
 <img src="/2018/08/14/%E7%94%A8hexo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99/hexo%E7%BD%91%E7%AB%99%E5%88%9D%E5%A7%8B%E5%8C%96%E7%9A%84%E6%A0%B7%E5%AD%90.png" class="">
<p> <br></p>
</li>
</ol>
<h1 id="3-hexo框架的基本结构"><a href="#3-hexo框架的基本结构" class="headerlink" title="3. hexo框架的基本结构"></a>3. hexo框架的基本结构</h1><p>在你的hexo project初始化完之后，project文件夹的目录结构如下。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">|----- node_modules</span><br><span class="line">|----- scaffolds</span><br><span class="line">|----- source</span><br><span class="line">|      |----- _posts</span><br><span class="line">|      |----- _drafts</span><br><span class="line">|----- themes</span><br><span class="line">|----- _config.yml</span><br><span class="line">|----- db.json</span><br><span class="line">|----- package.json</span><br></pre></td></tr></table></figure>
<h2 id="3-1-scaffolds"><a href="#3-1-scaffolds" class="headerlink" title="3.1 scaffolds"></a>3.1 scaffolds</h2><p>首先，hexo支持三种layout</p>
<ul>
<li>post：对应source文件夹的<code>_posts</code>，我们所有的文章都放在这个文件夹里面。</li>
<li>draft：顾名思义，草稿，对应<code>source</code>文件夹的<code>_drafts</code>，draft一般而言都不会显示在博客上，如果要让draft显示，可以修改<code>_config.yml</code>文件的<code>render_draft</code>，将其设置为true（默认为false）。</li>
<li>page：一个博客不单止有文章，还会有网页，如个人介绍，个人简历。page也是保存在<code>source</code>文件夹里面。</li>
</ul>
<p>而scaffolds里面有三个文件，<code>post.md</code>, <code>draft.md</code>, <code>page.md</code>。当我们新建一个layout的时候，hexo就会根据我们新建的layout类型，选择相应的markdown文件进行初始化，所以这三个文件相当于是模板，下面以<code>post.md</code>作为例子，顺便讲一下什么是front-matter。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">title: &#123;&#123; title &#125;&#125;</span><br><span class="line">date: &#123;&#123; date &#125;&#125;</span><br><span class="line">tags:</span><br><span class="line">---</span><br></pre></td></tr></table></figure>
<p>模板最前面的key-value pair就是front-matter，下面列举了一些常见的front-matter</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">front-matter</th>
<th style="text-align:center">description</th>
<th style="text-align:center">default</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">layout</td>
<td style="text-align:center">post, draft, page三者之一</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">tags</td>
<td style="text-align:center">标签</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">categories</td>
<td style="text-align:center">类别</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">title</td>
<td style="text-align:center">文章标题</td>
<td style="text-align:center">默认是创建时候的标题</td>
</tr>
<tr>
<td style="text-align:center">date</td>
<td style="text-align:center">创建时间</td>
<td style="text-align:center">默认是创建时候的时间</td>
</tr>
</tbody>
</table>
</div>
<p>tag和categories可以很好地将博客的文章进行分类，设置的方式如下，注意categories具有层级关系，如Cat3和Cat3.1<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">title: Hello World</span><br><span class="line">tags:</span><br><span class="line">- Tag1</span><br><span class="line">- Tag2</span><br><span class="line">categories:</span><br><span class="line">- Cat1</span><br><span class="line">- Cat2</span><br><span class="line">- [Cat3, Cat3.1]</span><br></pre></td></tr></table></figure></p>
<h2 id="3-2-source"><a href="#3-2-source" class="headerlink" title="3.2 source"></a>3.2 source</h2><p>source文件夹是整个博客里面最重要的文件夹，一个博客几乎所有的内容都存储在里面，之前提过，有两个子文件夹<code>_posts</code>和<code>_drafts</code>，当然还会有一些page和用户自定义的数据。<br><code>_posts</code>文件夹存储所有博客中显示的文章，<code>_drafts</code>文件夹存储所有的草稿，如果要将草稿转成文章，可以使用下面的命令<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo publish &lt;draft_name&gt;</span><br></pre></td></tr></table></figure><br>创建文章或者草稿非常简单，只需要一条命令<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new &lt;layout, default: post&gt; &lt;article_name&gt;</span><br><span class="line">eg:</span><br><span class="line">$ hexo new draft Hello-World</span><br><span class="line">$ hexo new (post) Hello-World</span><br><span class="line">$ hexo new page About-me</span><br></pre></td></tr></table></figure></p>
<h2 id="3-3-themes"><a href="#3-3-themes" class="headerlink" title="3.3 themes"></a>3.3 themes</h2><p>hexo博客在初始化的时候默认是用landscape主题，当然我们可以自己去下载另外的一些主题(<a href="https://hexo.io/themes/">https://hexo.io/themes/</a> )。切换主题只需要将<code>_config.yml</code>文件里面的<code>theme</code>改成对应的主题名字即可。<br>当然，我们也可以DIY自己的主题，主题无非就是一些模板的CSS和HTML文件，还有一些脚本文件，定义了不同的地方需要如何去解析。而每一个的主题都会有自己的<code>_config.yml</code>，注意和全局的<code>_config.yml</code>区分，前者是定义主题里面的configuration，后者是定义整个博客。</p>
<h2 id="3-4-config-yml"><a href="#3-4-config-yml" class="headerlink" title="3.4 config.yml"></a>3.4 config.yml</h2><p>config文件定义整个博客的设置，如网站的title，author，还有目录设置，文章写作的设置，这里特别说一个设置<code>post_asset_folder</code>，asset_folder是指一篇文章的数据，如图片，如果设置为true，在创建一篇post的时候，不仅仅创建一个文章的markdown，还创建一个对应的文件夹。还有一个设置是<code>render_draft</code>，默认是设置为false的，因为草稿不会显示在网页上，但如果设置为true，那么草稿也会显示出来。<br><br></p>
<h1 id="4-Tag-Plugin"><a href="#4-Tag-Plugin" class="headerlink" title="4. Tag Plugin"></a>4. Tag Plugin</h1><p>hexo博客的文章都是以markdown的形式来保存，markdown的强大使得文章写作变得简单，但hexo的tag plugin使得写文章变得更加的简单，它通过一些特殊的语句来添加特定的内容到文章，如图像、视频等。</p>
<ol>
<li>引用别人的话block quote<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;% blockquote [author[, source]] [link] [source_link_title] %&#125;</span><br><span class="line">content</span><br><span class="line">&#123;% endblockquote %&#125;</span><br></pre></td></tr></table></figure></li>
<li>代码块code block<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;% codeblock [title] [lang:language] [url] [link text] %&#125;</span><br><span class="line">code snippet</span><br><span class="line">&#123;% endcodeblock %&#125;</span><br></pre></td></tr></table></figure></li>
<li>图片，图片的添加有两种方式，一种是以source文件夹为root的相对路径，一种是通过asset_folder。<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;% img [class names] /path/to/image [width] [height] [title text [alt text]] %&#125;</span><br><span class="line">eg: &#123;% img /images/xxx.jpg %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% asset_img slug [title] %&#125;</span><br><span class="line">eg: &#123;% asset_img xxx.jpg %&#125;</span><br></pre></td></tr></table></figure></li>
<li>视频，可以添加YouTube或者Vimeo的视频。<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;% youtube video_id %&#125;</span><br><span class="line">&#123;% vimeo video_id %&#125;</span><br></pre></td></tr></table></figure></li>
<li>引用asset_folder的资料，可以给出一个link，或者path。<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;% asset_path slug %&#125;</span><br><span class="line">&#123;% asset_img slug [title] %&#125;</span><br><span class="line">&#123;% asset_link slug [title] %&#125;</span><br></pre></td></tr></table></figure>
<br><h1 id="5-hexo常见命令"><a href="#5-hexo常见命令" class="headerlink" title="5. hexo常见命令"></a>5. hexo常见命令</h1></li>
<li>本地调式，如果想让草稿显示出来，可以加draft；如果想浏览器自动打开网页，可以加open。<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo server (--draft) (--open)</span><br></pre></td></tr></table></figure></li>
<li>生成静态网站文件，hexo是一个静态的博客系统，会根据文章、主题等生成静态网页的html(不同于动态网站，通过前后端进行数据交互)，执行这条命令，会生成一个public文件夹，这个文件夹就是整个静态网站的文件夹，可以将其部署到服务器上面。<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo generate(g)</span><br></pre></td></tr></table></figure></li>
<li><p>部署到远端服务器，详看下一节。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo deploy(d)</span><br></pre></td></tr></table></figure>
<p><br></p>
<h1 id="6-部署到Github-Pages"><a href="#6-部署到Github-Pages" class="headerlink" title="6. 部署到Github Pages"></a>6. 部署到Github Pages</h1><p>Github Pages可以被认为是用户编写的、托管在github上的静态网页。<a href="https://pages.github.com/">GitHub Pages</a>本用于介绍托管在GitHub的项目，不过，由于他的空间免费稳定，用来做搭建一个博客再好不过了。最重要的是，Github Pages能为用户免费提供服务器，因此部署静态网站就不需要自己搭建服务器和数据库了。</p>
<img src="/2018/08/14/%E7%94%A8hexo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99/github_pages_homepage.png" class="" width="800" height="400">
</li>
<li><p>创建一个项目的仓库，仓库名为username.github.io，每个账号只能有一个仓库来存放个人主页，可以通过<a href="http://username.github.io">http://username.github.io</a> 来访问你的个人主页。</p>
</li>
<li>配置<code>_config.yml</code>文件<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Deployment</span><br><span class="line">## Docs: https://hexo.io/docs/deployment.html</span><br><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repo: git@github.com:Vincent-Hoo/Vincent-Hoo.github.io.git</span><br><span class="line">  branch: master</span><br></pre></td></tr></table></figure></li>
<li>安装部署到git的包<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure></li>
<li>部署到GitHub上，hexo d命令会直接将public文件夹部署到github上对应的仓库，然后github pages会自动根据仓库的改变，部署网页。<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo g</span><br><span class="line">hexo d</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p><br></p>
<h1 id="7-nexT主题配置和优化"><a href="#7-nexT主题配置和优化" class="headerlink" title="7. nexT主题配置和优化"></a>7. nexT主题配置和优化</h1><p>每一个主题都有自己的说明文档，具体看config文件就可以将基本的博客元素配齐，需要优化的可以参考附录的链接。</p>
<h1 id="8-安装MathJax"><a href="#8-安装MathJax" class="headerlink" title="8. 安装MathJax"></a>8. 安装MathJax</h1><p>在 hexo 中，你会发现我们不能用 Latex 语法来书写数学公式，这对于书写学术博客来说是很大的不便，因为我们会经常碰到很多的数学公式推导，但是我们可以通过安装第三方库来解决这一问题。<br>可以参考该链接：<a href="https://blog.csdn.net/u014630987/article/details/78670258">https://blog.csdn.net/u014630987/article/details/78670258</a></p>
<p><br></p>
<h1 id="9-参考资料"><a href="#9-参考资料" class="headerlink" title="9. 参考资料"></a>9. 参考资料</h1><ol>
<li><a href="https://hexo.io">hexo主页</a></li>
<li>一些博客<br> 2.1 <a href="https://blog.csdn.net/gdutxiaoxu/article/details/53576018">https://blog.csdn.net/gdutxiaoxu/article/details/53576018</a><br> 2.2 <a href="https://www.cgmartin.com/2016/01/03/getting-started-with-hexo-blog/">https://www.cgmartin.com/2016/01/03/getting-started-with-hexo-blog/</a><br> 2.3 <a href="https://www.cnblogs.com/dushao/p/5999593.html">https://www.cnblogs.com/dushao/p/5999593.html</a></li>
<li><a href="https://www.zhihu.com/question/24422335">hexo主题选择</a></li>
<li><a href="http://ibruce.info/2015/04/04/busuanzi/">统计字数，阅读量等</a></li>
<li><a href="https://livere.com/">评论系统</a></li>
<li>视频教学系列<br> 6.1 <a href="https://www.youtube.com/watch?v=Ud1xAhu7t2Y&amp;list=PLXbU-2B80FvDjD_RiuNwsSQ4eF8pkwAIa&amp;index=1">https://www.youtube.com/watch?v=Ud1xAhu7t2Y&amp;list=PLXbU-2B80FvDjD_RiuNwsSQ4eF8pkwAIa&amp;index=1</a><br> 6.2 <a href="https://www.youtube.com/watch?v=Kt7u5kr_P5o&amp;list=PLLAZ4kZ9dFpOMJR6D25ishrSedvsguVSm">https://www.youtube.com/watch?v=Kt7u5kr_P5o&amp;list=PLLAZ4kZ9dFpOMJR6D25ishrSedvsguVSm</a></li>
<li>nexT主题优化<br> 7.1 <a href="https://youngerli.github.io/2017/12/02/Hexo%E7%9A%84Next%E4%B8%BB%E9%A2%98%E8%AF%A6%E7%BB%86%E9%85%8D%E7%BD%AE/">基本配置</a><br> 7.2 <a href="https://www.jianshu.com/p/3a05351a37dc">优化</a></li>
</ol>
<p><br></p>
]]></content>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>致逝去的三年研究生时光</title>
    <url>/2022/07/05/%E8%87%B4%E9%80%9D%E5%8E%BB%E7%9A%84%E4%B8%89%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%97%B6%E5%85%89/</url>
    <content><![CDATA[<img src="/2022/07/05/%E8%87%B4%E9%80%9D%E5%8E%BB%E7%9A%84%E4%B8%89%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%97%B6%E5%85%89/cover.jpeg" class="" width="500" height="500">
<p>2022年7月4日，星期一，阴雨，7月7日就要入职了，6月29日回家休息了几天，马上就要开始新的生活，结束了7年大学生活，也结束了19年的学生生涯，可以说出生到现在的25年里，有八成的时间都在学校做学生，马上就要拥有新的社会人身份了，感慨万千，还是像本科一样总结一下过去的三年研究生生活吧，对比本科的各种悲剧收尾，研究生还是有很多值得纪念的难忘瞬间的，下面从各个方面总结研究生的三年：学校学院篇、科研工作篇、课外活动篇、朋友篇。</p>
<span id="more"></span>
<p><br></p>
<h2 id="学校学院篇"><a href="#学校学院篇" class="headerlink" title="学校学院篇"></a>学校学院篇</h2><p>其实我现在能拿到清华大学的研究生毕业证和计算机一级学科的学位证，我应该感谢我的学院：清华-伯克利深圳学院，还有当时面试看上我的夏树涛老师。按照我本科的成绩，我应该是进不去清华的，当时我保研夏令营拿到的offer也就只有南京大学和中大，清华和北大的夏令营连入营资格都没有，邮件套辞也没有什么回复，因为我的成绩和简历根本不够。但当时在保研论坛给我注意到了清华-伯克利深圳学院（TBSI），一开始投简历的时候就觉得这个学院很international，各种材料都需要中英文两份，夏令营也基本都是英文展示，面试也是英文的，当时大三的我，是在出国和国内读研之间纠结的，但是遇到TBSI，当时就想那不是完美解决了我的纠结了吗，研一在国内读，研二去美国伯克利读，但后面问了一些当时在tbsi的师兄师姐了解了之后发现，其实没多少人去伯克利的，大多数也都是在深圳读三年，因为去伯克利是完全自己自费，并且拿到的学位也不是很有用的学位，好像是什么领导力学位啥的，上的课也都是很普通的课，就感觉含金量非常低。所以到了九月份的到最后决定的时候，我又还在南京大学和清华深研院之间纠结，当时我参加TBSI的夏令营面试，面试官之一就是夏树涛老师，当时我面试完就受到夏树涛老师的短信，说让我去他办公室聊一下，我当时就感到很奇怪，因为我在tbsi的官网里，并没有看到夏老师的信息，但不知道为啥他会出现在tbsi的面试，后来在我了解之后，原来夏老师和江勇老师是比较好的朋友，然后江勇老师是在tbsi有名额的，并且会把他在tbsi的学生交给夏老师来带，所以夏老师才会出现在tbsi的面试里面，而夏老师是计算机系在深研院的扛把子，所以选择tbsi的话，我就能拿到计算机的学位，也能认识计算机系的同学，并且清华的牌子也是南大比不了的，即使在深研院，在各种因素的驱动下，我最终就选择了清华，扎根在深研院，对于我这种广东人来说，有好有坏吧，好处是考虑到以后找工作大概率在珠三角这边，所以在深圳上学的话，实习比较方便，而坏处就是读了那么多年书，都没出过省，没在其他地方长时间生活过，多多少少也是一种遗憾吧，并且在深研院读，而不是在北京本部读，也少了一些归属感吧，本来想毕业典礼能够去北京的，也因为疫情的原因去不了，更是遗憾中的遗憾吧。</p>
<p>南国清华，坐落在深圳大学城，地铁7号线终点站西丽湖站，大学城由清华北大哈工大三所学校组成，这里真的好小，跟本科任何一个校区都没法比，宿舍饭堂实验室就在一块，没有特别多的活动（这个就很想吐槽了，隔壁北大的学生会真的很会办活动好吗，为什么咱们清华啥活动都没有呢，只能表示羡慕了）。再来谈谈tbsi吧，其实我比绝大多数人都要更早地了解到真正的tbsi，可能在外人看来tbsi很高级，毕竟是和伯克利一起办学的，吹嘘的很多东西都很高级：三个中心，三条track，双导师等等，但实际上伯克利的老师基本都是挂名而已，甚至很多国内的老师也只是挂名（包括我的导师江勇老师），真正在深圳科研教学的老师少之又少，我在读研之前就已经知道这个状况了，但幸好我是跟在夏老师的课题组，实验室的同学也都是计算机系的同学，所以我没有太觉得自己是tbsi的，反而我更觉得自己是计算机系的（最后也是拿的计算机的学位）。所以对比其他tbsi的同学，我觉得我很幸运了，可以在夏老师的课题组，蹭到计算机系的光，如果不是夏老师捞了我，我都不知道我在tbsi能选哪个实验室，感觉就没有自己感兴趣的实验室了。研二的时候，据说伯克利那边没有和清华继续合作了，因此tbsi的一三中心也并入了深研院，只剩下一个二中心继续挂羊头卖狗肉。另外tbsi三年里换了无数个行政和教务老师，每一个都做不长久，也不知道为什么，可能这就是tbsi吧，虽然我在这里疯狂吐槽tbsi，但我又要非常感谢tbsi，没有tbsi，我硕士进不了清华，我进不去夏老师课题组，拿不到计算机学位，感谢tbsi！</p>
<p>其实我来到清华，我是挺自卑的，因为我觉得身边的同学都挺优秀的，特别是本科就在清华的同学，这里每个人都或多或少有些特长和技能，而我总觉得自己是那么的普通，可能就只是本科的时候会读书会考试所以成绩稍微好了一点，然后又恰逢给tbsi这种像调剂专业一样的学院给捞了，才来到的清华读研。每次家庭聚会，都会听到某些夸赞我在清华上学的声音，说实话，听到这些声音的时候，我总会觉得很尴尬，除了一笑了之和说谢谢之外，也不知道怎么回应，清华对比其他大学，在一般人眼里显得太过显著了，毕竟大家都会知道清华北大，而其他好大学可能一般人都没什么耳闻，也不会一直记着，但家族里有一个人在清华读书，就会被大家所铭记，这也成为了我的一种负担，有时在想，我真的达到了一个清华人应有的水平了吗，我真的是个合格的清华人吗，清华人该有的那种自强不息厚德的家国情怀我有吗，清华人为祖国健康工作五十年的体魄我有吗，这些灵魂拷问会使我为自己的清华人的身份而感到困惑，以至于有时候有人问我在哪个大学读书，我都不能很自豪很大方地和别人说我在清华读书，这种压力来自于人们对于清华学子的那种高期待，会觉得清华人是那种天之骄子，赚大钱做大事的人，但当他知道你可能没有达到他的预期的时候，这种落差可能尤为明显，因为我知道我自己真的是一个普通人，我的工作能力学习能力等可能还不如一个别的学校的同学，我可能真的是个水货，但我也希望自己不要妄自菲薄，虽然我不能做很多很宏大的事情，虽然我很平凡，但我不平淡，我能来到清华，我一定有比其他人优胜的地方，只是我们常常都是当局者迷，不会发现自己的美罢了，清华三年，带给我的肯定不仅仅是一张毕业证，在这里我也留下了很多深刻的回忆，我成长了不少，认清了自己的实力范围，以后挺起胸膛，骄傲地和别人说，我硕士毕业于清华大学，专业是计算机科学与技术！</p>
<h2 id="科研工作篇"><a href="#科研工作篇" class="headerlink" title="科研工作篇"></a>科研工作篇</h2><p>在本科的总结篇章里，我并没有过多地说到我本科的学习经历，只是说了我经常泡图书馆，那是因为本科的学习并没有给我带来过多的困扰，大多数课程也都在我的能力范围里，因此只要我认真上课完成作业，期末复习，就能取得不错的成绩。而计算机专业的研究生对比本科，毕业要求完全不同，本科更多的是上课考试，毕业要求仅仅是修够学分不挂科即可，而我们专业的研究生的毕业要求除了要修少量的学分外，还要发表论文，tbsi的毕业要求对于论文完全没有要求，而计算机专业或者说我们导师的要求是发表一篇清华计算机系B类论文即可，而三年里只有第一年有课程，并且课程不多且成绩不是那么重要，所以三年里极大部分时间都会是在做科研和发论文。</p>
<p>但说实话，我的科研道路真的不是那么顺利，这也导致我的科研热情不断消退，大四的时候我就已经来到这边做本科的毕业论文了，在深圳呆了几个月，刚开始戴涛师兄让我尝试下知识蒸馏在超分应用下是否可行，刚接触计算机视觉的我，花了大半个月的时间才入门，然后就开始我的科研之路了，说实话，我到现在都没有找到做科研的窍门，按道理做科研首先要从问题出发，找到问题，然后解决问题，但由于深度学习过度的玄学化，就会导致一个想法看着挺合理的，但是实验却不work，或者一个想法挺不合理的，但是实验却work了，而大四的我压根不懂得如何发现问题和解决问题（其实到现在我也不懂，所以很羡慕其他同学他们懂得分析任务本身的特殊性，再从中找到改进的地方），所以我的科研方法就是其他方法的迁移和拼凑，看到一个比较有趣的论文方法，就想着套过来看看有没有用，但结果大多数都是不太行的，就这样大四做毕业论文的时候就疯狂迁移和尝试各种方法，还有调节更种超参数（这在AI领域称为炼丹），要知道跑一个超分的实验可是要一天半到两天的，就这样我还跑了几十个实验，这个时间成本是很高的，但即使我跑了那么多的实验，实验结果也还是很不理想，最终就把这个半成品写成了本科毕业论文，当时大四在夏老师课题组组会上汇报的时候，当我汇报完我的毕业论文，我都尴尬地想找个洞钻进去了，要创新性没创新性，要好的实验效果也没有，看着别的同学的实验方法，都是一二三列举的，然后分别针对什么问题，做出什么改进什么解决方法，而我的方法就只是个简单的合并和套用，也说不出来为什么要这样做（记得我在wxg实习的mentor和我说，方法的依据很重要，要突出每一步的逻辑严谨性），但就这样的一篇毕业论文，竟然还拿到了本科优秀毕业论文，我简直一个大无语了，可能本科做AI方向的同学不多吧，真正懂行的都知道这是一个什么烂工作。。</p>
<p>对于我身边的同学，本科毕业论文都只是科研道路上的小试牛刀，但谁能想到我本科毕业论文的这个工作（简称超分蒸馏）竟然也是我硕士毕业论文的一个工作，那就是意味着我研究生三年都没有做出什么其他能够写进硕士毕业论文的的科研成果，确实是一件很丢人的事，可能也就只有我是这样吧。研究生一年级，戴涛师兄建议我把本科毕业论文投稿出去，一来是把之前那个半成品工作收个尾，二来是如果文章中了就能满足毕业要求了，以后就不用为毕业苦恼，就这样我又开始我的科研之旅了，同样还是各种玄学调参数和尝试（苦笑），但幸运的是，这次竟然给我找到了一个有用的方法，实验结果也好看了许多，这个工作终于可以收个尾了，于是就开始整理实验，开始写论文准备投稿10月的ICASSP（ccf-b类会议，清华列表也是b类），对比很多同学，我算是比较早就开始投稿论文的了，毕竟8月底才开学，我10月份就投稿论文了，研究生第一次写英文学术论文，第一次用latex，也是困难重重，但最终还是投出去了。投完这个工作之后，我就感觉我自己已经能够毕业了，但显然投稿的结果也没出，也不是必中，即使是必中，也还有两年半才要毕业，难道就不继续科研吗？但当时的我确实是这样想的，投完稿之后我的科研就停了，记忆中好像很少一段时间都没怎么科研看论文，也没人监督我啥的，也没啥idea，再加上当时被拉去做vivo的项目，所以好像就没怎么科研。但这篇我看来好像一定会中的论文究竟中没中呢？答案是否定的，印象非常深刻，ICASSP公布结果是在2020年的大年初一，当时群里好多同学都中了论文，新春佳节+中稿论文，多么开心的事情，但我没中，多多少少有点失落和遗憾吧，而我也知道科研路从来都不是一帆风顺的，就这样春节休息了几天之后，我就开始改论文和做实验，将原来的论文改投到ICIP（另外一个毕业神会），而幸运的是，这次论文终于中稿了，也是达到了实验室的毕业要求，意味着可以毕业了。</p>
<p>2020年春，当时疫情很严重，在家隔离的期间，基本就在做项目，vivo项目和悦动圈项目，完全没有做科研，还记得当时戴涛师兄问我科研进展的时候，我都哑口无言。就这样到了暑假，我听说实验室很多同学都去实习了，有腾讯AI Lab的，有华为诺亚实验室的等，而且都是科研岗，说白了就是去公司做科研，有mentor指导，那时我也想去公司做科研发论文，因为在学校实验室基本没人指导，而我又不知道怎么做科研，而在公司的话，有服务器有人指导有讨论，并且目标就是发论文，这样科研起来会更加顺利。就这样我也投了几个公司，最开始面试我的是商汤，面了一轮就录用了，hr给我打电话说是工程岗，一听就是那种特别缺人干工程的岗位，不然也不会就只有一轮面试，所以我拒绝了。后面我投了腾讯的teg，当时的官网还只能具体到bg，不能具体到部门，因为我也想去ai lab做科研，但没想到teg的没捞我的简历，反而ieg的捞了我，面了两三轮后就过了，我当时也没仔细问是不是科研岗，发论文啥的，就稀里糊涂地答应了，现在想起来是真的很后悔，去的部门是ieg的光子学习发展组，去了三个月，啥东西都没做，也有可能是我去得晚，我7月再去，当时的暑期实习生8月就答辩了，而我是日常实习生，当时给我的任务就是交接某一个暑期实习生的工作，然后一交接就交接了一个月，每天无所事事就是看他的代码和文档，然后全程mentor也没有和我讨论啥的，也没有管我，就一个星期或者两个星期丢给我一两篇论文让我看看能不能用到项目里，我印象很深刻的是，当时坐我旁边那个实习生和我聊天说到，这个组招很多实习生，但又不管，并且也全部不能转正留用，完全不懂为啥要招那么多人，她因为是不考虑转正的，mentor也没给她布置任务，她每天就在工位准备秋招，真正的带薪摸鱼，就这样我呆了三个月就离职了，离职之后没有继续找实习又是一个错误的决定，起码在现在看来是的，当时离职的另外一个原因是回学校和实验室同学打算一起投稿一篇cvpr，毕竟谁能拒绝cvpr这种顶级会议的魅力呢，简历上多一篇cvpr加的分可不是一点点，大家都知道都认可cvpr，但很少人知道icip这种水会，想着再发一篇论文，我就离职回学校继续科研了，当时三个人合作，最开始的时候决定给我第三作者，但怎么说呢，我很积极讨论，也很积极地写论文，甚至idea很大一部分也是我提的（虽然又是把别人的方法迁移过来），只是我没有参与过实验部分（实验部分是刘沛东用华为的服务器跑的实验），最终改为大家都是共同第一作者，我是第三个，同样很幸运的是，实验work了，我们也顺利投稿出去了，但没中，后面改投了ACMMM2021，幸运的是中了，虽然MM没有cvpr出名，但也比icip好不少了，这也是我研究生阶段唯二的论文了。</p>
<p>上面说到我没有继续找实习是一个错误的决定，因为我高估了自己的科研能力，当时2020年11月份投稿完cvpr后，我就尝试自己做科研，自己尝试idea做实验（其实又是简单拼凑和迁移。。），结果可想而知，就是没有进展，那个时候实验室的同学基本都出去实习了，他们都是科研岗，都是拿着工资在公司做科研，还有mentor指导讨论，而我没工资，自己一个人在实验室做科研，心酸。所以我说我在ieg的那份实习很失败，别人研二的实习都是半年以上的，长的能达一年，然后都有不少的科研成果产出，而我去ieg一来不是科研岗，mentor也不管，二来时间短，就是纯纯的浪费时间，当时如果自己的目标更加明确一点，更加清楚知道自己想要什么的话，我可能会更加果断，更早离职去找下一份科研实习，或者说如果我不那么急答应ieg那边，等学校和优图的专业实践会不会更好一点呢，毕竟优图就是类似AI Lab的科研部门，但没有如果了。</p>
<p>后面当我意识到我不能自己一个人漫无目的的科研时，已经太晚了，我那个时候再去投简历，别人已经不招人了，并且时间马上就来到研二下学期，要开始找暑期实习和秋招了，所以我也没再找要发论文的科研岗了，而是选择对以后工作更有帮助的业务部门，这里感谢我本科的inpluslab，不得不说本科实验室的资源是真的丰富，我暑期实习就是靠着本科实验室师兄的内推才能去到的广州wxg实习，在wxg实习的四个月里面，给我布置了一个不适图片检测的任务，四个月里面，我其实没有干太多的工作，可能是前两年没有受到科研思维的训练，我在想解决方案的时候，思维会比较局限，然后遇到问题了，也不会解决，因此最终也没有很好的完成这个任务，虽说最后还是转正成功了，马上就要入职工作了，我真的很怕以后工作的时候会应付不来，毕竟我实习的时候就是很不适应，但也希望以后工作能慢慢培养出自己的一套方法论吧，但可能就需要付出沉重的试错成本吧。</p>
<p>秋招结束决定了工作后的研三，可能慢慢地习惯了自己不适合科研，科研热情早就被燃尽了，到了要写毕业论文的时候，才又草草地做了一个工作出来，这里感谢白杨师姐的帮助，幸运的是实验work了，不然我的毕业论文是真的愁死。</p>
<p>曾几何时，我也希望有一个mentor能够手把手地教我科研，能到一个科研岗，或者科研氛围强一点的小组，每天头脑风暴，分享论文，可惜我没有遇到，或许我遇到了，但归根到底羊毛出在羊身上，我对科研的那把火始终没有燃烧地特别旺盛，对未知的探索也不那么激动，有时候听别人分享论文，听不懂的地方也不会追问，就我这种需要人推着前进的人，怎么能做好科研，毕竟科研本身就是孤独的，有的人能够沉的住这种孤独，能够自我驱动，而我不是这种人，对比本科的学习，本科更多的是考试这种有标准答案的题目，而研究生的科研、未来工作上布置给你的任务，更多是那些不存在唯一解的题目，可能需要你更多的自我思考，需要你有一定的探索能力和创新能力，这也是我在本科的时候很少训练自己的能力。另外我觉得自己对于我自己从事的这个行业并不是特别的感兴趣，有些人喜欢去看每天都有什么好论文，什么best paper啥的，而我压根不关注这些，久而久之，我的知识面就变得非常窄，在想解决方案的时候也少能发散思维，因此在后面的工作中，也是需要提高自己知识面的广度，少看一点b站游戏，每天看下论文推送吧哈哈！</p>
<h2 id="课外活动篇"><a href="#课外活动篇" class="headerlink" title="课外活动篇"></a>课外活动篇</h2><p>本科四年，我都很少有参加其他课外活动，也就只有大一的时候参加了一下合唱团和学生会，大二开始就没有参加过任何的社团和课外活动了，所以我本科基本就一直在图书馆学习，生活极其无聊，可能唯一的课外活动就是打打球，看看电影啥的吧，但可能也是因为本科的学习比较重要，学习比较有目标（考试，保研，考英语啥的），所以花了很多时间在学习上，这样想没有本科四年的学习又怎能上得了清华的研究生呢。而研究生三年，对比本科四年的学习，科研是比较没有目标的，并且也没有必须要上的课程，时间安排上就会比本科要多很多，甚至没有了周中和周末的概念，而上面也说了，研究生的我在科研上比较喜欢摆烂，所以就多了很多分配给课外活动的时间，因此研究生我尝试了很多课外活动，生活相较本科丰富了不少，也留下了不少难忘的时刻。</p>
<p>研一的时候又重新加入了合唱团，本科是在男高声部，研究生尝试了一下男低声部，果然是难唱的一批，基本没有主旋律可言，但也不为是一种新的尝试了。加入了合唱团后，有幸能参加深圳举办的国庆70周年大型交响合唱演出——祖国颂，合唱的曲目分为上下两个乐章，第一乐章是关于祖国的组曲，第二乐章是黄河大合唱，参加这次演出我非常的自豪，并且现场指挥的是姚贝娜的父亲姚峰老师，到现在，这次演出合唱的曲目还在我的歌单里面，每每播放这个歌单，我都能感到很激动澎湃，能为自己出生在神州大地、生活在中国而自豪！然后就到了学校举办129合唱的时候，一班班长看我是在合唱团，就把我拉去组织同学合唱，我也当了一回指挥，教其他人唱旋律，果然当指挥是很难的事，相对于合唱队员只用学会自己的旋律，指挥要学会每一个声部的旋律，还要准确地引领每一个声部进行演唱，是个非常有挑战的工作，唱的两首歌曲也是非常红的歌曲，分别是在灿烂阳光下和另外一首忘了，演出的当晚，由于我们tbsi是在最后一个学部演唱，然后唱完之后要光速换衣服参加合唱团的压轴演出，于是就有了我光速换衣的一幕了哈哈，tbsi合唱穿的是军装，而合唱团要求是西装，于是我就把西裤衬衫穿里面，然后唱完，马上脱掉军装，套上外套换个皮鞋急匆匆地上台了哈哈，然后后面有同学把我脱下来的衣服收好，合唱团唱的歌是东方之珠，恰逢最近香港回归25周年，这首歌又再一次进入了人们的眼前，也是一首能唤起很多记忆的歌了，研一下学期疫情在家，参与了合唱团的原创抗疫歌曲的录制，也是挺自豪的，然后到了研二就没再去合唱团了，一是热情消退了（可能本身也不怎么热爱合唱吧，还是喜欢自己瞎唱做个浴室歌手），二是社恐的我，合唱团里都是新人，没我认识的，我就不想去了，至此我学生时代的合唱生涯就结束了。</p>
<p>相对于唱歌这个我一直伴随着我的兴趣，研究生阶段我尝试了跳舞，首先是迎新晚会上和tbsi的其他同学一起尝试了一些简单的舞蹈。然后当时社团招新，我有两个选择，一是街舞，二是国标舞（摩登和拉丁），我选择了后者，但其实现在回想起来，可能街舞更适合我这种完全零基础的人，毕竟街舞要相对简单一点，国标舞，尤其是拉丁，太要求时间的积累了，很多跳拉丁的同学都是小时候学过的，这对于零基础的我十分困难，所以研一刚开始试跳了两节课之后，我也放弃了拉丁舞，专心跳摩登舞，摩登舞相对于拉丁舞，可能对于新手稍微适合一点，也会更加适合social，但因为跳摩登舞需要讲究两个人的配合，但是我们上课是没有固定的舞伴的，每节课可能都会和不同的人跳，这其实对于新手是不友好的，因为两个人需要磨合，对于新手的我，没有一个固定的舞伴，进步就会非常慢。但幸运的是，研一上学期隔壁汇丰商学院举办假面舞会，然后我有幸能参加表演，这其实对于我来说，既是机遇又是挑战吧，因为刚学跳舞没多久就要上台，好在我的舞伴稍微老道一点，然后我们一遍遍的抠动作，磨合，才勉强跟上了其他人，但是最后表演的时候，或许还是太紧张了，我们表演一共五对，只有我疯狂在抢拍，并且也没和其他四对跳到一块去，就特别尴尬，以前本科总是以合唱队员的身份上台演出，有一堆人在身边安全感很够，但现在作为舞蹈者站在舞台，本来在舞台上就只能有平时的五成不到，一紧张，3成都没了，但是也算是一种不一样的体验吧。接下来的两年里，我也断断续续地在上摩登舞的课，和国标队的小伙伴们也逐渐混熟，一起参加过一些舞蹈相关的活动，比如研二的男生节舞会的开场表演，然后研三的时候他们国标队的去溧阳参加国标舞的比赛，叫上了我让我做领队，我也十分乐意，虽然我自己不能参加比赛，也不敢参加（毕竟是个新手），但能看到很多专业选手同台竞技也是一种享受。</p>
<p>虽然我硕士的三年经常吐槽南国清华这边没什么氛围，羡慕北京本部啥的，但现在想想我应该感谢深研院，因为要不是深研院这边人少，很多活动可能压根就轮不到我去参加，我也不会有那么多不一样的体验，就比如研二的时候为了庆祝国庆，深研院这边就打算录制《我和我的祖国》再做个mv给祖国母亲献礼，当时就在招募歌手，我厚着脸皮报名了，没想到报名的就没几个，最终我也入选了，去了录音室录歌，这也是我第一次去录音室，在其他同学面前，疯狂地唱一句还唱不好，属实有点尴尬。然后研二下学期，为了庆祝清华110周年，深研院这边也打算拍一个mv，也是同样的招募歌手拍摄mv，但这次不用我们唱了，就单纯拍mv而已，我又是厚着脸皮报名了，然后又是没什么人报名，我又蹭到了一次拍摄mv的体验，那天还正好是4.2我的生日，你说是不是值得纪念，但可惜的是后面做的mv本部那边没有接收，但我觉得不遗憾，我有不一样的体验就够了，并且正片花絮啥的原视频我也拿到了，以后拿来纪念也好。所以说嘛，深研院人少确实挺好的，只要你够勇敢，机会就是你的，另外研二参加了学校的十大歌手比赛，我虽然说一直都喜欢唱歌，喜欢录歌，也被各种同学夸赞唱歌好听，但其实我知道我唱歌有很多不足，只是外人看不出来罢了，要不然我怎么会一直在本科参加维纳斯歌手比赛都是一轮游呢，来到深研院这边，我继续参加唱歌比赛，人少的好处就是，我终于打破了多年一轮游的命运，成功进入了复赛，当时还想着如果进入决赛，我就唱《爱是永恒》，朋友圈文案我都想好了，就是有始有终，爱是永恒开始（本科第一次参加比赛就是唱的这个），爱是永恒结束。做梦真的好，梦里什么都有，但现实就是复赛被刷了，其实我早就认清了自己的实力了，我就是一个ktv歌手，我就喜欢瞎唱，但有时候发下白日梦也挺好的哈哈。另外还有一件值得纪念的事就是，当时加入了中大跑跑卡丁车车队，然后一起合作演唱了队歌《繁荣山丘》，现在在qq音乐和网易云音乐都能搜到，咱就是说我也是在qq音乐有歌的人了～</p>
<p>在研究生的三年里，我的民族自豪感在一次次的国庆校庆献礼里得到增强，让我对自己的祖国能有更深的情感。在本科阶段我压根没有尝试过去不同的地方，感受不同地方的文化，四年里就保研的时候去了北京上海南京这三座大城市，毕业旅游去了三亚玩了一会，而研究生三年我去了更多的城市，宁夏、成都、云南、溧阳，让我得以感受更多不同的地方文化，在研一寒假参加了清华的社会实践，去宁夏红寺堡进行十天的地方社会调查，拜访当地人了解当地习俗，参观当地葡萄酒产业等等，因为冬天去，本广东人还第一次感受了一下北方的冬天，暖气、炕、下雪、滑雪等等，</p>
<p>总的来说，在学业压力较小的研究生三年，我得以尝试不同的东西，体验不同的生活，这一篇章我很满意。</p>
<h2 id="朋友篇"><a href="#朋友篇" class="headerlink" title="朋友篇"></a>朋友篇</h2><p>还记得本科的朋友篇最终是以悲剧收尾，那研究生的朋友篇呢，庆幸的是，研究生结束了，还是有那么几个还在活跃的群聊，和能聊天的朋友，足矣！希望以后这些群聊会成为我们周末的快乐源泉和吐槽地方，大家晒一晒自己做的饭菜，养的猫狗。</p>
<p>我研究生阶段活跃的两个群体分别是实验室（尤其是小白菜们，钟昊翔、徐佳、严欣）和国标队的朋友们（尤其是罗阳），我的毕业照基本就是和这两个群体在拍。和小白菜们的“邂逅“发生在研一上学期，当时夏老师拉了一个悦动圈的项目，然后我们研一正好四个人报名了，那时就经常一起去悦动圈公司那里上班做项目，一起打车一起吃饭，渐渐就熟起来了，咱们的小白菜群最开始还是收车费而建的，后来就是一起做项目，一起在腾讯实习，在我影响下开始入坑王者荣耀（虽然现在大家都不玩了，在翔哥的影响下大家都开始玩原神了），一起去顺德吃喝，一起去欢乐谷过万圣节，一起云南毕业旅游，可以说研究生阶段最熟的人就是他们三了，咱们基本每一个生日都是一起过的，除了疫情阻隔外，现在翔哥和佳佳去了杭州蚂蚁工作，我和欣姐都在腾讯，但我在广州她在深圳，希望友谊能长存吧，我们都不是善于表达自己内心情感的人，但我相信他们也是像我这样认为彼此是重要的人。</p>
<p>另一波比较熟的人是国标队的朋友们，比如钱伟师兄、罗阳、李婉思、杨若凝，他们几个本来就比较熟的，因为他们都是跳国标舞的，并且都有基础，都是国标队的骨干成员，而我是研二才逐渐才和他们熟起来的，但对比他们几个，我就显得很内向很寡言了，因为他们真的太能说了，太能带动气氛了，和他们在一起的时候就会被他们带动着一起各种“哇哦哇哦”的叫（这些是和小白菜们在一起都没有的，毕竟小白菜们都不是很活泼外向的人），因此我在小白菜群体可能我是那个比较活泼的人，但我在国标队群体里我就是比较不说话的人，还有一个原因是我不读博，他们都读博，我是唯一一个出来工作的。这几个人里面最熟的就是罗阳了，他是和我一个班的同学，他和钱伟钱叔都是我初次摩登舞的启蒙老师，最开始也是跳舞课上交流一下，到后面就是经常一起吃饭，出去玩，国标队群体还有一个最爱的活动，就是玩桌游，虽然我玩桌游没什么天赋，但和他们在一起就会很快乐很解压，毕竟他们很有梗，很能带动气氛，“你清醒一点”，“听我说谢谢你”，每次听到都能笑很久哈哈。</p>
<p>我现在反思我本科的人际关系，比较失败的点在于没有一个可以维系的小群体（比如常说的闺蜜群基友群），而研究生找到两个可维系的群体，以后去深圳还能和他们约约饭啥的，也挺好的。但研究生阶段我觉得我自己的人系关系处理能力还有待提高吧，不知道是不是性格原因，我总喜欢冷处理对待所有有问题的人际关系，我不喜欢将东西摊开说，就解决问题那样，反而总是把东西都藏着掖着，比如研三和舍友在作息上发现了矛盾，然后我们就一个多月没说过话，但其实就是聊一聊相互妥协一下就能解决的问题，我的冷处理就非常愚蠢，所以有时候我很羡慕那些大大方方的人，他们的神经可能比较大条，不会有隔夜仇，睡了一觉啥事就没有了，而我发生了矛盾后，我再看到这个人或者和这个人交流，就会不自觉地想起之前的矛盾，然后不断地强化这个矛盾在我心里的位置，咱就是说是一个恶性循环吧，只能等时间过去，慢慢冲淡了才能解决，和陈煜钊等关系就是这样，我俩本来是很熟的朋友吧，以前都是戴涛师兄带的，也都是做vivo项目，有时候我也去他姐家吃饭，可以说无话不谈吧，但研二我们都去广州微信实习，就决定一起合租，但其实也不是相处容易同住难的问题，而是有一次他和我的对话，让我感觉自己不被尊重，就让我很不爽，自此之后我就没和他聊天，他好像也看出来也没找我聊，从此我们的关系就很普通了，毕业典礼才见到面聊了天，甚至有段时间我还在想他以后工作去深圳不在广州就好了，我们就不会再见面，但是最终他也是选择留在广州，不过现在时间也冲淡了很多东西了，以后我们都在一间公司，也不在一起住了，所有的过去都称为过去了，也可以慢慢熟络回来了。从这就可以看出来，我其实是一种逃避型人格，人际关系遇到问题了就会选择逃避，不主动解决问题，这在所有的关系里面可不是一种好的品质哦，有可能会损失很多朋友，因为没有朋友之间是不发生矛盾和争吵的，那万一发生了争吵，就需要冷静下来解决问题，不要逃避，不要冷处理，希望自己以后这方面能改一下吧。</p>
<p>研究生阶段的朋友篇，我也挺满意的，收获了一些朋友，以后在广州也有研究生朋友可以一起约起来，希望好好维护一下自己的这些资源吧。</p>
<p><em>写在最后</em></p>
<p>感谢清华三年来对我的培养，三年硕士，收获满满，虽有遗憾，不足挂齿，念念不忘，必有回响，愿此去繁花似锦，归来仍是少年。</p>
<p><br></p>
]]></content>
  </entry>
  <entry>
    <title>致逝去的四年大学时光</title>
    <url>/2019/07/14/%E8%87%B4%E9%80%9D%E5%8E%BB%E7%9A%84%E5%9B%9B%E5%B9%B4%E5%A4%A7%E5%AD%A6%E6%97%B6%E5%85%89/</url>
    <content><![CDATA[<img src="/2019/07/14/%E8%87%B4%E9%80%9D%E5%8E%BB%E7%9A%84%E5%9B%9B%E5%B9%B4%E5%A4%A7%E5%AD%A6%E6%97%B6%E5%85%89/head.JPG" class="" width="500" height="500">
<span id="more"></span>
<p>2019年7月7日，星期天，晴，毕业已经有一个星期了，在家躺了几天之后，在整理抽屉途中，无意间找到了自己高中三年的日记，看着看着眼角突然间湿润了，高中的自己是多么的单纯，那时的自己目标很明确，也清楚知道高考是自己的唯一出路，因而为此奋斗，那本日记记录了我的叛逆期，我的奋斗史，无论多久之后再去看，总能回想起自己当时生活和学习上的点点滴滴。</p>
<p>大学四年也就这样过去了，即将迈入新的学校，开始新的生活，因此我决定写点东西来纪念我那逝去的四年大学光阴，下面主要将以回忆录的方式去记录我的大学时光，分为：宿舍篇、合唱团篇、学院篇、朋友篇。</p>
<h3 id="宿舍篇"><a href="#宿舍篇" class="headerlink" title="宿舍篇"></a>宿舍篇</h3><p>有人说，大学中能分到一个宿舍是缘分，大学中陪伴我们最长时间的无疑也是舍友。大一，我们来到了一个全新的学校——中山大学珠海校区，这个依山傍水的全新的环境，被高年级的师兄师姐调侃为珠海养老院，寓意着生活舒适。而在命运的驱使下，四个素未谋面的大学生相遇在榕园9楼431房间，这是缘分，他们分别是何智通、洪佳平、侯伟权和我。作为大学中最先认识的人，这三个舍友理应会成为我大学中比较重要的朋友，我一开始也是这样认为的，我们一起吃饭，一起上下课，一起打游戏（在舍友洪佳平的影响下，大一上学期我们宿舍集体开始玩起了地下城与勇士这款游戏），我们大家参加的社团都不一样，但是当时我看到他们几个都做了社团活动的PM，并且为了筹办活动忙得飞起，而我却一直默默地在社团打着酱油，当时的我是羡慕他们的。后来到了大一下学期，我也成功地做上了PM，举办了活动，而我的大学在这一方面上也算是没有遗憾了。</p>
<p>而这个和谐的宿舍环境最后为什么破裂了？是因为学习问题上。上面提到我们当时一个宿舍都在玩DNF这款游戏，但是我和何智通玩了半个月就觉得无聊而没有继续了，洪佳平作为发起人，当然是这款游戏的死忠粉，而侯伟权虽然不是死忠粉，但是也慢慢地沉迷上了DNF。移动学院在大一上学期每个星期天下午都会有编程训练，我每次都会参加，而侯伟权一开始也会参加，但是后来慢慢地由于沉迷游戏，参加的次数越来越少；而洪佳平从一开学接触到编程之后，就没有起过兴趣，因此自然他一次都没参加过。但是就这件事不至于使得我讨厌他们，真正让我对他们反感的是大一期末复习周，大一的我对待学习的态度是绝对的认真，大一真正考试的课程可能就是高数和线代，期末复习周，每一科，我都会总结知识点，并且写到本子上，除此之外我甚至都把两本书课后的习题全部刷完，还反过来教当时大二的姚彤昕线性代数，而最后这两科的成绩我也考得很好。和我相反的是，侯伟权和洪佳平到了考前都没有去复习，侯伟权本身就要比洪佳平要聪明，从他对新知识的接受能力上就能看得出来，当时他的编程能力也甚至在我之上，但是当时期末复习周他的一句话，使我彻底对他改观了。当时他说：现在剩那么少时间了，复习了也不会过，复习来干嘛用。。。听到这句话我瞬间无语了，但是我已经无力反驳他了，这是他自己个人的选择，我也不想干涉。他的这句话有两点使我无语的地方，一是为什么时间只剩下那么少了而你却还没开始复习？为什么不会提前早点去复习呢？时间都拿去打游戏了吗？当时的我理解为什么他的复习时间那么少，我们当时貌似是1月10多号的期末考试，他举办了一个跨年的活动，所以我也理解他在举办完活动后想放松一下的心情，但是元旦即使你玩过去了，你还有10天的复习时间，那么为什么你不开始复习呢？二是为什么时间剩下那么少，复习就没有用了呢？这一点是我非常无语的，他直接否定了临时抱佛脚的有效性，虽说临时抱佛脚用处不大，但是不能否定他的有效性，当时洪佳平线性代数什么都没复习，靠我考前一晚上给他的知识点，最后他也顺利地及格了，而侯伟权你比洪佳平要聪明地多，为什么你就不相信临时抱佛脚呢？而最后侯伟权也是顺利地挂了科，当然洪佳平也不例外，毕竟不是所有东西都能临时抱佛脚的，比如他那糟糕的编程能力。</p>
<p>从那开始，我和他们的交流也就变少了，我也经常不在宿舍学习，或去图书馆或去自习室，当然也有姚彤昕的原因。大一下学期，我本来以为挂了科的两兄弟会突然觉悟过来，开始发奋学习，没想到是我想多了，他们依旧老样子，甚至变本加厉，大一上学期都鲜有逃课的侯伟权，大一下直接就不去上课，在宿舍打游戏（当时还有个另外觉得好笑的事情，侯伟权打DNF走火入魔了，说要进行第一批小号计划，然后第一批小号满级后，接着第二批小号计划，真的是可笑至极）。而洪佳平作为DNF的死忠粉，自然不会放弃DNF，而没有大一上学期基础的洪佳平，大一下学期听课就更为难受了，上了几个星期课后，也是进入半放弃状态，而此时的侯伟权已经是全放弃状态了。当时我还听到一个更为可笑的东西，据说洪佳平是生物自主招生进的中大，他也说自己不太喜欢移动这个专业，但是当时从他的学习接受能力来看，我完全没有看出来他像是自主招生进来的，以我的说法就是太蠢了，最基本的数学和编程的逻辑思维能力都没有，也怪不得他学得如此吃力，以至于放弃。但是既然你不喜欢这个专业，那你又有没有转专业的想法呢？如果有，那为什么不努力学呢？知道自己蠢，为什么不多花点时间呢？大学最后一年，洪佳平自己坦言说自己当时上了大学就开始放松了，可能高中压抑了太久，上了大学终于可以肆无忌惮地打DNF，于是变一发不可收拾，在我眼里，他不仅学习不好，而且自制能力也是真的很差。</p>
<p>大一下学期中，我对他们俩压抑的愤怒已经很多了，某天中午午觉后，理应该是要去上课的，但是毫无疑问，他俩依旧在打游戏，毫无上课的想法，于是我就在床上拍了张他俩的照片，并且发到宿舍群，说两条咸鱼。没想到侯伟权还自嘲自己是salted fish之类的，当时在图书馆学习的我，不知道为什么就和他喷了起来，可能压抑了太久了。没过几个星期的一天，回到宿舍，突然发现侯伟权把东西全部收拾好已经走了，后来才发现他退学了，至于他退学之后是去复读还是什么我就不知道了，我只记得在他退学前的一段时间，他总是晚上12点后自己在床上写东西，或许是实在受不了自己颓废的样子了吧。到此为止，我们宿舍变成三人宿舍。</p>
<p>大一下学期由于院系调整的原因，我们院会回迁到东校，而有消息传出回到东校后可以自主组建宿舍，当时的我急切想摆脱我的舍友，因此找了三个伙伴组建了一个宿舍，但是最后自主组建的政策没有实施，因此只能维持原宿舍，来到东校后，我们被告知有一个名叫陈瑶的14级师兄要入住我们的宿舍，当时的我并没有因为他是师兄而高兴，待我查清楚后才知道这个师兄是个留级生，并且在14级几乎没有存在感，我当时就有了危机感，等到和他相处下来后发现果然和我预想的一样，留级生，性格孤僻，独来独往，不上课，每天宅在宿舍，吃外卖，睡觉，他也不是游戏迷，就是单纯地不想上课而已。他和洪佳平两个组成了宿舍最坚实的“后盾”，每天守护着宿舍，当时大二我出门压根不用带钥匙，因为回到宿舍肯定会有两个人在。当时大二的洪佳平是有点后悔自己大一的样子的，他很挣扎，来来回回地卸载了DNF好几次，想学习，但是在陈瑶的影响下，最终大二下彻底放弃了，整个学期都不去上课，他俩甚至期末都不去考试（其实去不去都一样，反正已经没有了全部平时分）。大三下学期，陈瑶搬出去自己一个人住，我们宿舍又回到三人宿舍，一直到毕业。</p>
<p>如今洪佳平因为学分不够无法正常毕业而留级了，另外一个舍友何智通考研考到了本校，而我即将去清华读研。可以说我们宿舍是比较传奇的，有退学的，有留级的，有保研的，很多人的大学舍友会成为他们大学四年里很好的朋友，而我的舍友并没有，我曾不止一次想要摆脱他们，到最后也只能默默接受。</p>
<p>有些人相遇后会成为一生挚友，有些人相遇后注定只会成为漫长人生路上的一个过客，而我的舍友是后者。</p>
<h3 id="合唱团篇"><a href="#合唱团篇" class="headerlink" title="合唱团篇"></a>合唱团篇</h3><p>合唱团作为单独的一篇，可见其特殊性。从高中开始，唱歌一直作为我的一大爱好，到了大学也一直继续发扬，由于只是单独喜欢瞎唱，可以理解为KTV选手，我对乐理乐器等均为一窍不通，并且当我上了大学才发现，唱歌厉害的比比皆是，而我只是芸芸众生中的一个小透明，独唱、乐队等都不适合我，因此我选择加入合唱团。而加入合唱团的另外一大原因是，当时大一军训时候举办军歌大赛，而我毫不犹豫就参加了，在那里认识了隔壁班的CJ，在他的安利下，我提前认识了合唱团的骨干成员，如团长方东仪，艺术副团姚彤昕，团长助理谢育廷，其中有一些还是我们移动的师兄师姐，比如常务副团赵扬波（后面同样在他的安利下，加入了学生会秘书处），男高声部长钟璟华，女低声部长尹晓琳，在军训的某一晚还一起去了珠海富华里通宵唱k，并且还很骚的跳了一个凳子舞（羞死），也因为这个后面才会私下创建舞蹈部，大家还叫我为部长。可以说除了我的两个组爸外，合唱团是我第二个地方认识各位师兄师姐，因此在种种原因下，我毫不犹豫地加入了合唱团。</p>
<p>我在合唱团的一年里，可谓收获满满。第一次团员大会学唱as long as I have music，至今为止，一听到前奏就会瞬间感动落泪；第一次学做海报，虽然做得一坨屎一样；光棍节送歌，作为小透明的我，竟然也有10来首可以唱（在合唱团呆久了，就会发现网红特别多，对比他们，我真的就只是一个小透明）；mini concert的小组唱以及活动之后的轰趴；大一寒假的南校演出，当时考完线代还下着雨，就要赶过去了；大二下学期的四校合唱演出，以及最后的专场音乐会，无数的回忆，由于当时移动需要回迁，这也使得我不能继续留任，育廷和我说如果我竞选的话，团长助理这个位置肯定是非我莫属的，而如果我选择留任了，我是否也会有机会去竞争阿卡的男高呢，但是没有如果，我和珠团的故事只停留在了大一。</p>
<p>现在回想起来，我当时大一经常去合唱团训练，经常去音教，一部分原因是出于当时和姚彤昕在一起，但是另一部分原因也在于我想学习一些乐理知识，经过一年后，我的基本乐理知识已经掌握差不多了，但是我的耳朵和嘴巴就差得太多，当时的我和姚彤昕整天在一起，在这个游走的钢琴旁边，我的试唱和练耳还是没有半点进步，实在惭愧，到现在我还是不能从一个合唱歌曲中，清楚地分出男声和女声（可能大一都把时间花在恋爱上了，真正学音乐的时间太少哈哈），育廷当时也说如果我不是因为姚彤昕，我是不会去合唱团那么勤的，我当时就否定了他的说法，毕竟大一的我还是比较闲的哈哈。在珠团的最后一个活动专场音乐会当天，下起了小雨，晚上演出时，当最后响起团歌as long的时候，瞬间泪崩，这个地方真的承载了我太多的记忆了。虽然说这里是我大一最重要的地方，但是真正和我玩得来的当时并没有太多，除了自己的男高声部外，就只有主席团和移动天团的几个师兄师姐，其它声部就没有太多熟悉的了。玩得最多的应该还是当时的主席团，一起夜跑，一起唱k，一起吃冒菜，一起去荣一吃宵夜等，育廷也是到现在为止很好的一个朋友，每次回去珠海，他都无一例外地和我睡一个房间。</p>
<p>在这里，我收获了第一份爱情，收获了一个挚友，可以说已经足够了。回迁到东校之后，我和波波彩虹他们一起加入了东团，当时正值中山情演出，我们也顺理成章的加入了演出，中山情演出是四校巡回演出，南校那场最后的几分钟，我站在台上忽然间眼前就一团黑，什么都看不见，猛出冷汗，在快要撑不住倒下的时候，旁边的鸡煲及时发现了，并把我搀住，好在有他，不然我从台上掉下去了，影响可就不是一点点了，那时我就真的一夜成名了。演出过后，我去东团的次数越来越少，一是作业增多，二是东团训练严厉，三是缺少了动力，以前大一干劲满满可能真的是因为姚彤昕呵呵，四是作为一个外来人，归属感太少，况且我不是一个自来熟的人，因此训练中场休息，我总是无聊地自己一个人玩手机，训练结束后也是第一时间就溜。就这样我就退了东团，和东团最后的接触是大二下学期东团的音乐会，当时冷爷可能看在我中山情上面的“卖力”演出，给了我两张票，最后是我和郭鑫去看了。</p>
<p>在后面的日子里，和合唱团的故事仅仅发生在每年的mini和专场，大二我回去了，大三mini也回去了，隔了一年，到大四毕业季的时候最后一次回去珠海听专场，这时，专场已经从以前的风雨球场到现在的新体育馆里举行了，而珠海也是各种拆拆建建，每一次地回去只会加深我的一个想法：这里并没有我留念的东西，我在这里的归属感越来越少。大一这里是我精神的寄托，到大二回去看看旧友，到大三回去发现没有一个认识的，到大四最后一次回去，更多地是去纪念吧，那一晚和育廷和岳安坐在台下看演出，同样地最后一首歌山高水长开始上台一起唱，那时我才发现自己已经把男高的旋律忘得七七八八了，而as long的歌词也同样忘了，虽然感动依旧，但是已经没有了当年的那种感动了，剩下地更多地是感叹吧，感叹时间的流逝，感叹好友的疏远。当时回珠海也没有人叫我，说好地一起拍毕业照，又把我踢出群聊，27号南校毕业典礼后拍毕业照，也没有叫我，当时我就已经心淡了，因为他们压根就没有想起我，也没有把我当成是合唱团的一份子。</p>
<p>至此，我的合唱生涯结束了，我想我以后也不会再回去了吧。</p>
<h3 id="学院篇"><a href="#学院篇" class="headerlink" title="学院篇"></a>学院篇</h3><p>“读着读着把学院读没了”，这句话来形容我们这一届的学生是最适合不过的了。2015年，高考的失利使我一直以来的外省求学梦破灭，只能屈居于华南首府——中山大学，并且只能沦落到几乎被调剂的地步，好在我遇到了移动信息工程学院，这个每年都几乎以中山大学最低录取分数线为分数线的学院，当时我的成绩也只能选这个学院了。因此命运让我和移动信息工程学院相遇，这个唯一坐落在珠海校区的信息类学院，也是比较特殊的。</p>
<p>开学院长第一课就给我们灌输移动信息工程学院是一个软硬兼施的学院，顾名思义就是既要学习软件学院的知识也要上电子学院的课，虽然是移动信息工程学院的录取分数线很低，但是院长的办学理念却让我敬佩，从往届师兄师姐的发言中，我都可以看得出这个学院的前景，工程实训、工程嘉年华、创新班（忘了是不是叫这个名字，反正就是大二选择导师进实验室开始干活）等等都让我眼前一亮，并且从2012年开始招新的移动学院，2015年正好招满了4届的学生。回想2012年刚开始招新的移动学院，我不由得感叹12届的师兄师姐实在太强了，当时的他们没有前辈带领，自己成立学生会团委，并且一步步地把学生会完善壮大，实在不容易，在学习上，12、13届的师兄师姐也是人才辈出，最让我印象深刻的是13届的师兄师姐们，竟然有3个人去了清华的交叉信息学院继续读研，叉院是出了名的难进的，而我们一个小小的移动学院竟然去了3个，厉害的师兄师姐还包括剑飞师兄，超颖师姐等等，这使我不由得感叹，这个移动学院真的是中大分数线最低的学院吗？不过也有可能是他们作为移动学院的开山鼻祖，受到的资源自然很多，所以我总有一种感觉是，12、13级的师兄师姐和院里面的老师和领导都很熟。而我们移动学院的老师和领导都很亲切，在学生会工作的一年里，接触到的老师都给我留下深刻印象，杨然副院长，大一就教我们导论课，没想到大四答辩让我再一次碰到她；刘念辅导员，念姐；14级辅导员琼哥；陈凌书记；校长助理，移动学院院长李文军，还有很多的移动的老师，张子臻，栋哥，饶阳辉等等。而移动出品的活动也是一流，班风大赛，我和郭鑫筹办的年度沸点大会，工程嘉年华，第一次让我知道了，我们以后可能发展的方向，各种智能小车，智能小船，智能穿戴设备等等；而大一的工程实训也是很难忘的一段时光，可以说珠海的移动学院给我留下了很多美好的回忆。</p>
<p>2015年，中山大学换了校长，新校长罗俊一上台就开始了院系调整，我印象中，第一个动刀的学院是国际商学院，当时国商被改名为国际金融学院，国商一下子变成了国金，而这个改动只是名字的改动，影响不大。2015年12月，罗俊终于对我们开刀了，整合软件学院，信科院，超算等几个学院成一个数据科学与计算机学院，同时整合电子学院，微电子学院，通信工程等成一个电子信息与技术学院，很明显这个改动就是要将软件和硬件的学院彻底分开（因为当时的信科院和电子学院是在一个学院，只是不同专业）。而作为软硬兼施，并且身在珠海校区这个偏远山区的移动信息工程学院就成了争议最大的学院了，它到底属于软还是硬呢？最开始学校给出的答案是：1.将移动信息工程学院整合到电子信息工程学院，2.我们为移动信息工程的最后一届，2016年不在招新，3.学院完全迁去广州的可能性不大，东校床位有限，只能先把师兄师姐迁过去。这一下子整个学院就炸了，我们学院是学校改革的白老鼠吗？不再招新且只留下我们在珠海，让我们自生自灭吗？并且学校在做决定之前，完全没有征求过民意。不过最后在师兄师姐的努力争取下，我们学院将全部并入数计院，并且全部迁入东校，这场闹剧也总算有了结尾。从那以后，我就开始了珠海生活半年倒计时。怎么说呢？这个决策有好有坏吧，对于高年级的师兄师姐，广州找工作要比珠海简单不少，整合后资源是否会增多？（其实也不见得，以前移动的老师都很尽责，工程实训等等，和老师接触机会也很多，合并成一整个数据院后，反而觉得和老师的距离变大了），不过当时对我来说，影响最大的可能是社团不可以留任了，即使说，移动学生会会和软院学生会等整合成一整个的数计学生会，这就使得竞争更大了，并且去到东校的我们和新生并没有什么不同，也是一个完全新的环境，因此我们没有任何的优势。所以我也没有留任，大二去到东校后，每天就是图书馆学习，还有图书馆期刊组上班。</p>
<p>接下来在东校的三年里面，我几乎没有感觉到学院的存在，没有感受到辅导员的存在，到了毕业了，我甚至还不太记得院长叫什么名字，这也是院系合并带来的，以前在珠海，移动学院亲似一家，合并后瞬间成了孤儿。现在毕业了，我更加希望说自己是SMIEer，而不是SDCSer。</p>
<h3 id="朋友篇"><a href="#朋友篇" class="headerlink" title="朋友篇"></a>朋友篇</h3><p>在大学里，我们会碰到很多很多的人，同一个学院的，同一个社团的，大大小小活动认识的，上大学前我微信的好友仅限于初高中同学，加起来就200多个，一上大学之后，微信好友直接突破500，而大一时看过姚彤昕和育廷的微信好友，甚至是1000+，这都能够说明大学里面我们确实会遇到很多人，但是在大学里面，认识到真正成为朋友之间的鸿沟却一直难以跨越，这问题当然出在我自己身上，我的性格和我的心态决定了我大学里面没有和任何一个人成为朋友，可能这样说得过于绝对，但是在我的世界里面，朋友是那种即使许久不见，但是一见面却能无话不说的人，生活中遇到困难你能想他倾诉的人，满足我所说的就只有高中的谭展宏、李晓慧、年梓帆，大学里我曾经觉得是朋友的梁泳嘉，韩硕轩可能也符合我的朋友定义。除此之外，其他人都只能是我的同学或者甚至是生命中的一个过客而已。</p>
<p>大一，刚刚进入大学的我，就遇到了一个为我指明道路的师姐：姚彤昕，说她是我大一里最重要的人一定也不为过，因为我的大一有50%以上的空余时间是和她一起过的。第一次见到她是在我军训的某一晚，CJ带我去认识合唱团的老肉们，那一晚他们去了富华里唱k通宵，当时我也不知道为什么就答应了，然后这就成了我大学生涯里的第一次刷夜，刚遇到她的时候，觉得她美丽动人，穿衣品味好，会打扮，打听过后知道她是合唱团的艺术指导，那一晚唱完k后，就开始等天亮，在富华里的星巴克前玩起了游戏，然后我可能是游戏输了，然后围着我旁边的CJ跳了一个凳子舞，从此我就有了一系列与跳舞有关的别称，什么凳子彬，什么舞蹈部部长等等。然后各种的原因，我就加入了合唱团，因为CJ总是和他们老人家玩在一起，特别是嘉成、方东仪、老板，而我大一刚开始的时候，就经常跟着CJ一起混，于是就有了很多和姚彤昕育廷接触的机会。和姚彤昕第一次单独接触是在国庆，当时移动学生会的全员大会每个部门都要表演节目，我抽到了跳舞，并且还是很骚的舞，当时我就知道姚彤昕是个能歌善舞的女生，因此我就借这个机会和她聊天（可能当时我是想和她拉近关系？）然后接下来的一个月，我接着跟着CJ和他们主席团玩，当时也正好有一个粤唱粤强的比赛，我也去参加了，并且进了复赛，在准备复赛的时候，我就顺势找她帮我指导，虽然最后也没进决赛，但是我们的关系在一步步地拉近，后来在某一个周五的下午，我竟然和她两人一起去看了个电影（当时我并不知道只有我和她，好像还有方东仪，但是方东仪放鸽子了，最后只有我和她去看），回来的时候好像还买了个玫瑰花，我现在记起来那个玫瑰花是给育廷在粤唱决赛上用的，当时的我实在太蠢了，我应该也买一支玫瑰送给她，粤唱的决赛我俩一起去看，并见证了移动霸占前三。我俩关系真正突破性进展可能是在我们移动工程嘉年华的前一天，犹记得那个星期五是短码之美的第一次比赛，合唱团的训练我请了假去参加比赛，我比赛结束他们的训练也正好结束，然后我们就去我们常去的荣一吹水宵夜，那晚我貌似借了她一个什么东西，然后回到宿舍才发现还没还，之后她就说明天星期六，我们可以约个馆，顺便还给她，还说要早起去霸位？她说怕起不来，然后我要求给她morning call，这就开始了我们的morning call的传统。第二天星期六我们就一起去图书馆，下午她和方东仪出去浪了，我也和学生会的其他人布置场地，晚上她好像给我买了泡芙？然后第二天星期天，我也忘了发生了什么事，我好想惹她生气了，星期天那天是我们移动的工程嘉年华，同时他们地院在榕广摆摊，我那天在嘉年华做志愿者，我很记得她早上去榕广逛了下，给我买了个钥匙扣，我现在还挂在我书包，接下来就发生了一系列比较暧昧搞笑的事情，当时我惹她生气了，她和方东仪来图书馆，泡馆顺便看看我？然后我就躲着她，但是还是给她发现了哈哈，那晚我还清楚记得是k歌的初赛，她去做评委，总的来说那个周末是比较关键的转折点。而另外一个对我们关系起推动作用的是舞蹈部的每周一约饭，她和育廷捧我做了舞蹈部部长，后来还把婧宜拉进了舞蹈部，后来我们的关系就是平稳中发展，一直到mini，那晚刷夜，很冷，我们一起睡在地上，她竟然握住了我的手，当时我就觉得很暖，也意识到我们的关系已经到达了一定阶段了，于是第二天我就想去表白，还在谭展宏的教唆下，那晚我和她一起走回宿舍，我们没有走平常回宿舍的路，而是绕到了岁月湖那边，我走在她后面，然后一下子从后面把她抱住，并且说了一些很肉麻的话，她马上就挣扎开了，然后我陪她去了音教，她自己回宿舍了，第二天晚上她约我出来走一下，我本以为成了，没想到还是没成功，这次的失败一定程度上促成了后面我们的成功，期末复习周我们一起在自习室复习，每天都泡在一起，期末考结束后，我们去南校演出，我没有令她满意，可能我们的暧昧关系，使我在公众场所表现得很不自然，我也不好意思在合唱团其他人面前表现得和她很暧昧的感觉，她就有点不开心，我也理解，这问题出在我身上，即使下学期我们关系再进一步，我也很难在合唱团其他人面前表现出那种她就是我要追的女孩的样子，毕竟大学第一次追别人，并且还是追一个这样的网红，怎么都觉得自己有点不配。这个小风波就这样结束了，她南校演出结束就马上飞去哈尔滨旅游了，而我也回家了，放假我们关系进一步进展，甚至到了视频聊天的阶段。大一下学期，我们选了同一个公选，一起去上课，关系也越来越暧昧，在某一个周末我们两个出去浪，我顺利地牵了她的手，这一牵就没松过了。3月份我们俩周末一起去了澳门一天游，也算是小约会了，总体来说还是玩得很开心的，虽然在威尼斯人那里迷路了很久，因为不能下赌场，所以总是要绕路。4月份我的生日，她带我去了boocafe，过了一晚属于我们的二人世界，那个小阁楼，我们一起跳舞，我抱起她，她躺在我身上看杂志，终于我没忍住，把我的初吻给了她，这一吻便一发不可收拾，我们的关系更加暧昧了，我很感谢她给了我这样一个难忘的生日，真的很感动，也在生日拿到她的吻，可以说是很完美了。这一吻之后的4月份可以说是痛苦与甜蜜双重奏，我们的关系已经进展到几乎情侣的阶段了，因此吵闹也增多了，可能是我不满足她内心所预想的模样，也可能是我第一次拍拖不太会经营感情，我一直都是被动地接受她对我的好，从来没有主动地对她好，花心思为她做过一点事，这也就导致我们的争吵比较多，而每次都是我去哄她，渐渐地磨去了我的耐性，而她也为了合唱团的事，维纳斯的事忙得不可开交，她们主席团也在冷战，当时的我同时也是很迷茫，我做了年度沸点大会的PM，也做了专场音乐会的资宣组长，也在弄转专业的事，大家都很忙，也没有互相体谅，最终在5月初的某个星期六，我们的关系彻底破裂了，那一个下午，我说要不我们一起去游个泳，我载着她去到游泳馆才发现，原来游泳馆下午不开，然后她就很不开心，可能觉得我没有事先调查清楚吧，然后就一直生气，那一晚我坐在岁月湖旁边的长凳上，坐了很久，后来我叫育廷来了，我们聊了很久，最后聊得我也哭了，我们也有聊到姚彤昕的事，聊完之后，我瞬间感觉姚彤昕在我心中的人设好像突然间崩塌了，可能是积压太久的怨气，也可能是育廷和我说的一些关于姚彤昕的生活的事，总之突然间，我对她的好感没了，这也是一件很奇怪的事，为什么对一个人的态度可以瞬间变化那么大，人设瞬间崩塌的感觉，那一晚她自己一个人去游泳了，还妥协叫了我，我犹豫了很久，最后还是去了，看到她之后，我们没有说一句话，我看着她，不知道该说什么，因为我不想哄，第二天，周日，当时我本来就计划好煲汤给她喝，并且顺便表白，我还问佳民借了她的小套间，没想到星期六发生了那样的事，可能上天也不想我继续这段感情，但是我还是自己一个人去买好材料，自己煲了汤，煮了菜，自己在房间里面看着她送我的纪念本，还有以前的聊天记录，很是伤感，最后我还是叫她来了，她来了之后，依旧表现出一副很冷漠的样子，我也没有想去哄，所以我们两个人在房子里就干坐着，自己做自己的事，最后大家还是一起走了，路上也是没有说什么。从那之后，我们的关系就破裂了，微信恢复以前的样子，一整天都没响几下，有一晚她叫我出来走走，我们走到岁月湖旁边我们以前经常坐的长凳，她觉得很累，社团生活的各种事快把她逼疯了，她躺我肩上的时候，我突然有种想挣扎开的冲动，我知道我们已经结束了，但是我又没有勇气说出口，于是就一直晾着，终于等到快闪当天，我们上午拍完快闪，下午赶去北理工，她给我发了信息，说我们彻底结束，到这为止，我大一的感情生活就告一段落了，我不知道为什么自己好像没有什么伤心难过的感觉，可能还是爱得不够，付出得不多吧，其实甚至说我俩一直都只是处于暧昧，压根还没有到达情侣。</p>
<p>大二到大四，我的感情生活全部给了郭鑫，其实如果大一不是姚彤昕闯进了我的生活，可能大学四年我都在跟郭鑫耗着。其实大一军训的时候，我就对郭鑫有所感觉了，然后分在一个英语班之后，才感受到她的强大，一口流利地道的英语口语，恰巧的是我们同在学生会的秘书处，还是一个班，理应该我们的关系应该很不错，但是大一一直到沸点大会之前，我们的关系都只是处于普通同学，平时也并没有什么交流，沸点大会，阴差阳错地我们一起做了PM，接触机会增多了，一起策划活动，但是都是工作上的接触，以前别人做PM，几个PM在一起都会擦出火花，甚至一起浪浪浪啊，但是我们就真的只是单纯的工作，因此这次活动我们并没有变成多熟，只是给了她一个印象，我是一个做事负责，很靠得住的人而已。大二去到东校，我就开始追她，但是和她聊天下来就觉得很无趣，中间也没有发生什么特别印象深刻的事，都是不温不火的，她可能会有时候过来问问我作业，所以我们的关系就真的只是普通同学，而我自己总是在yy，她可能对我有点感觉，但其实啥都没有，一直到4月份，寒假的时候冷爷给了我两张东团音乐会的票，日期是4月1号，正好是我生日前的一天，我就约了郭鑫请她去看，这好像是我们第一次单独出去，虽然后来她和我说她不是很喜欢看合唱，其实我也不是很喜欢，我也不会欣赏，但是回宿舍前，她竟然祝我生日快乐了，这是让我很吃惊的，她竟然记得我的生日，那天过后4月份，我明显感到她好像对我有点意思，她和我说想去话剧社看话剧，虽然最后没抢到票，然后我们的交流也变多了，但是大多数的也都是在学习上的交流，可能这都是我YY的，但是后来rrr竟然说她对我有好感，还和她聊过我，那我就更加自信的去找她聊天了，还想约她去吃饭，也不知道为什么突然间，她就对我冷漠起来，也不找我问作业了，真的是很奇怪，明明说对我有好感，但是又回避我，一下子搞得很尴尬，后面的关系都很尴尬了，实训见到面也不知道说什么，聊天也不知道聊什么，后来隔了几个月没聊天，最后还是我自己亲自打破了，我知道她考过了托福，我也要考托，就问她取经，不过也是简短的聊一下而已，后来她去了英国，我朋友圈托她带手信，时间到了9月份，大三刚开学，她约我出来吃饭，顺便给我手信，手信是一个英国伦敦的磁铁，还有一张明信片，她叫我看完之后给她个答复，明信片的内容写得很清楚，就是我的过度关心和关注使她受宠若惊，她不值得我那样对她，也是写得很委婉了，但是很明显就是拒绝了我了，为啥还要叫我给她一个答复呢，于是我就索性向她表白了，结果也很明显，她拒绝了我，虽然我们说好像没发生过一样，但是后面的日子，我们越来越疏远，她也不会再问我作业问题，没了这一环节，我们几乎就是几个月可以不聊天了，甚至有一次被迫和她还有思璐吃饭，也是很尴尬，这样一直维持到了大三下学期，当时我想改善一下我们的关系，正好想参加那个政务的大数据比赛，就拉上了她，那时正好也是我生日，就终于成功地和她约了次饭，后来她也因为要备考GRE而退赛了，就这样，我们的联系又断了，就这样过了大半年，期间她的生日我也没去祝福，一直到大四下学期开学前，她找了我想说大家彻底谈一下，于是我们边走内环边聊，终于走到一个公园那里坐下来，然后终于谈到了关键性问题，还聊到了很多她的价值观，爱情观，以及她对我的看法，她直言，其实她对我并不是很了解，因为在她眼里，我一直都是表现得很奇怪，所以奇怪就是她对我的唯一印象，不过仔细一想，我们真的也没啥太deep的交流，基本就是她问我作业，我回答，过了大三没课之后，也就基本不会有交流，所以我和她充其量就是一个可能关系比较好的同班同学罢了。然后也有聊到她的爱情观，她说同龄的男生都太幼稚了，她应该会找比她大的成熟的男人，至于平时她也不会是一个喜欢找别人聊天倾诉的女生，她内心独立坚强，自主，果然是和一般的女生不太一样，也难怪说谭展宏说她是BOSS级别，我还是太弱了。聊过之后，我们的关系好转了些，后来断断续续地也在微信聊了下天，就到了4月份，又是我生日了，我们又是一起吃了饭，为了和她约饭，我还专程从深圳回来广州，不过还是值得的。生日过后就几乎没啥联系了，甚至我深圳回来广州，告诉她我回来了，也没有回复，估计是没看到，就这样差不多一直到了毕业季，拍毕业照，我们俩终于有了一张照片了，去掉大一沸点那张和宇翔一起的三人照，我和她后面三年就一张照片都没有，最后留张毕业照也算是做纪念了。时间一直到在学校的最后一天，也就是6月28号，那一天是领学位证和毕业证的日子，那一天的前一个晚上，她说写了点东西给我，其实我猜到她会写给我，其实我一早就想写给她，但是看着我们的关系也就一直都这样，并且我很懒，就没写什么了，那现在她说她都已经写了东西给我了，我就顺势也说我写了个她，虽然她不信，我也强作镇定说就是这样，我一早就写好了，就等你先开口。过后，我急急忙忙地写了张明信片，第二天一大早还把我们那张唯一的毕业照打印出来，买了个相框放进去，没想到那相框还不太管用，时间太急，我也没去弄那个相框，就连着明信片一起给她了。回去我看了她的信，信一开头就说以她对我的了解，我是不会给她写东西的，确实我没写，我是假装我已经写了，其实是后补的，后来她好像还是发现了我是临时准备的，但是我还是死口不认，我才不管了，反正也是学校的最后一天了，打死不认。看完她的信，我是真的对我和她的未来彻底不抱希望了，感觉我们真的不会有未来，就这样，追了她三年，最后也就没有结果地毕业了。</p>
<p>至于大学里面其他的同学，还真的没有特别熟的，韩硕轩可以算一个，经常互相调戏为儿子，也经常晚上一起打球，除此之外也没有太多交集了。至于梁泳嘉，大四前，我们可以算是比较熟的，她以前喜欢陈煌辉，也经常找我聊，我喜欢郭鑫的事她也知道，后来她找了男朋友之后，我们的聊天就少了，她男朋友也不喜欢她和我聊得太多，久而久之，我们就疏远了，一直到大四下学期有一次我回去珠海，本来是想和她和婧宜一起吃饭，没想到她男朋友也在，还说要我和她男朋友聊一聊，是关于不要在骚扰梁泳嘉的问题，我当时就无语了，我怎么了吗？瞬间就爆了粗，说了句叼你（粤语），她男朋友马上怼我：叼你（粤语）我还是能听得懂的，我瞬间转身离开，不想再和这个人说半句话，就这样我和梁泳嘉的关系彻底破裂，6月专场我回去，也没和梁泳嘉有什么交流，后来她主动请求合好，我才和她稍微说了几句话。现在毕业了，微信一天都是不响的，和大学的同学也再没有联系，也没有人联系我，也印证了我前面说的，我在大学并没有真正的朋友，不知道这是不是一件可悲的事呢？</p>
<p><em>写在最后</em></p>
<p>四个篇章都以悲剧收尾，宿舍篇道出了和他们同为舍友的无奈；合唱团篇高开低走，最后也是以悲剧收尾；学院篇最后以移动信息工程学院的消亡结束；朋友篇最后变成了无朋友。写到这里，不知不觉写了上万字了，基本上就是回忆录和流水账，希望以后老了的自己看到这些东西还能有所触动吧。</p>
<p><br></p>
]]></content>
  </entry>
  <entry>
    <title>北京上海那些事</title>
    <url>/2018/10/03/%E5%8C%97%E4%BA%AC%E4%B8%8A%E6%B5%B7%E9%82%A3%E4%BA%9B%E4%BA%8B/</url>
    <content><![CDATA[<p>保研那些事已经在上一篇文章中分享了，保研面试的另外一个好处就是可以去各大城市和高校参观，9月份，我去了北京和上海这两座城市，虽然待的时间不长，但是也参观了一些地方，见过一些人，做过一些事，这篇文章想把这一段经历记录下来，顺便分享一些奇闻轶事。</p>
<span id="more"></span>
<h3 id="北京篇"><a href="#北京篇" class="headerlink" title="北京篇"></a>北京篇</h3><p>说实话，这是我第一次去北京，说起来都有点羞愧，当我还没到北京的时候，我对北京的第一印象就是：贵！！当时我提前订酒店，上网一查，酒店清一色的400+一晚，当时我就懵逼了，南京和上海的酒店我也住过，也就200+一晚，北京直接来了个double，逼得我只能跟同学一起合租双人房。当我进入北京城，亲身感受到真正的北京后，我对北京的印象从”贵”转变成了”破”，北京的很多建筑和公共设施都有一种破旧的感觉，不是那种历史文物的那种破，而是由于时间太久且没有翻新的那种破。</p>
<p>由于这次上京的主要任务是去面试（虽然面试很水），所以没有太多的时间去逛北京城，参观旅游景点，北京的旅游景点是真的很多，天安门、故宫、长城等等，而我真正自由的时间就只有一天半，因此我只挑了清华附近的颐和园和圆明园参观，颐和园是皇帝的后花园，而故宫是皇帝的家，颐和园属于国家级旅游景点，因此门票也是白菜价格，颐和园的景点主要以山水寺庙为主，不像故宫以宫殿为主，而颐和园里面的每个景点都有一定的历史，如苏州街、败家石、石船，颐和园内国内外游客都很多，还有很多导游，基本每一个景点都能遇到一个正在讲解的导游，这时候你就可以靠过去听。而圆明园是古代皇帝修建的一个中西方结合的高级园林，第二次鸦片战争英法联军火烧圆明园后，就成了现在的遗迹，里面比较著名的就是大水法（历史书上的照片），十二生肖的一个园林，以及迷宫。</p>
<p>清华大学不愧是中国最大的校园，不骑自行车根本没法生存，校园绿化比较多，建筑风格中西合璧，整体给我的感觉很舒服，每一个校门都是一个自然的景点，其中二校门最为出名，其实二校门现在并不是一个校门，由于清华经历过扩建，二校门属于扩建前的校门，但是它最具历史性。另外清华给我的第二感觉就是很像中大南校，同样有一个大草坪，草坪中央是一个大礼堂，同样晚上有很多老人散步，同样存在旧区（惊奇地发现有老人还在打麻将）。</p>
<p>吃的方面由于待的时间不多，也没有专门去找攻略，就只和北京的同学去尝试了一下北京菜馆“局气”，以及小吊梨汤。<br>另外北京9月的天气很舒服，昼夜温差略大，但对于一个广东孩子，终于能感受到什么是秋天了。</p>
<img src="/2018/10/03/%E5%8C%97%E4%BA%AC%E4%B8%8A%E6%B5%B7%E9%82%A3%E4%BA%9B%E4%BA%8B/%E9%A2%90%E5%92%8C%E5%9B%AD%E4%BD%9B%E9%A6%99%E9%98%81.png" class="" width="700" height="700" title="佛香阁">
<img src="/2018/10/03/%E5%8C%97%E4%BA%AC%E4%B8%8A%E6%B5%B7%E9%82%A3%E4%BA%9B%E4%BA%8B/%E9%A2%90%E5%92%8C%E5%9B%AD%E4%B8%80%E9%9A%85.png" class="" width="700" height="900" title="颐和园一隅">

<img src="/2018/10/03/%E5%8C%97%E4%BA%AC%E4%B8%8A%E6%B5%B7%E9%82%A3%E4%BA%9B%E4%BA%8B/%E5%9C%86%E6%98%8E%E5%9B%AD%E5%A4%A7%E6%B0%B4%E6%B3%95.png" class="" width="700" height="700" title="大水法">
<img src="/2018/10/03/%E5%8C%97%E4%BA%AC%E4%B8%8A%E6%B5%B7%E9%82%A3%E4%BA%9B%E4%BA%8B/%E5%9C%86%E6%98%8E%E5%9B%AD%E8%BF%B7%E5%AE%AB.png" class="" width="700" height="900" title="大迷宫">
<img src="/2018/10/03/%E5%8C%97%E4%BA%AC%E4%B8%8A%E6%B5%B7%E9%82%A3%E4%BA%9B%E4%BA%8B/%E4%B8%9C%E9%97%A8.png" class="" width="700" height="700" title="清华东南门">
<img src="/2018/10/03/%E5%8C%97%E4%BA%AC%E4%B8%8A%E6%B5%B7%E9%82%A3%E4%BA%9B%E4%BA%8B/%E4%BA%8C%E6%A0%A1%E9%97%A8.png" class="" width="700" height="700" title="清华二校门">
<img src="/2018/10/03/%E5%8C%97%E4%BA%AC%E4%B8%8A%E6%B5%B7%E9%82%A3%E4%BA%9B%E4%BA%8B/%E6%B0%B4%E6%9C%A8%E6%B8%85%E5%8D%8E.png" class="" width="700" height="700" title="水木清华">
<h3 id="上海篇"><a href="#上海篇" class="headerlink" title="上海篇"></a>上海篇</h3><p>这是我第二次去上海，上一次还得追溯到上大学前，上海这个城市给我的感觉就是很有活力，少了一份北京的那种历史感，但是多了一份朝气。公交车站的命名方式也是unique，清一色的叫xx路xx路，复旦大学张江校区附近的公交车站，惊现各种名人路，比如祖冲之路，张衡路等等，并且上海的公交会以上海话播报，很有意思。复旦的张江校区灰常的普通，没有标志性建筑，整体感觉像某技术学校。今次去上海除了去复旦面试外，还参观了上交的闵行校区和被人称为复旦后花园的小破财：上海财经大学，据某狗所说，上交的闵行校区的面积仅次于清华，果真如此，学校内竟然有出租小电驴，类似共享单车，但是上交不像清华里面有旧区，上交的每一寸土地都是教学区域，建筑之间比较稀疏，也没有明显的生活区和教学区的分界线。小破财闻名不如见面，半小时走完校园，面积可能比纪中还小。</p>
<p>上海的CBD，静安区附近相当的繁华，南京路也有很多外国人出入，而去上海最值得去做的一件事就是去看展览，上海的展览很多，我挑了上海展览中心，当天是一个photofair，门票算是比较良心，同样看展和展览的外国人也很多。其实上海并没有北京那么多的旅游景点，因为这座城市比较年轻，因此也比较现代化，没有太多的历史沉淀，但去上海一定要去外滩，还有东方明珠、上海中心一看，毕竟是上海的地标性建筑了。</p>
<img src="/2018/10/03/%E5%8C%97%E4%BA%AC%E4%B8%8A%E6%B5%B7%E9%82%A3%E4%BA%9B%E4%BA%8B/%E5%87%AF%E6%97%8B%E9%97%A8.png" class="" width="700" height="700" title="上交凯旋门">
<img src="/2018/10/03/%E5%8C%97%E4%BA%AC%E4%B8%8A%E6%B5%B7%E9%82%A3%E4%BA%9B%E4%BA%8B/%E5%BA%99%E9%97%A8.png" class="" width="700" height="700" title="上交庙门">
<img src="/2018/10/03/%E5%8C%97%E4%BA%AC%E4%B8%8A%E6%B5%B7%E9%82%A3%E4%BA%9B%E4%BA%8B/%E4%B8%8A%E6%B5%B7%E8%B4%A2%E7%BB%8F%E5%A4%A7%E5%AD%A6.png" class="" width="700" height="700" title="小破财">
<img src="/2018/10/03/%E5%8C%97%E4%BA%AC%E4%B8%8A%E6%B5%B7%E9%82%A3%E4%BA%9B%E4%BA%8B/%E7%94%B0%E5%AD%90%E5%9D%8A.png" class="" width="700" height="700" title="田子坊">
<img src="/2018/10/03/%E5%8C%97%E4%BA%AC%E4%B8%8A%E6%B5%B7%E9%82%A3%E4%BA%9B%E4%BA%8B/%E4%B8%8A%E6%B5%B7%E5%B1%95%E8%A7%88%E4%B8%AD%E5%BF%83.png" class="" width="700" height="700" title="上海展览中心">
<p><br></p>
]]></content>
  </entry>
  <entry>
    <title>保研历程分享</title>
    <url>/2018/09/27/%E4%BF%9D%E7%A0%94%E5%8E%86%E7%A8%8B%E5%88%86%E4%BA%AB/</url>
    <content><![CDATA[<p>从今年3月开始关注保研的信息，到9月份的尘埃落定，半年里奔跑于各一大城市，各种交流面试，这一路保研的历程充满着焦虑、紧张到最后的喜悦，现在回想起来，总有值得总结和分享的地方。这篇文章将我的保研历程分为三部分：前期准备、夏令营和九月推免。</p>
<span id="more"></span>
<p>夏令营中最后悔的是没有申请中科院，无论是计算所、软件所还是自动化所，都很值得去尝试；上交的报名系统是我见过最简单的，没有之一，像平常问卷那样，不用上传任何材料；北大本部是我一心向往却无法接近的存在（同清华）</p>
<ul>
<li>夏令营申请：清华叉院，清华伯克利，北大信科，北大信工，北大叉院，南大AI学院，复旦计科，上交计科</li>
<li>夏令营入营：清华伯克利，北大信工（推掉），南大，复旦（与考试冲突）</li>
</ul>
<p><br></p>
<h3 id="前期准备"><a href="#前期准备" class="headerlink" title="前期准备"></a>前期准备</h3><p>3月份开始，各大论坛和学院官网就会公布夏令营的报名信息，一般的夏令营都会在7月上中旬举行，但是也有6月份的，如清华叉院，南大lambda实验室（为了提前抢人）。然后就是准备各种申请材料，包括简历、个人陈述、研究计划、推荐信以及各种获奖证书，一般情况，材料只需要中文即可，但是也存在某些院要求中英文（如清华的某院），所以最好就是两个版本都准备一下。</p>
<p>材料准备的过程十分漫长和煎熬，跟出国的材料基本一致，只是国内夏令营的材料水分更大，不需要特别的精雕细刻。准备材料的时候，你会感受到自己的中文和英文都无比的差 &gt;_&lt;</p>
<ul>
<li>简历：英文简历最好用latex，中文简历自己找个像样的模板即可。</li>
<li>PS：保证一定的叙事性，同时加入一定的学术性，可以说是非常难了。</li>
<li>研究计划：并不是简单的写自己未来想做什么，要写出文献综述的感觉，总结过去的工作，找出研究问题的关键之处，并且给出有一定可行性的解决方案作为未来的研究计划。</li>
<li>推荐信：国内的推荐信基本都是学生写，然后老师签名，与申请国外学校不一样，国内推荐信只是形式；而信的内容就是各种自夸。</li>
</ul>
<p>另外：强烈推荐做一个自己本科阶段科研工作展示报告的ppt，用处很大，面试的时候也可以给老师看，这比老师看简历要来得更加容易。</p>
<p>材料准备完，然后就去学校夏令营的报名系统报名，然后寄纸质材料。如果没有推荐信的话，我觉得夏令营的申请都比较容易，但是由于推荐信需要老师亲自签名，所以经常会出现拿着好几封推荐信要老师签名的情况，老师找得太多次又不好意思，然而推荐信的作用在所有申请材料里面基本为 0，这个形式上的推荐信，我是非常想吐槽的！！！</p>
<hr>
<p>至此，夏令营申请基本完成了，然后你就可以每天进行焦急的等待。</p>
<p>但如果你有明确的研究方向，或者你有明确的导师意向，你就可以提前进行套磁，套磁的过程基本就是发一封邮件给有意向的导师进行自荐，并且附上自己的简历等材料。套磁的结果需要理性分析，并不能太过先入为主，要分析老师的回复是套话还是真的对你有兴趣；我发了邮件给十几个老师，清华的老师基本就是不回，或者回复名额已满；北大的sx老师直接回复“欢迎报考xxx院，我那里还有名额，我会重点关注你”，给了我一种老师对我很有兴趣的假象，然而北大本部岂是我这种渣渣能进去的地方；复旦hxj老师直接回复“夏令营进了再跟我说”，拒绝套磁；南大dxy老师反问有无读博的想法……</p>
<p>总之，套磁有一定成功率，但是也不要报太大的希望，虽然说老师有主导权，但是就凭简单的几封邮件交流，老师又怎么能一定保你能进呢？</p>
<p><br></p>
<h3 id="夏令营"><a href="#夏令营" class="headerlink" title="夏令营"></a>夏令营</h3><p>本渣只去了两个夏令营，清华伯克利和南大，复旦由于期末考推掉，北大信工由于夏令营不发offer且没有感兴趣的老师也推掉了。</p>
<h4 id="清华伯克利（TBSI）"><a href="#清华伯克利（TBSI）" class="headerlink" title="清华伯克利（TBSI）"></a>清华伯克利（TBSI）</h4><p>安排住宿且有交通费补贴，夏令营三天，第一天报道，第二天开营宣讲加实验室参观，第三天面试，无机试，TBSI招收的学生面比较广，对编程能力要求不高，基础知识和英语比较重要。面试如下</p>
<ul>
<li>全程英语</li>
<li>面试前阅读英文文献，内容是关于语音识别的，面试时会提问</li>
<li>简单的知识问答，如dropout是什么，距离度量有哪些等</li>
<li>开放性问题：喜欢哪个实验室，GPA5分制解释一下，为什么那么多人参加美赛，最喜欢的编程语言</li>
<li>气氛比较轻松，老师也比较nice，但是被安排在当天的最后一个面试。</li>
</ul>
<p>面试完十几分钟，就有老师联系我说想让我跟他做毕设，他就是我未来的老师，起初还是各种纠结，最后还是决定先答应着，然后参观了他们的实验室，和老师聊了一下，老师人很nice，听了他们的组会，老师的指导也蛮到位</p>
<h4 id="南大计算机系"><a href="#南大计算机系" class="headerlink" title="南大计算机系"></a>南大计算机系</h4><p>南大国际四星级酒店，超级豪华，夏令营三天，第一天报道，第二天自由交流，第三天机试面试。南大会有一个意向接受导师签名的东西，相当于是一个第一志愿，一般你对某个老师感兴趣，就找他签名，老师不会拒绝。</p>
<ul>
<li><p>交流</p>
<ul>
<li>dxy老师<ul>
<li>研究方向对口的老师，同NLP</li>
<li>简单介绍下项目，问了下实验结果</li>
<li>想读硕士还是博士</li>
</ul>
</li>
<li>hsj老师<ul>
<li>跟dxy老师同一个组</li>
<li>叫我英文介绍项目，好在我TBSI那时准备过</li>
<li>项目有没有能改进的地方，全程瞎逼，天马行空</li>
</ul>
</li>
<li>lm老师<ul>
<li>lambda实验室的老师，lambda的名额大部分都已经提前招完了，还有少数留在夏令营</li>
<li>同样项目介绍</li>
<li>给8分钟阅读他的论文，概括论文整体思想</li>
<li>IQ题</li>
</ul>
</li>
</ul>
</li>
<li><p>机试</p>
<ul>
<li><p>共3道题，按样例给分，共300分</p>
</li>
<li><p>1）找规律题：给定一个正整数 $n$，计算所有 $n$ 位长度的二进制串且没有连续 $1$ 的个数，如 $n=3$，有 000, 001, 010, 100, 101。找下规律，就可以推出递推公式。</p>
</li>
<li>2）搜索题：给定一个正整数 $n$，从 $1$ 到 $n$ 中选择 $n-1$ 个数，以随机的顺序拼接成字符串 $s$，给定这个字符串，找出缺失的那个数字</li>
<li>3）动归：题目忘了。。。</li>
</ul>
</li>
<li><p>面试</p>
<ul>
<li>3个老师分别对应三个大方向问题，面试10分钟</li>
<li>第一个老师问项目</li>
<li>第二个老师问算法，分治法是什么，动归是什么，分治和动归的差别是什么，当场眩晕。。</li>
<li>第三个老师问操作系统，死锁和怎么预防死锁</li>
<li>最后补了一个英文问题：最喜欢的编程语言</li>
</ul>
</li>
</ul>
<p>虽然南大面试感觉不好，但是机试发挥比较正常，最后也如愿拿到offer，然后又是各种纠结，对比南大和TBSI，最后还是选择了TBSI，拒绝了南大。</p>
<p><br></p>
<h3 id="九月推免"><a href="#九月推免" class="headerlink" title="九月推免"></a>九月推免</h3><h4 id="TBSI"><a href="#TBSI" class="headerlink" title="TBSI"></a>TBSI</h4><p>TBSI九月份的面试纯属走流程，水到爆炸，但我又是最后一个面试。。面试前同样看文献，这次是nature上面一篇关于AlphaGo Zero的文章，然后面试过程基本就是随便问下项目，然后就是open discussion，讨论阿法狗以及强化学习等，全程瞎逼逼，不懂强化学习的渣渣路过</p>
<h4 id="中大"><a href="#中大" class="headerlink" title="中大"></a>中大</h4><ul>
<li>机试不算分数，10道题</li>
<li>面试注重基本知识，如高数线代，问了微分中值定理，线性方程组有解的必要条件，还有就是开放性问题，机器学习和深度学习不同点</li>
</ul>
<h4 id="复旦"><a href="#复旦" class="headerlink" title="复旦"></a>复旦</h4><ul>
<li>机试<ul>
<li>给定一条直线和一个圆，求直线在圆上的长度。分情况讨论即可</li>
<li>给定一个数组，在某一位置的左边，求比该数大的且距离最近的位置</li>
<li>面试经典题：一个数组，求三个数字相加等于某一个数的组合数</li>
</ul>
</li>
<li>面试<ul>
<li>自我介绍，聊项目，问了一下项目相关的一些论文</li>
<li>聊完项目后，问有无其它学校offer，回答若为有，面试结束，回答若为无，面试官质疑你，面试也结束。  </li>
</ul>
</li>
</ul>
<p><br><br><em>写在最后</em></p>
<p>我的选择是清华伯克利，一方面是因为清华的学位确实很吸引，另一方面是我一直有一个留学的梦想，但是由于种种原因最后放弃，但是现在有一个去伯克利修读一年拿硕士学位的机会摆在我面前，我不想放弃，虽然清华伯克利学院在深圳，且办学年限补偿，不如北京本部历史积累丰富，但是我相信这个联合培养计划不会像中大和卡耐基那样突然中止，我也相信我能够成功毕业，拿到双学位，祝愿我吧。</p>
<p>托福GRE我又要来了（我以为我不会再见你了）。</p>
<p><br></p>
]]></content>
  </entry>
</search>

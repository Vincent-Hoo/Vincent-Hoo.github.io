---
title: 【概率论与数理统计】———— 随机变量的数字特征
date: 2018-10-22 20:10:00
mathjax: true
tags:
- 概率论
---


> 课程名称：概率论与数理统计
> 开设学校：中科大
> 课程平台：icourse
> 第三章：随机变量的数字特征，研究随机变量的几种重要的数字特征，如期望、方差、协方差，以及讲述连接概率论和数理统计的两大重要定理：大数定理和中心极限定理。

<!-- more -->


*preface*

概率论主要研究随机事件发生的概率，最开始我们研究单独的一个随机事件，后来研究一串事件，我们用随机变量来研究它，通过研究随机变量的分布，可以知道随机事件能取哪些值，以及取不同值的概率。知道了随机变量的分布，就能对随机变量的全部概率特征有了全面的理解，它是最高境界，但是随机变量的分布不容易得到，因为分布是最宏观的理解。而有时候我们也不一定需要去了解随机变量的整个分布，我们关心的可能只是一些数字特征，如均值。数字特征分为两类，位置参数和刻度参数，位置参数包括数学期望、中位数、众数，刻度参数包括方差、标准差。另外的包括矩、协方差、相关系数。

<br>

# 数学期望
## 离散型随机变量

- 定义：若随机变量 $X$ 为离散型随机变量，其概率分布为 $P(X = x_i) = p_i, i = 1,2,3,...$，若 $\sum_{i=1}^n p_ix_i < \infty$，则称 $\sum_{i=1}^n p_ix_i < \infty$ 为随机变量 $X$ 的数学期望，记为 $EX$ 或 $E(X)$ 或 $\mu$ 
- 注意：$EX$ 是一个数，非随机
- 数学期望又叫均值，是 $X$ 的可能取值与其概率乘积的累加，看成加权平均
- 几个经典分布的数学期望
  - $X \sim B(n, p)$，$E(X) = np$
  - $X \sim P(\lambda)$，$E(X) = \lambda$ 

## 连续型随机变量

- 定义：若随机变量 $X$ 为连续型随机变量，概率密度函数为 $p(x)$，若积分 $\int_{-\infty}^{+\infty} xp(x)dx$ 绝对收敛，则称积分 $\int_{-\infty}^{+\infty} xp(x)dx$ 的值为随机变量 $X$ 的数学期望
- 设 $(X,Y)$ 为二维连续型随机变量，则
  - $E(X) = \int_{-\infty}^{+\infty} xf_X(x)dx = \int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty} xf(x,y)dxdy$ 
  - $E(Y) = \int_{-\infty}^{+\infty} yf_Y(y)dx = \int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty} yf(x,y)dxdy$ 

- 几个经典分布的数学期望
  - $X \sim U(a,b)$，$E(X) = \frac{a+b}{2}$，均分分布的数学期望位于区间的中点。
  - $X \sim Exp(\lambda)$，$E(X) = \frac{1}{\lambda}$
  - $X \sim N(\mu, \sigma^2)$，$E(X) = \mu$

## 随机变量函数的数学期望

- $X$ 为离散型随机变量，$Y = g(X)$，$E(Y) = \sum_{k=1}^{\infty} g(x_k)p_k$ 
- $X$ 为连续型随机变量，$Y = g(X)$，$E(Y) = \int_{-\infty}^{+\infty} g(x)f(x)dx$，积分就是求和的极限，$f(x)dx$ 就相当于某一点附近的概率
- $(X,Y)​$ 是二维随机变量，$Z = g(X,Y)​$
  - 若是离散型随机变量，$E(Z) = \sum_{i=1}^{\infty}\sum_{j=1}^{\infty} g(x_i, y_j) p_{ij}$ 
  - 若是连续型随机变量，$E(Z) = \int\int g(x,y)f(x,y)dxdy$ 

## 数学期望的性质

1. $C$ 是常数，$E(C) = C$
2. $k$ 是常数，$E(kX) = kE(X)$ 
3. $E(X + Y) = E(X) + E(Y)$，$E(X-Y) = E(X )-E(Y)$
4. 若 $X$ 和 $Y$ 相互独立，则 $E(XY) = E(X)E(Y)​$，反过来不能推独立 

<br>

# 方差和标准差

- 定义：设 $X$ 为随机变量，称 $E((X-EX)^2)$ 为随机变量 $X$ 的方差，其算术平方根称为标准差，分别记为 $Var(X)$ 和 $\sigma$ ，$D(X) = Var(X) = \sigma^2$ 
- 方差是刻画随机变量在其中心位置（期望）附近波动程度的一种数字特征
  - $EX$ 为随机变量 $X$ 的均值，$X - EX$ 为随机变量距离均值的距离，为了防止距离正负抵消，所以平方处理，但其实所表示的数学意义和 $E(|X- EX|)$ 是一样的
  - 当我们讨论数学性质的时候，用方差比较多；在实际应用时，用标准差比较多，因为标准差与 $X$ 有相同的量纲
- 计算：$Var(X) = E((X-\mu)^2) = E(X^2 - 2\mu X + \mu^2) = E(X^2) - 2\mu E(X) + \mu^2$，由于 $E(X) = \mu$，所以 $Var(X) = E(X^2)-(E(X))^2$ ；或者用方差的定义算，将 $(X-EX)^2$ 看成一个新的随机变量，再算其数学期望。
- 常见分布的方差
  - $X \sim B(n, p)$，$Var(X) = np(1-p)$
  - $X \sim P(\lambda)$，$Var(X) = \lambda$
  - $X \sim Exp(\lambda)$，$Var(X) = \frac{1}{\lambda^2}$
  - $X \sim N(\mu, \sigma^2)$，$Var(X) = \sigma^2$
- 方差的性质：
  - 设 $C$ 是常数，则 $Var(C) = 0$，常数没有波动，所以方差为0
  - 若 $a$，$b$ 是常数，则 $Var(aX+b) = a^2Var(X)$
    - $E(aX+b) = aE(X) + b$
    - $Var(aX+b) = E[aX+b - E(aX+b)]^2 = E[aX+b-aE(X) - b]^2$ 
      ​                         $=E[a(X-E(X))]^2=a^2E(X-E(X))^2=a^2Var(X)$ 
  - 若 $X$，$Y$ 相互独立，$Var(X+Y)=Var(X) + Var(Y)$ 
    - 注意区分期望的这条性质，在期望中，$E(X+Y)=E(X)+E(Y)$，是不需要独立条件
    - $Var(X+Y) = E[X+Y-(E(X)+E(Y))]^2 = E[(X-E(X)) + (Y-E(Y))]^2$ 
      ​                        $=E[(X-EX)^2 + (Y-EY)^2-2(X-EX)(Y-EY)]$  
      ​                        $=Var(X) + Var(Y)-2E[(X-EX)(Y-EY)]$ 
    - 因为随机变量$X$ 和 $Y$ 相互独立，$X-EX$ 和 $Y-EY$ 也相互独立，因此
      - $E[(X-EX)(Y-EY)] = E(X-EX)E(Y-EY) = 0*0 = 0$
    - 所以，$Var(X+Y) = Var(X) + Var(Y)$
- 随机变量的标准化
  - 假设随机变量 $X$ 的期望是 $\mu$，方差是 $\sigma^2$，则 $X^n = \frac{X-\mu}{\sigma}$ 为 $X$ 的标准化随机变量，易见 $E(X^n) = 0$，$Var(X^n) = 1$

     
<br>




# 马尔科夫不等式

- 假设 $X$ 是一个非负的随机变量，则对任意的 $\epsilon > 0$，$P(X \ge \epsilon) \le \frac{E(X)}{\epsilon}$ ，马尔科夫不等式将概率和期望联系在一起
- 证明：$E(X) = \int_0^{+\infty} xf(x)dx > \int_{\epsilon}^{+\infty}xf(x)dx > \epsilon\int_{\epsilon}^{+\infty}f(x)dx = \epsilon P(X \ge \epsilon)$ 
  - 所以，$P(X \ge \epsilon) \le \frac{E(X)}{\epsilon}$ 
- 特例：令随机变量 $Y = (X - EX)^2$，$Y$ 满足非负的条件，令 $\epsilon = \epsilon_1^2$
  - 则，$P(Y \ge \epsilon) = P(|X-EX| \ge \epsilon_1) \le \frac{E(Y)}{\epsilon} = \frac{Var(X)}{\epsilon_1^2}$  
  - 这是马尔科夫不等式的一个特例，叫做**切比雪夫不等式**：$P(|X-EX| \ge \epsilon) \le  \frac{Var(X)}{\epsilon^2}$
- 例子：需要调查吸烟率 $p$ ，需要调查多少人才能保证频率和与真正吸烟率的差不超过 $0.005$ 的概率不低于 $0.95$
  - 假设调查的人数为 $n$，吸烟的人数为 $n_a$，那么频率 $f=\frac{n_a}{n}$，题意要求 $P(|f-p| < 0.005) \ge 0.95$，则 $P(|n_a-np| < 0.005n) \ge 0.95$
  - $n_a \sim B(n, p)$，二项分布的期望为 $E(n_a) = np$，则上面要求的可以看成$P(|n_a-E(n_a)| < 0.005n) \ge 0.95$，其对立事件为 $P(|n_a-np| \ge 0.005n) \le 0.05$ 
  - 利用马尔科夫不等式，$P(|n_a-E(n_a)| \ge 0.005n)  < \frac{np(1-p)}{0.005^2n^2}$ 
  - $p(1-p)$ 的最大值为 0.25，所以 $\frac{np(1-p)}{0.005^2n^2} \le \frac{1}{4*0.005^2n} \le 0.05$，所以 $n = 40000$

<br>

# 矩
- 设 $X​$ 为随机变量，若 $E(X^k), k = 1,2,3,...​$ 存在，则称它为 $X​$ 的k阶原点矩
- 设 $X$ 为随机变量，若 $E((X - EX)^k), k = 1,2,3,...$ 存在，则称它为 $X$ 的k阶中心矩
- 数学期望是一阶原点矩，方差是二阶中心矩
- 在实际应用中，高于4阶的矩很少使用，3阶中心距用来衡量随机变量分布是否有偏差，4阶中心矩用来衡量随机变量分布在均值附近的陡峭程度

<br>  

# 协方差与相关系数

之前提到的方差、期望、矩都是描述一个随机变量的数字特征，反应随机变量的某一特性。而协方差和相关系数是描述两个随机变量关系的数字特征
- 协方差定义：$Cov(X,Y) = E[(X-EX)(Y-EY)]$，若 $X=Y$，$Cov(X,Y) = Var(X)$，所以协方差是方差的一个推广
- 协方差的性质
  - $Cov(aX+b, cY+d) = acCov(X,Y)$ 
    - $Cov(aX+b, cY+d) = E[(aX+b - aEX - b)(cY+d - cEY - d)] $ 
      $= E[ac(X-EX)(Y-EY)] = acCov(X,Y)$  
  - $Cov(aX+bY, cX+dY) = acVar(X)+adCov(X,Y) + bcCov(Y,X) + bdVar(Y)$ 
  - 若随机变量 $X$ 和 $Y$ 独立，那么 $Cov(X,Y) = 0$，协方差为0，则两个随机变量不相关
    - 随机变量 $X$ 和 $Y$ 独立，则随机变量 $X-EX$ 和 $Y-EY$ 独立
    - $E[(X-EX)(Y-EY)] = E(X-EX)E(Y-EY) = 0 \times 0=0$ 
    - 两个随机变量独立，必定不相关；但是不相关，不一定独立
      - $Cov(X,Y) = E[(X-EX)(Y-EY)] = E(XY) - EXEY$ 
      - 若两个随机变量不相关，则 $E(XY) = EXEY$，但是这推不出随机变量独立
  - $Cov^2(X,Y) \le Var(X)Var(Y)$，等号成立，当且仅当 $X$ 和 $Y$ 存在严格的线性关系，如 $Y=aX+b$ 

但是协方差描述的两个随机变量的关系，并没有考虑随机变量本身的量纲，如身高和体重之间的关系，单位不一样，因此又引入了相关系数 
- 相关系数的定义：$Corr(X,Y)= \rho_{x,y}= \frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}}$  
- 相关系数的性质
  - $|\rho_{x,y}| \le 1$，因为 $Cov^2(X,Y) \le Var(X)Var(Y)$ 
  - 若 $|\rho_{x,y}| = 0$，则两个随机变量不相关，严格来说，是线性不相关，即随机变量 $X$ 和 $Y$ 不存在严格的线性关系，但可能存在其他的函数关系，下面是一个反例
    - $X \sim U(-\pi,\pi)$，$Y = cos(X)$，$EX=0$，$EY=0$，$Cov(X,Y) = 0$
    - 协方差为0，证明不相关，但只是说明，两个随机变量没有线性相关性
  - 若 $\rho_{x,y} = 1$，则 $Y = aX + b$，且 $a > 0$，称两个随机变量是严格的线性关系
  - 若 $\rho_{x,y} = -1$，则 $Y = aX + b$，且 $a < 0$，称两个随机变量是严格的线性关系
  - 若 $\rho_{x,y} \in (0,1)$，则两个随机变量正相关，称两个随机变量存在相关关系 
  - 若 $\rho_{x,y} \in (-1,0)$，则两个随机变量负相关，称两个随机变量存在相关关系 
  - 若两个随机变量独立，则相关系数为0；但相关系数为0，不能推出两个随机变量独立
    - 特例：二元正态分布，$(X,Y) \sim N(\mu_1,\mu_2, \sigma_1^2, \sigma_2^2, \rho)$，其中 $\rho$ 为相关系数，则 $X$ 和 $Y$ 独立当且仅当 $\rho = 0$ 

<br>

# 大数定律

- 如果对于任何 $\epsilon > 0 $，都有 $lim_{n \rightarrow \infty} P(|\xi_n - \xi| > \epsilon) = 0$，那么我们称随机序列 $\xi_n$ 依概率收敛到 $\xi$，记为 $\xi_n \rightarrow^p \xi$ 
- （弱）大数定律：{$X_n$} 是一列独立同分布的随机变量序列，具有共同的数学期望 $\mu$，共同的方差 $\sigma^2$，则 $(\bar{X} = \frac{1}{n}\sum_{k=1}^n X_k) \rightarrow^p \mu$，称 {$X_n$} 服从（弱）大数定律    
  - 大数定律描述的事情就是：$lim_{n \rightarrow \infty} P(|\bar{X} - \mu| > \epsilon) = 0$ 
  - 利用切比雪夫不等式：$P(|X-EX| \ge \epsilon) \le  \frac{Var(X)}{\epsilon^2}$
  - $E(\bar{X}) = \mu$，$Var(\bar{X}) = \frac{\sigma^2}{n}$ 
  - 所以，$P(|\bar{X} - E(\bar{X})| \ge \epsilon) \le \frac{Var(X)}{\epsilon^2} = \frac{\sigma^2}{n\epsilon^2} \rightarrow 0$，当 $n \rightarrow \infty$ 
- 推论：如果以 $\xi_n$ 表示 $n$ 重伯努利实验中的成功次数，则 $f_n = \frac{\xi_n}{n} \rightarrow p$，即频率（依概率）收敛到概率。简单来说就是当抽样足够多的时候，频率趋近于真实的概率
- 在大量重复试验的基础上，随机事件多次发生的平均结果具有稳定性，不再具有随机性；大数定律表达了大量重复试验中随机现象所呈现的稳定性
- 几个常见的大数定理
  - **切比雪夫大数定理**：{$X_n$} 是相互独立的随机变量，他们都有有限的方差，且方差有共同的上界，即 $Var(X_i) \le C$，则对于任意的 $\epsilon > 0$，$limP(|\frac{1}{n}\sum X_i - \frac{1}{n} \sum E(X_i) | < \epsilon) = 1$，切比雪夫大数定理表明：当 $n$ 充分的大，$n$ 个独立随机变量平均值的离散程度较小，即经过算术平均以后的随机变量 $\frac{1}{n}\sum X_i $ 将比较密集地聚集在它的期望 $E(\frac{1}{n}\sum X_i ) =\frac{1}{n} \sum E(X_i) $  附近
  - **辛钦大数定理**： {$X_n$} 是独立同分布的随机变量，且 $E(X_i) = \mu$，则对于任意的 $\epsilon > 0$，$lim_{n \rightarrow \infty}P(|\frac{1}{n}\sum X_i  - \mu| < \epsilon ) = 1$，辛钦大数定理表明：当随机变量序列独立同分布时，随机变量序列的算术平均依概率收敛于整体均值。
  - **伯努利大数定理**：设 $n_A$ 是 $n$ 次独立重复试验中事件 $A$ 发生的次数，$p$ 是事件 $A$ 在每次试验中发生的概率，则对于任意的 $\epsilon > 0$，有 $lim_{n \rightarrow \infty}P(|\frac{n_A}{n} - p| < \epsilon ) = 1$，伯努利大数定理表明：在大量试验的基础上，频率依概率收敛于概率。

<br>

# 中心极限定理

- 定义：{$X_n$} 是一列独立同分布的随机变量序列，具有共同的数学期望 $\mu$，共同的方差 $\sigma^2$，则 $P(\frac{X_1 + X_2 + \dots +X_n - n\mu}{\sigma \sqrt{n}} \le x) = \Phi(x)$ 
  - 令 $S_n = X_1 + X_2 + \dots + X_n$，则 $E(S_n)= n\mu$，$Var(S_n) = n\sigma^2$
  - $\frac{X_1 + X_2 + \dots +X_n - n\mu}{\sigma \sqrt{n}} $ 就是随机变量 $S_n$ 的标准化
  - 中心极限定理描述的就是，当 $n$ 趋于无穷时，序列和的标准化趋向于标准正太分布，即$\frac{X_1 + X_2 + \dots +X_n - n\mu}{\sigma \sqrt{n}} \rightarrow N(0,1) $ 
  - 注意：{$X_n$} 的随机变量不必是正态分布，可以是离散型随机变量，也可以是连续型的
- 用中心极限定理逼近二项分布
  - 假设 {$X_n$} 都是0-1分布，那么 $S_n \sim B(n,p)$，可以将二项分布标准化成标准正太分布，从而计算二项分布的概率
  - $E(S_n) = np$，$Var(S_n) = np(1-p) = npq$，令 $q = 1-p$ 
  - $P(k_1 \le X \le k_2) = P(\frac{k_1 - np}{\sqrt{npq})} \le \frac{X-np}{\sqrt{npq}} \le \frac{k_2 - np}{\sqrt{npq}}) = \Phi(\frac{k_2 - np}{\sqrt{npq}}) - \Phi(\frac{k_1 - np}{\sqrt{npq}})$ 
  - $P(X=k)=P(k - \frac{1}{2} \le X \le k +\frac{1}{2}) = P(\frac{k - 0.5 - np}{\sqrt{npq})} \le \frac{X-np}{\sqrt{npq}} \le \frac{k + 0.5 - np}{\sqrt{npq}})$
    $ = \Phi(\frac{k + 0.5 - np}{\sqrt{npq}}) - \Phi(\frac{k - 0.5- np}{\sqrt{npq}}) $
    再根据微分中指定理
    $=\phi(\frac{k-np}{\sqrt{npq}})\frac{1}{\sqrt{npq}} = \frac{1}{\sqrt{2\pi npq}}exp(-\frac{(k-np)^2}{2npq})$  

<br>

*In Conclusion*
大数定理和中心极限定理是连接概率论和数理统计的重要定理




<br>
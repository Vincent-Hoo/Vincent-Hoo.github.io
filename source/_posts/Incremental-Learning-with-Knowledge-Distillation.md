---
title: Incremental Learning with Knowledge Distillation
mathjax: true
date: 2019-11-30 12:20:39
tags:
---

本文将介绍增量学习 (incremental learning) 和知识蒸馏相结合的文章

- Large Scale Incremental Learning
- End-to-End Incremental Learning
- iCaRL: Incremental Classiﬁer and Representation Learning

<!--more-->

*preview*

增量学习：传统的学习策略是，给定一堆数据集，然后训练一个模型，数据集限定了模型需要学习的东西；但人类的学习策略是渐进的，我们可能会先从一批数据中学习出一些知识，然后再从另外一批数据中学习另外的知识并且不会忘记之前的知识。但是到了深度学习模型，假如我们先让模型学习一个数据集 A，再让模型学习数据集 B（这时候数据集 A 已经不可获取），这时候模型就会彻底忘记数据集 A 中学到的知识，这个称为灾难性遗忘 (catastrophic forgetting)，是增量学习中所要解决的问题。增量学习是一种渐进式的学习策略，后面学习的时候，前面学习用到的数据集不可获取，这是增量学习的基本背景，如何能够在新数据集下引入旧数据集，或者如何能够保存旧数据集下学习到的知识，成为关键。

增量学习又分为数据增量、特征增量和类别增量。数据增量比较好理解，就是增加了额外的数据继续训练；特征增量指在特征数量增多；类别增量指类别数量增多，比如初代模型 $M_0$ 学习到 10 个类别，第二批数据为另外的 10 个类，我们要求第二代模型 $M_1$ 要学习到 20 个类别，既要记住之前学习的 10 个类别，也要学会新的 10 个类别。

我们重点关注的是类别增量，因为标准数据集 CIFAR-100 和 ImageNet 天然就是一个类别增量的数据集，我们可以 10 类一个单位的增加。类增量的算法围绕如何在新数据集学习下引入旧数据的信息，其主要有三类：

- 不采用旧数据
- 用合成数据，训练一个生成模型去生成一些旧数据，然后混合新数据一起训练。
- 用少量的旧数据作为 exemplar，从旧数据中挑选出一些代表数据 exemplar，然后混合新数据一起训练。

在研究一个增量学习算法时，我们要时刻关注这四个方面：旧数据，新数据，旧模型，新模型。旧数据如何引入到新模型？旧模型的参数如何辅助新模型训练？旧模型和新模型之间是否存在共用参数？

下面我们看看这 3 篇论文如何解决上面提到的这些问题。

<br>

# Large Scale Incremental Learning

>Conference: CVPR2019
>Authors: Yue Wu, Yinpeng Chen, et al

## Methods

整体的流程如下，首先我们先看数据方面，可以看出该方法属于上面提到的第三类方法，即保存少部分的旧数据作为 exemplar，然后混合新数据一起训练，数据集分为训练集和验证集，训练的过程作者分为两个阶段，第一阶段新旧数据一起训练，第二阶段为 bias correction，用验证集去训练。

{% asset_img 1.png 500 500 %}



### Stage I: baseline method

我们先看下第一阶段的训练过程，如下图，可以看出新模型和旧模型并不是共享某些层的，新模型需要重新训练，但他们的结构基本相同，除了最后的 FC 层的类别数量，旧模型输出为 n 类的概率值，新模型输出的是 n+m 类的概率值

{% asset_img 2.png 500 500 %}



回答上面提到的问题，旧模型和旧数据如何利用？

- 新数据为 $X^m=\{(x_i, y_i), 1\le i\le M, y_i \in [n+1, ...,n+m]\}$
- 旧数据 exemplar 为 $\hat{X}^n=\{(\hat{x}_j, \hat{y}_j), 1\le j\le N_s, \hat{y}_j \in [1, ...,n]\}$
- 新模型的输出 $o^{n+m}(x)=[o_1(x), ...,x_n(x), o_{n+1}(x),...,o_{n+m}(x)]$
- 旧模型的输出 $\hat{o}^n(x)=[\hat{o}_1(x), ...,\hat{o}_n(x)]$\

我们把旧模型作为 teacher 模型，进行知识蒸馏，注意这里蒸馏的时候只在前面 n 个类别上进行，并且用到的是全部数据，即新数据和旧数据，有人可能会问为什么新数据也要进行蒸馏，旧模型是在旧类别上训练的，但是将新数据输入到旧模型里面，我们依旧能得到新数据在就类别上的预测概率值，可以知道新图片在旧类别上的概率分布，这对于新模型 FC 层前 n 个神经元的训练还是有所帮助的。

利用蒸馏，我们可以一定程度上缓解灾难性遗忘，把旧数据在旧模型输出的 logit 迁移到新模型 FC 层的前 n 个神经元上

{% asset_img 3.png 500 500 %}

除了蒸馏损失，分类损失也是必须的，因为我们现在有新数据还有旧数据的 exemplar，所以直接将它在新模型上训练，这里训练的是 FC 层的所有神经元。

{% asset_img 4.png 500 500 %}

我们看一下新模型 FC 层的 n+m 个神经元分别受到哪些监督信息

- 前 n 个神经元：新旧数据在旧模型输出 logit 的知识蒸馏，旧数据 exemplar 的 ground-true 信息
- 后 m 个神经元：新数据的 ground-true 信息

增量学习需要解决的就是如何使得新模型的前 n 个神经元能够保存旧类的分类能力，这里采取的是蒸馏的方法。

上面的方法是把蒸馏用到增量学习的 baseline 方法，但由于新旧类数据量不均衡，新类的数据要远多于旧类的 exemplar，这会导致 FC 层会更多的偏向于后面 m 个神经元，因为后面 m 个神经元受到的监督信息更多，虽然前面 n 个神经元我们已经尽力去补充监督信息，但依旧无法缓解 bias 的问题，从下图可以看出，当在做最后一次增量学习的时候（100 类别，每次增加 20 类），预测的类别多数都在 81-100 类。

{% asset_img 5.png 300 300 %}

作者猜测这是由于数据不均衡所导致的，因此做了个小实验，先用上面的这个 baseline 方法训练一个新模型（这个模型 FC 层存在 bias 问题），然后再用全部的新旧数据（该数据不存在类别不均衡问题）去 fine-tune 最后的 FC 层，发现 bias 问题被解决，分类准确率也提升了。但是现实情况，我们肯定是不可能获取得到全部的旧数据的，因此作者提出一种 bias correction (biC) 的方法去调整 FC 层。

{% asset_img 6.png 300 300 %}

### Stage II: bias correction

首先需要把数据集划分为训练集和验证集，注意验证集新旧类别必须均衡，可以看上面的总流程图，由于验证集很小，新旧类别的数据都很少，所以作者在 FC 层后加了一个线性模型来矫正 bias，对于 FC 层前 n 个输出，保持原值，对于后 m 个输出，进行矫正

{% asset_img 12.png 400 400 %}

 当进行 bias correction 的时候，前面的特征提取层和 FC 层都是固定的，只训练线性模型，loss 函数如下

{% asset_img 13.png 400 400 %}

<br>

## Experiments

实验设置基本和 EEIL 和 iCaRL 一样（下面讨论这两篇文章），作者分别在大数据集 ImageNet 和小数据集 CIFAR-100 上测试了结果，实验表明 bias correction 这种方法都比 EEIL 和 iCaRL 要好，但是在大数据集上的提升会更加明显

### ImageNet

{% asset_img 7.png 700 700 %}

{% asset_img 8.png 300 300 %}

### CIFAR-100

{% asset_img 9.png 700 700 %}

<br>

# End-to-End Incremental Learning

>Conference: ECCV2018
>Authors: Francisco M. Castro, et al

EEIL 同样是基于 exemplar 的方法，在上面的文章，我们侧重说了蒸馏如何用到增量学习上，这篇文章的蒸馏思路和上篇基本类似，这里我们侧重说下 exemplar 的选取。

## Method

流程图如下，训练分为四步：

- 构造训练集
- 蒸馏训练
- balanced fine-tune
- 构造 exemplar set

{% asset_img 10.png 600 600 %}



### exemplar

作者提出了两种策略，分别为固定总的 memory size 和 固定每个类别的 memory size，前者随着增量学习的进行，每个旧类的样本数量会减少，后者随着增量学习的进行，内存会增大。

训练前，先将新数据和旧数据进行融合，构造出训练集，训练完后，将新数据加入到 memory，如何挑取加入到 memory 的样本，作者提出了两种方法

- 随机策略
- herding selection：计算该类别样本的均值，计算每个样本距离均值的距离，选出最靠近均值的样本

新数据的加入必然会导致旧数据样本数量的减少，因为旧数据也是通过 herding 的方式加入到内存的，在移除旧数据的时候，同样采取该策略，移除距离均值最大的样本。



### training process

训练过程，旧模型输出的是 n 个类别的 logit，作为 teacher 模型，新模型输出的是 n+m 个类别的 logit，同样分为两个 loss

- cross entropy loss：新旧数据同时用上来训练新模型
- distillation loss：和上一篇文章唯一不同的是，这里它只用了旧数据，旧数据在旧模型输出的 logit 作为软标签，指导新模型 FC 层的前 n 个神经元。而上篇文章是将新旧数据都输入到旧模型来作为软标签。



### balanced fine-tuning

为了解决类别不均衡带来的问题，跟上篇文章一样，新旧类选取相同数量的样本作为验证集进行 fine-tune，这里会用更小的学习率

<br>



# iCaRL: Incremental Classiﬁer and Representation Learning

总结前两篇文章，发现增量学习的知识蒸馏的基本围绕以下 4 个损失来设计，分类损失均为数据在新模型下输出 logit 和标签之前的损失，蒸馏损失均为数据在新旧模型输出 logit 之间的损失。

- 新数据的分类损失
- 新数据的蒸馏损失
- 旧数据的分类损失
- 旧数据的蒸馏损失

第一篇文章为 4 个损失均有，EEIL 为第1、3、4 个损失的结合；而 iCaRL 为第1、2、4 个损失的组合。

除此之外，iCaRL 在分类上采取的是最近邻分类策略，算出每个类别的平均特征图，计算最近的一个，则为分类结果。

{% asset_img 11.png 500 500 %}

<br>

